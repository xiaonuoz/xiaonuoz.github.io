[{"content":"此文章为学习 《Go 语言设计与实现》的总结性文章\n编译原理 词法分析和语法分析 词法分析 源代码在计算机眼中其实是一团乱麻，是一个由字符组成的、无法被理解的字符串，为了理解这些字符我们需要做的第一件事情就是 将字符串分组 ，这能够降低理解字符串的成本，简化源代码的分析过程。\n源文件在机器的眼中是很难理解的字符串，词法分析会将这些字符串解析成一个个token序列以便机器理解。使用的就是类似lex之类的词法分析器。\n1 2 3 4 5 6 7 8 9 PACKAGE IDENT IMPORT LPAREN QUOTE IDENT QUOTE RPAREN IDENT IDENT LPAREN RPAREN LBRACE IDENT DOT IDENT LPAREN QUOTE IDENT QUOTE RPAREN RBRACE 从上面的输出我们能够看到 Go 源代码的影子，lex 生成的词法分析器 lexer 通过正则匹配的方式将机器原本很难理解的字符串进行分解成很多的 Token，有利于后面的处理。\n早期Go是使用lex进行词法解析，但是之后还是会用Go自己实现的词法分析器分析解析出的数据。而如今词法解析和词法分析都是用Go自己实现的工具。\n语法分析 语法分析是根据某种特定的形式文法对 Token 序列构成的输入文本进行分析并确定其语法构成的过程。词法分析器输出的结果Token序列是语法分析器的输入。\n文法 上下文无关文法 是用来形式化、精确描述某种编程语言的工具，我们能够通过文法定义一种语言的语法，它主要包含一系列用于转换字符串的生产规则。上下文无关文法中的每一个生产规则都会将规则左侧的非终结符转换成右侧的字符串，也可以理解为用左侧的规则名来表示右侧的文件具体内容，从下面代码就可以看出来右侧有很多属于Go日常开发中的常用关键词。\n从 src/cmd/compile/internal/syntax/parser.go文件中摘抄一些 Go 语言文法的生产规则：\n1 2 3 4 5 6 7 8 9 10 SourceFile = PackageClause \u0026#34;;\u0026#34; { ImportDecl \u0026#34;;\u0026#34; } { TopLevelDecl \u0026#34;;\u0026#34; } . PackageClause = \u0026#34;package\u0026#34; PackageName . PackageName = identifier . ImportDecl = \u0026#34;import\u0026#34; ( ImportSpec | \u0026#34;(\u0026#34; { ImportSpec \u0026#34;;\u0026#34; } \u0026#34;)\u0026#34; ) . ImportSpec = [ \u0026#34;.\u0026#34; | PackageName ] ImportPath . ImportPath = string_lit . TopLevelDecl = Declaration | FunctionDecl | MethodDecl . Declaration = ConstDecl | TypeDecl | VarDecl . 因为每个 Go 源代码文件最终都会被解析成一个独立的抽象语法树，所以语法树最顶层的结构或者开始符号都是 SourceFile。\n从 SourceFile 相关的生产规则我们可以看出，每一个文件都包含一个 package 的定义以及可选的 import 声明和其他的顶层声明（在花括号中表示可选，类似正则），每一个 SourceFile 在编译器中都对应一个 cmd/compile/internal/syntax.File 结构体，你能从它们的定义中轻松找到两者的联系：\n1 2 3 4 5 6 7 type File struct { Pragma Pragma PkgName *Name DeclList []Decl Lines uint node } 顶层声明有五大类型，分别是常量、类型、变量、函数和方法，你可以在文件 src/cmd/compile/internal/syntax/parser.go 中找到这五大类型的定义。\n1 2 3 4 5 6 7 8 9 10 ConstDecl = \u0026#34;const\u0026#34; ( ConstSpec | \u0026#34;(\u0026#34; { ConstSpec \u0026#34;;\u0026#34; } \u0026#34;)\u0026#34; ) . ConstSpec = IdentifierList [ [ Type ] \u0026#34;=\u0026#34; ExpressionList ] . TypeDecl = \u0026#34;type\u0026#34; ( TypeSpec | \u0026#34;(\u0026#34; { TypeSpec \u0026#34;;\u0026#34; } \u0026#34;)\u0026#34; ) . TypeSpec = AliasDecl | TypeDef . AliasDecl = identifier \u0026#34;=\u0026#34; Type . TypeDef = identifier Type . VarDecl = \u0026#34;var\u0026#34; ( VarSpec | \u0026#34;(\u0026#34; { VarSpec \u0026#34;;\u0026#34; } \u0026#34;)\u0026#34; ) . VarSpec = IdentifierList ( Type [ \u0026#34;=\u0026#34; ExpressionList ] | \u0026#34;=\u0026#34; ExpressionList ) . 上述的文法分别定义了 Go 语言中常量、类型和变量三种常见的结构，从文法中可以看到语言中的很多关键字 const、type 和 var\n分析方法 语法分析的分析方法一般分为自顶向下和自底向上两种，这两种方式会使用不同的方式对输入的 Token 序列进行推导：\n自顶向下分析：可以被看作找到当前输入流最左推导的过程，对于任意一个输入流，根据当前的输入符号，确定一个生产规则，使用生产规则右侧的符号替代相应的非终结符向下推导。 自底向上分析：语法分析器从输入流开始，每次都尝试重写最右侧的多个符号，这其实是说解析器会从最简单的符号进行推导，在解析的最后合并成开始符号。 自顶向下和自底向上都是从开始符号开始解析，直到整个字符串中不存在任何的非终结符，整个解析过程才会结束。个人理解就是从源文件最开始时解析，根据生产规则向下进行逐个匹配。而两者的区别是对于生产规则的匹配一个是从头到尾一个是从尾到头\nLL文法是一种使用自顶向下分析方法的文法。常见的四种文法 LR(0)、SLR、LR(1) 和 LALR(1) 则使用了自底向上的处理方式\nLookahead 在语法分析中除了 LL 和 LR 这两种不同类型的语法分析方法之外，还存在另一个非常重要的概念，就是 向前查看，在不同生产规则发生冲突时（当前内容符合多个生产规则），当前解析器需要通过预读一些 Token 判断当前应该用什么生产规则对输入流进行展开或者归约，例如在 LALR(1) 文法中，需要预读一个 Token 保证出现冲突的生产规则能够被正确处理。\nGo 语言的解析器使用了 LALR(1) 的文法来解析词法分析过程中输出的 Token 序列，最右推导加向前查看构成了 Go 语言解析器的最基本原理，也是大多数编程语言的选择。语法分析就是将token序列根据各种生产规则解析成抽象语法树\n1 2 3 4 5 6 7 \u0026#34;json.go\u0026#34;: SourceFile { PackageName: \u0026#34;json\u0026#34;, ImportDecl: []Import{ \u0026#34;io\u0026#34;, }, TopLevelDecl: ... } 在Go中词法分析和语法分析是同时进行的，词法分析获取一个Token后马上对这个Token进行语法分析，然后循环这个步骤直到读完文件。\n类型检查 强弱类型 强类型和弱类型经常会被放在一起讨论，然而这两者并没有一个学术上的严格定义，我们很多时候只能根据现象和特性从直觉上进行判断：\n强类型的编程语言在编译期间会有更严格的类型限制，也就是编译器会在编译期间发现变量赋值、返回值和函数调用时的类型错误后结束编译。 弱类型的编程语言在出现类型错误时可能会在运行时自动进行隐式的类型转换，在类型转换时可能会造成运行错误，即它对类型的正确与否没有很强的限制，在运行时也只是可能会类型转换失败。 因为Go语言会在编译期间发现类型错误，也属于是强类型的编程语言。\n静态类型和动态类型 静态类型检查 静态类型检查是基于对源代码的分析来确定运行程序 类型安全的过程，如果代码能够通过静态类型检查，那么当前程序在一定程度上可以满足类型安全的要求，它能够减少程序在运行时的类型检查，也可以被看作是一种代码优化的方式。\n静态类型检查可以帮助我们在编译期间发现程序中出现的类型错误，一些动态类型的编程语言都有社区提供的工具为这些编程语言加入静态类型检查功能，比如JS的Flow，这些工具能够在编译期间发现代码中的类型错误。\n静态类型检查为代码在编译期间提供了约束，编译器能够在编译期间约束变量的类型。避免类型错误直到运行时才会报错崩溃，要是业务逻辑很复杂且这个错误在判断语句中，那么寻找这个错误就会花费大量的时间（特定情况才出现bug，很难排查），而静态类型检查在编译时就会明明白白的告诉你错误发生在何处，是什么原因。\n静态类型检查在重构时能够帮助我们节省大量的时间并避免遗漏，但是如果编程语言仅支持动态类型检查，那么就需要写大量的单元测试保证重构不会出现类型错误。\n动态类型检查 大部分的动态类型编程语言是解释型的，也就是一边解释一边执行，没有生成中间代码或者二进制文件的过程。所有的语言都需要经过编译这一过程，但是不一定有编译器(compiler)这东西，它们可能是在解释器(interpreter)中进行这一过程。\n动态类型检查是在运行时再确定程序类型安全的过程，它需要编程语言在编译时为所有的对象加入类型标签等信息，运行时可以使用这些存储的类型信息来实现动态派发、向下转型、反射以及其他特性。\n动态类型检查能为程序员提供更多的操作空间，可以在运行时获取一些类型相关的上下文并根据对象的类型完成一些动态操作。虽然使用动态类型检查的编程语言在使用上非常灵活也不需要经过编译，但是有问题的代码不会因为更加灵活就减少错误，它在提高灵活性的同时也提高了对程序员开发能力的要求。\n静态类型检查和动态类型检查不是完全冲突和对立的，它们一个作用与编译期一个作用与运行时，很多编程语言都会同时使用两种类型检查。例如Java不仅在编译期间检查类型错误，还为对象添加了类型信息，在运行时使用反射根据对象的类型动态地执行方法增强灵活性并减少冗余代码。\ngolang就属于静态类型检查的编程语言，但通过反射机制提供了一定程度的动态类型检查的能力。这使得Go语言在处理类型信息和实现某些特定的动态行为时更加灵活和强大。但静态类型检查仍然是它的主要特点和设计原则。\ngolang可以使用 ( := )来进行类型推断，但是这个是在编译期推断的，并不是在运行时，编译期就已经根据 ( := )的初始值确定了变量的类型，后续不能再进行更改，这个只是为了方便变量声明的简洁性和可读性。\nGo语言的编译器不仅使用静态类型检查来保证程序运行的类型安全，还会和Java一样在编程期间引入类型信息，让程序员可以使用反射来判断参数和变量的类型。当我们想要将 interface{} 转换成具体类型时就会进行动态类型检查(运行时)，如果无法发生转换就会出现程序崩溃。\n类型检查是Go语言编译的第二个阶段，在词法和语法分析之后我们能得到每个文件对应的抽象语法树，随后的类型检查会遍历抽象语法树中的节点，对每个节点的类型进行检验，找出其中存在的语法错误，并且在这个过程中也会对抽象语法树进行改写，这不仅能够去除一些不会被执行的代码、对代码进行优化以提高执行效率，而且也会修改make、new等关键字对应节点的操作类型。\nmake和new 这些内置函数其实并不会直接对应某些函数的实现，而是会在编译阶段被转换成真正存在的其他函数。make就会在类型检查阶段根据创建的具体类型(make的第一个参数) 将make 替换成特定的函数 makechan、makeslice、makemap，这样后面 生成中间代码阶段 就不会再处理OMAKE类型的节点了，而是会根据生成的细分类型进行处理。\n中间代码生成 词法分析和语法分析以及类型检查都属于编译器前端，它们负责对源码进行分析并检查其中存在的词法和语法错误，经过这两个阶段生成的抽象语法树已经不存在语法错误了。而中间代码生成属于编译器后端。\n中间代码是编译器或者虚拟机 (JAVA中的JVM) 使用的语言，它可以帮助我们分析计算机程序。在编译过程中，编译器会在 将源码转换为机器码的过程中，先将源码转换成一种中间的表示形式，即中间代码。\n很多人可能认为中间代码没有太多价值，可以直接将源码翻译成目标语言，这种看起来可行的办法实际上有很多问题，其中最主要的问题就是它忽略了编译器所面对的复杂场景，很多编译器都需要将源码翻译成多种机器码，那么在这种情况下直接翻译高级编程语言就比较困难，翻译程序会十分庞大。而中间代码是一种更接近机器语言的表达形式，对中间代码的优化和分析相比于直接分析高级语言会更加容易。\n中间代码的生成过程是从AST 抽象语法树到 SSA 中间代码的转换过程，在这期间会对语法树中的关键字再进行改写 (defer 及new之类的内置函数)，改写后的语法树会经过 多轮处理 转变成最后的SSA中间代码，而在这多轮处理时就会包含一些针对机器的特定修改，包括根据目标架构对代码进行改写（目标架构不一定是当前电脑，go可以指定编译各种架构的二进制文件），相关代码中包含了大量switch语句、复杂的函数和调用栈（部分文件一个switch就占了上千行，正常开发是不允许有这种情况的）。\n很多Go关键字和内置函数就是在这个阶段被转换成运行时包中方法的。\n中间代码就是指令集，而在机器码生成阶段会先将 SSA中间代码 转换成汇编语言最终再编译为机器码，而机器码就是最终的二进制文件。中间代码是一种特定于编程语言和平台的中间表示形式，通常比源代码更接近于机器码，但仍不是直接的机器码。\n机器码生成 Go语言编译的最后一个阶段是根据 SSA 中间代码生成机器码，这里谈的机器码是在目标 CPU架构上能够运行的二进制代码，而在中间代码环节的将近50 个生成中间代码的步骤中有一些过程严格上说是属于机器码生成阶段的。\n机器码的生成过程其实就是对 SSA 中间代码的降级过程，在SSA中间代码降级的过程中，编译器将一些值重写成了目标CPU架构的特定值，降级的过程 处理了所有 机器特定的重写规则并对代码进行了一定程度的优化；在 SSA 中间代码生成阶段的最后，Go 函数体的代码会被转换成 cmd/compile/internal/obj.Prog 结构。\n指令集架构 指令集架构是计算机的抽象模型，在很多时候也被称作架构或者计算机架构，它是计算机软件和硬件之间的接口和桥梁；一个为特定指令集架构编写的应用程序能够运行在所有支持这种指令集架构的机器上，也就是说如果当前应用程序支持x86的指令集，那么就可以运行在所有使用x86指令集的机器上，这其实就是抽象层的作用。每一个指令集架构都定义了支持的数据结构、寄存器、管理主内存的硬件支持（例如内存一致、地址模拟以及虚拟内存）、支持的指令集和IO模型，指令集的引入其实就是在软件和硬件之间引入了一个抽象层，让同一个二进制文件能够在不同版本的硬件上运行。\n当前这里只是说是可以在同一种指令集机器上运行，并没有考虑系统，系统也是属于软件，不同的系统针对二进制文件可能有不同的处理，例如windows编译后的二进制文件放在linux上可能就无法运行，这是因为他们依赖并不相同。\n如果一个编程语言想要在所有的机器上运行，它就可以将中间代码分别转换成使用不同指令集架构的机器码，这可比为不同硬件单独移植要简单的太多了。\n复杂指令集（CISC）和精简指令集（RISC） 最常见的指令集架构分类方法是根据指令的复杂度将其分为 复杂指令集（CISC）和精简指令集（RISC），复杂指令集架构包含了很多特定的指令，但是其中的一些指令很少会被程序使用，但是又不能舍弃。而精简指令集只实现了经常被使用的指令，不常用的操作都会通过组合简单指令来实现。\n因此复杂指令集性能更强大，因为它在电路板上焊了特定的指令集，但是这也意味着功耗更高，体积更大，而精简指令集由于是组合而来，它会弱一些，但是它功耗和体积会小点。\n复杂指令集的特点就是指令数目多且复杂，每条指令的字节长度并不相等，x86就是常见的复杂指令集处理器，它的指令长度大小范围非常广，从 1 到 15 字节不等，对于长度不固定的指令，计算机必须额外对指令进行判断，这需要付出额外的性能损失。\n而精简指令集对指令的数目和寻址方式做了精简，大大减少指令数量的同时也更容易实现，指令集中的每一个指令都使用标准的字节长度、执行时间相比复杂指令集会少很多，处理器在处理指令时也可以流水执行，提高了对并行的支持。\nARM 作为一种常见的精简指令集处理器，使用了 4个字节来作为指令集的固定长度，解决了因判断指令长度造成的性能损失问题。精简指令集其实就是利用了我们耳熟能详的二八原则，用20%的基础指令和它们的组合来解决大部分问题。\n其宗旨就是通过优化设计，使得用尽可能少的基础指令可以覆盖尽可能多的操作需求。剩余的少部分特殊问题会在实际情况中通过如微码指令、专门的硬件单元等方法解决。\n最开始的计算机使用复杂指令集是因为当时计算机的性能和内存都比较有限，业界需要尽可能地减少机器需要执行的指令，所以更倾向于高度编码、长度不等以及多操作数的指令。不过随着计算机性能的提升，出现了精简指令集这种牺牲代码密度换取简单实现的设计；除此之外，硬件的飞速提升还带来了更多的寄存器和更高的时钟频率，软件开发人员也不用再直接接触汇编代码，而是通过编译器和汇编器生成指令，复杂的机器指令对于编译器来说很难利用，所以精简指令在这种场景下更适合。\n复杂指令集和精简指令集的使用是设计上的权衡，经过这么多年的发展，两种指令集也在相互借鉴和学习，与最开始刚被设计出来时已经有了较大的差别。\n机器码生成 机器码的生成在 Go 的编译器中主要由两部分协同工作，其中一部分是负责 SSA 中间代码降级和根据目标架构进行特定处理的 cmd/compile/internal/ssa 包，另一部分是负责生成机器码的 cmd/internal/obj\nSSA降级 SSA 降级是在中间代码生成的过程中完成的，其中将近 50 轮处理的过程中，lower 以及后面的阶段都属于 SSA 降级这一过程，这么多轮的处理会对架构进行特定的优化和重写并生成 cmd/compile/internal/obj.Prog 指令：\n1 2 3 4 5 6 7 8 var passes = [...]pass{ ... {name: \u0026#34;lower\u0026#34;, fn: lower, required: true}, {name: \u0026#34;lowered deadcode for cse\u0026#34;, fn: deadcode}, // deadcode immediately before CSE avoids CSE making dead values live again {name: \u0026#34;lowered cse\u0026#34;, fn: cse}, ... {name: \u0026#34;trim\u0026#34;, fn: trim}, // remove empty blocks } 重写的过程会将通用的 SSA中间代码 转换成目标架构特定的指令。可以通过对指令的压缩和优化减少在目标硬件上执行所需要的时间和资源。而紧接着就会使用obj包将SSA中间代码转换成汇编代码再编译为机器码。\ncmd/compile/internal/gc.buildssa 中的 lower 和随后的多个阶段会对 SSA 进行转换、检查和优化，生成机器特定的中间代码（之前属于中间代码生成阶段）， cmd/compile/internal/gc.compileSSA 中的genssa会创建一个新的 cmd/compile/internal/gc.Progs 结构并将生成的 SSA 中间代码都存入新建的结构体中，至此SSA降级结束，随后调用的 pp.Flush() - cmd/compile/internal/gc.Progs.Flush 会使用 cmd/internal/obj 包中的汇编器将 SSA 转换成汇编代码并编译成机器码。\n1 2 3 4 5 6 7 8 func compileSSA(fn *Node, worker int) { f := buildssa(fn, worker) pp := newProgs(fn, worker) defer pp.Free() genssa(f, pp) pp.Flush() } 转换汇编代码和编译成机器码都是由pp.Flush() 中的cmd/internal/obj.Flushplist 中的 Preprocess 和 Assemble 方法组合完成的，这两个方法非常复杂，且会根据不同架构执行不同的方法，比如x86架构就会执行cmd/internal/obj/x86.preprocess 和 cmd/internal/obj/x86.span6 。\n汇编器 汇编器是将汇编语言翻译为机器语言的程序，Go 语言的汇编器是基于 Plan 9 汇编器的输入类型设计的，不过Go 语言对于汇编语言 Plan 9 和汇编器的资料十分缺乏。\n流程为：\n源码 -\u0026gt; 词法解析 -\u0026gt; 语法解析 -\u0026gt; 抽象语法树 -\u0026gt; 类型检验 -\u0026gt; 抽象语法树 -\u0026gt; 中间代码 -\u0026gt; 汇编 -\u0026gt; 机器码\n数据结构 数组 数组是由相同类型元素的集合组成的数据结构，计算机会为数组分配一块连续的内存来保存其中的元素，我们可以利用数组中元素的索引来快速访问特定的元素。\n数组作为一种基本的数据类型，我们通常会从两个维度描述数组，也就是数组中存储的元素类型和数组最大能存储的元素个数。Go语言中 数组在初始化之后大小就无法改变，它的大小在初始化时就已经确定了，存储元素类型相同、但是大小不同的数组类型在Go 语言看来是两个完全不同的类型，只有两个条件都相同才是同一个类型。\n1 2 3 4 5 6 7 8 9 func NewArray(elem *Type, bound int64) *Type { if bound \u0026lt; 0 { Fatalf(\u0026#34;NewArray: invalid bound %v\u0026#34;, bound) } t := New(TARRAY) t.Extra = \u0026amp;Array{Elem: elem, Bound: bound} t.SetNotInHeap(elem.NotInHeap()) return t } 编译期间的数组类型是由上述的 cmd/compile/internal/types.NewArray 函数生成的，该类型包含两个字段，分别是元素类型 Elem 和数组的大小 Bound，这两个字段共同构成了数组类型，而当前数组是否应该在堆栈中初始化也在编译期就确定了。\n初始化 Go语言的数组有两种不同的创建方式，一种是显式的指定数组大小，另一种是使用 [...]T 声明数组，Go语言会在编译期间通过源码自动推导数组的大小。这两种声明方式在运行期间得到的结果是完全相同的，后一种声明方式在编译期间就会被转换成前一种，这就是编译器对数组大小的推导。\n上限推导 这两种不同的声明方式会导致编译器做出完全不同的处理，如果使用 [10]T 的方式，那么变量的类型在编译进行到类型检查阶段就会被提取出来，随后会调用NewArray函数创建包含数组大小的结构体。\n如果使用 [...]T 的方式声明数组，编译器会在 cmd/compile/internal/gc.typecheckcomplit 函数中先对该数组的大小进行推导，其中的 typecheckarraylit 函数会通过遍历元素的方式来计算数组中元素的数量，再调用NewArray函数创建数组结构体。\n所以，[...]T{1, 2, 3} 和 [3]T{1, 2, 3} 在运行时是完全等价的，[...]T 这种初始化方式也只是 Go 语言为我们提供的一种语法糖，当我们不想计算数组中的元素个数时可以通过这种方法减少一些工作量。\n语句转换 对于一个由字面量组成的数组（字面量在数组中指的就是初始化时附带的元素集合，即 [4]int{1,2,3,4}中的[1,2,3,4]就是字面量），根据数组初始化的元素数量的不同，编译器会在负责 初始化 字面量的 cmd/compile/internal/gc.anylit 函数中做两种不同的优化：\n当元素数量小于或者等于 4 个时，会直接将数组中的元素放置在栈上； 当元素数量大于 4 个时，会将数组中的元素放置到静态区并在运行时取出； 大于4个放置在静态区是因为 当数组元素较多时，栈上分配可能导致栈溢出等问题，所以会先在静态存储区初始化然后拷贝到栈上，不要纠结初始化并赋值和先初始化后赋值谁快谁慢的问题，在编译阶段会对 [4]int{1,2,3,4}进行展开，变成先声明后一一赋值的形式，日常开发不用考虑初始化时元素个数的情况。\n1 2 3 4 5 6 7 var arr [5]int statictmp_0[0] = 1 statictmp_0[1] = 2 statictmp_0[2] = 3 statictmp_0[3] = 4 statictmp_0[4] = 5 arr = statictmp_0 访问和赋值 无论是在栈上还是静态存储区，数组在内存中都是一连串的内存空间，我们通过指向数组开头的指针、元素的数量以及元素类型占的空间大小表示数组。如果我们不知道数组中元素的数量，访问时可能发生索引越界；而如果不知道数组中元素类型的大小，就没办法知道一次应该取出多少字节的数据，无论丢失了哪个信息，我们都没办法知道这片连续的内存空间到底存储了什么数据。\n数组访问越界是非常严重的错误，Go 语言中可以在编译期间的静态类型检查判断数组越界，但是由于切片的大小是动态扩容的，切片就没办法在编译期间判断出是否越界的问题，cmd/compile/internal/gc.typecheck1 会验证访问数组的索引。\n访问数组的索引是非整数时，报错 “non-integer array index %v”； 访问数组的索引是负数时，报错 “invalid array index %v (index must be non-negative)\u0026quot;； 访问数组的索引越界时，报错 “invalid array index %v (out of bounds for %d-element array)\u0026quot;； 数组和字符串的一些简单越界错误都会在编译期间发现，但是如果使用变量去访问数组或者字符串时，编译器就无法提前发现错误，我们需要 Go 语言在运行时来阻止不合法的访问。Go 语言运行时在发现数组、切片和字符串的越界操作会由运行时的 runtime.panicIndex 和 runtime.goPanicIndex 触发程序的运行时错误并导致异常崩溃退出。\n只有当编译器无法对数组下标是否越界作出判断时才会加入 PanicBounds 指令（即runtime.panicIndex 函数）交给运行时进行判断，在使用常量数组下标时，如果静态类型检查阶段通过就会生成非常简单的中间代码。\n生成ssa.html:\n1 2 3 4 5 6 7 8 9 10 11 package check func outOfRange() int { arr := [3]int{1, 2, 3} i := 4 elem := arr[i] return elem } $ GOSSAFUNC=outOfRange go build array.go dumped SSA to ./ssa.html 访问arr[2]生成的对应的SSA代码(start 阶段)：\n1 2 3 4 5 6 7 b1: v14 (?) = Const64 \u0026lt;int\u0026gt; [2] ... v21 (5) = LocalAddr \u0026lt;*[3]int\u0026gt; {arr} v2 v20 v22 (5) = PtrIndex \u0026lt;*int\u0026gt; v21 v14 v23 (5) = Load \u0026lt;int\u0026gt; v22 v20 (elem[int]) ... v21是寻址获取数组地址，v22是利用 PtrIndex 通过数组地址和下标计算出目标元素的地址，最后使用 Load 操作将指针中的元素加载到内存中\nGo语言对于数组的访问还是有着比较多的检查的，它不仅会在编译期间提前发现一些简单的越界错误并插入用于检测数组上限的 PanicBounds 函数调用，还会在运行期间通过插入的函数来保证数组不会发生越界。\n数组的赋值和更新操作 a[2] = 2 也会生成SSA中间代码：\n1 2 3 4 5 6 b1: ... v21 (5) = LocalAddr \u0026lt;*[3]int\u0026gt; {arr} v2 v19 v22 (5) = PtrIndex \u0026lt;*int\u0026gt; v21 v13 v23 (5) = Store \u0026lt;mem\u0026gt; {int} v22 v20 v19 ... 赋值的过程中会先确定目标数组的地址，再通过 PtrIndex 获取目标元素的地址，最后使用 Store 指令将数据存入地址中。而如果下标不是常量也会有 PanicBounds 函数调用来避免越界。\n总结一下就是，数组的访问和赋值需要同时依赖编译器和运行时，它的大多数操作在编译期间都会转换成直接读写内存，并在中间代码生成阶段，如果在编译期间无法判断数组是否越界（即下标非常量），编译器还会插入运行时方法 runtime.panicIndex 调用防止发生越界错误。\n切片 数组在Go 语言中没有那么常用，更常用的数据结构是切片，即动态数组，其长度并不固定，可以随时向切片中追加元素，它会在容量不足时自动扩容。\n数据结构 编译期间的切片是 cmd/compile/internal/types.Slice 类型的，但是在运行时切片是由 reflect.SliceHeader 结构体表示，其中:\nData 是指向底层数组的指针; Len 是当前切片的长度； Cap 是当前切片的容量，即 Data 数组的大小： 1 2 3 4 5 type SliceHeader struct { Data uintptr Len int Cap int } Data 是一片连续的内存空间，这片内存空间可以用于存储切片中的全部元素，元素在底层存储时都是连续的，因此可以将切片理解成一片连续的内存空间加上长度与容量的标识。\n切片与数组的关系非常密切，切片引入了一个抽象层，提供了对底层数组中部分连续片段的引用，而作为数组的引用，我们可以在运行期间修改它的长度和范围。当切片底层的数组长度不足以存放新增元素时就会触发扩容，扩容会导致其指向另一个新的底层数组，不过在上层看来切片是没有变化的，上层只需要与切片打交道，不需要关心底层数组的变化。\n数组的内存布局在编译时就已经确定（长度固定），在运行时可以直接读写内存的特定位置，而切片是在运行时确定结构的，因为切片的长度可能会变化，依赖运行时来支持动态调整大小等操作。\n初始化 Go 语言中包含三种初始化切片的方式：\n通过下标的方式获得数组或者切片的一部分； 使用字面量初始化新的切片； 使用关键字 make 创建切片： 1 2 3 arr[0:3] or slice[0:3] slice := []int{1, 2, 3} slice := make([]int, 10) 使用下标 使用下标创建切片是最原始也是最接近汇编语言的方式，它是所有方法中最为底层的一种，编译器会将 arr[0:3] 或者 slice[0:3] 等语句转换成 OpSliceMake 操作\n1 2 3 4 5 6 7 8 9 package opslicemake func newSlice() []int { arr := [3]int{1, 2, 3} slice := arr[0:1] return slice } // GOSSAFUNC=newSlice go build arr1.go 通过 GOSSAFUNC 变量编译上述代码可以得到一系列 SSA 中间代码，其中 slice := arr[0:1] 语句在 “decompose builtin” 阶段对应的代码如下所示：\n1 2 3 4 5 6 7 8 9 v9 (4) = SelectN \u0026lt;*[3]int\u0026gt; [0] v7 (\u0026amp;arr[*[3]int], slice.ptr[*int]) ... v12 (?) = Const64 \u0026lt;int\u0026gt; [1] (slice.len[int]) v15 (?) = Const64 \u0026lt;int\u0026gt; [3] (slice.cap[int]) name \u0026amp;arr[*[3]int]: v9 name slice.ptr[*int]: v9 name slice.len[int]: v12 name slice.cap[int]: v15 SliceMake 操作会接受四个参数创建新的切片，元素类型、数组指针、切片大小和容量，从 ssa.html 中可以发现它的ptr指针指向的依然是原arr数组，而不是重新创建一个新的底层数组，因此使用下标来初始化切片不会拷贝原数组或者原切片中的数据，它只会创建一个指向原数组的切片结构体，因此修改新切片的数据也会修改到原切片（两者指针指向同一片内存）。\n字面量 当我们使用字面量 []int{1, 2, 3} 创建新的切片时，cmd/compile/internal/gc.slicelit 函数会在编译期间将它展开成如下所示的代码片段：\n1 2 3 4 5 6 7 8 9 10 11 // 声明底层数组 var vstat [3]int // 赋值 vstat[0] = 1 vstat[1] = 2 vstat[2] = 3 // 创建底层数组指针 var vauto *[3]int = new([3]int) // 指针指向底层数组 *vauto = vstat slice := vauto[:] 根据切片中的元素数量对底层数组的大小进行推断并创建一个数组； 将这些字面量元素存储到初始化的数组中； 创建一个同样指向 [3]int 类型的数组指针； 将静态存储区的数组 vstat 赋值给 vauto 指针所在的地址； 通过 [:] 操作获取一个底层使用 vauto 的切片； 第 5 步中的 [:] 就是使用下标创建切片的方法，从这一点我们也能看出 [:] 操作是创建切片最底层的一种方法。\n关键字 如果使用字面量的方式创建切片，大部分的工作都会在编译期间完成。但是当我们使用 make 关键字来创建切片时，很多工作都需要运行时的参与；调用方必须向 make 函数传入切片的大小以及可选的容量，类型检查阶段时会校验入参（判断len必须，且cap\u0026gt;=len）并将 OMAKE 节点转换成 OMAKESLICE，然后在中间代码生成阶段会依据两个条件来转换 OMAKESLICE 类型的节点：\n切片的大小和容量是否足够小； 切片是否发生了内存逃逸； 当切片发生内存逃逸或者非常大时，切片的初始化会延后至运行时的runtime.makeslice 在堆上初始化切片（即汇编语言代码中不会有初始化切片的代码，而是一个调用 runtime.makeslice 的函数），如果切片不会发生内存逃逸且非常小的时候，它会在编译阶段直接转换初始化切片的代码。例如 make([]int, 3, 4) 会被直接转换成如下所示的代码：\n1 2 var arr [4]int n := arr[:3] 上述代码会初始化数组并通过下标 [:3] 得到数组对应的切片，这两部分操作都会在编译阶段完成（只是生成代码，内存分配肯定还是在程序运行时，但是不会再调用 runtime.makeslice初始化切片），而这种方式初始化的切片会放在栈上，其底层数组会根据数组的规则放在栈或者静态存储区中。\n运行时函数 runtime.makeslice的实现很简单：\n1 2 3 4 5 6 7 8 9 10 11 12 func makeslice(et *_type, len, cap int) unsafe.Pointer { mem, overflow := math.MulUintptr(et.size, uintptr(cap)) if overflow || mem \u0026gt; maxAlloc || len \u0026lt; 0 || len \u0026gt; cap { mem, overflow := math.MulUintptr(et.size, uintptr(len)) if overflow || mem \u0026gt; maxAlloc || len \u0026lt; 0 { panicmakeslicelen() } panicmakeslicecap() } return mallocgc(mem, et, true) } 上述函数的主要工作是计算切片需要占用的内存空间并在堆上申请一片连续的内存供切片使用。它的计算方式是：\n内存空间 = 切片中单个元素大小 * 切片容量\n当然实际情况中还需要考虑内存对齐、垃圾回收的情况，最终切片申请的内存不一样是公式计算出来的内存，但是肯定会大于等于公式值。\n编译期间可以检查出很多错误，不过有些问题在编译器是检查不出来的。在运行时创建切片的过程中如果发生了以下错误会直接触发运行时错误并崩溃：\n内存空间的大小发生了溢出； 申请的内存大于最大可分配的内存； 传入的长度小于 0 或者长度大于容量（传入的len是变量时，编译阶段无法判断是否小于cap）； runtime.makeslice 在最后调用的 runtime.mallocgc 是用于申请内存的函数，这个函数的实现比较复杂，如果遇到了比较小的对象会直接初始化在 Go 语言调度器里面的 P 结构中（不只是针对于切片），而大于 32KB 的对象会在堆上初始化。在Go语言的调度器中，对于较小的对象，为了减少内存分配的开销，会将这些对象直接初始化在P的本地内存池中，而不是在堆上分配。这个过程称为\u0026quot;P本地对象缓存\u0026quot;，而对于大于32KB的对象，它们往往会直接在堆上进行初始化，因为对于较大的对象，为它们分配独立的堆内存比较合理，这可以更好地管理内存碎片和利用堆的垃圾回收机制。\n总结一下切片的分配逻辑：\n如果切片容量非常小且没有发生内存逃逸时，它会存储在栈中，其底层数组会根据数组的规则放在栈或者静态存储区中；如果切片容量大或者发生了内存逃逸，此时会判断其申请的内存是否大于32KB，大于则会分配在堆上，小于则会分配到P的本地内存池中。当然这只是简单的理解，实际上还需要考虑到内存管理机制等情况。\n使用下标方式创建切片，其底层数组由开发者自己创建；而字面量和关键字的方式创建底层数组都是由编译期或运行时声明创建。\n访问元素 使用 len 和 cap 获取长度或者容量是切片最常见的操作，编译器会将它们看成两种特殊操作，即 OLEN 和 OCAP，gc.state.expr 函数会在 SSA中间代码生成阶段 将它们分别转换成 OpSliceLen 和 OpSliceCap\n访问切片结构中的字段可能会触发 “decompose builtin” 阶段的优化，len(slice) 或者 cap(slice) 在某些情况下会直接替换成切片的长度或者容量 （v13 (?) = Const64 \u0026lt;int\u0026gt;`` [5]），不需要在运行时获取。\n除了获取切片的长度和容量之外，访问切片中元素使用的 OINDEX 操作也会在中间代码生成期间转换成对地址的直接访问。除此之外，编译期间也会将包含 range 关键字的遍历转换成形式更简单的循环\n测试发现直接替换成切片长度或容量以及访问元素只会出现在切片定义在函数内部且它没有发生内存逃逸的情况，如果发生了内存逃逸或者直接定义在外部，编译期是无法判断它的长度容量以及是否会扩容导致切片移动到其他内存区的（其他函数也可以使用或扩容它）\n追加和扩容 使用 append 关键字向切片中追加元素也是常见的切片操作，中间代码生成阶段的gc.state.append 方法会根据返回值是否会覆盖原变量来选择进入两种流程\n如果 append 返回的新切片不需要赋值回原有的变量（即没有变量接收append后新切片的指针），就会进入如下的处理流程：\n1 2 3 4 5 6 7 8 9 10 11 // append(slice, 1, 2, 3) ptr, len, cap := slice newlen := len + 3 if newlen \u0026gt; cap { ptr, len, cap = growslice(slice, newlen) newlen = len + 3 } *(ptr+len) = 1 *(ptr+len+1) = 2 *(ptr+len+2) = 3 return makeslice(ptr, newlen, cap) 首先会解构切片结构体获取它的数组指针、大小和容量，如果在追加元素后切片的大小大于容量，那么就会调用 runtime.growslice 对切片进行扩容并将旧的元素依次加入切片。\n如果使用 slice = append(slice, 1, 2, 3) 语句，那么 append 后的新切片会覆盖原切片变量，这时 gc.state.append 方法会使用另一种方式展开关键字：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // slice = append(slice, 1, 2, 3) a := \u0026amp;slice ptr, len, cap := slice newlen := len + 3 if uint(newlen) \u0026gt; uint(cap) { newptr, len, newcap = growslice(slice, newlen) vardef(a) *a.cap = newcap *a.ptr = newptr } newlen = len + 3 *a.len = newlen *(ptr+len) = 1 *(ptr+len+1) = 2 *(ptr+len+2) = 3 它与前面逻辑最大的区别在于得到的新切片是否会赋值给原变量，首先会定义一个 a 临时变量，它指向的就是原切片，然后获取原切片的数组指针、大小和容量，如果扩容后的大小大于原cap就会发生扩容并更新原切片的cap和指针为新切片，无论是否发生扩容都会更新len和元素数据。\n如果选择覆盖原有变量就不需要担心切片发生拷贝影响性能，编译期对这种常见情况进行了优化。\n当切片的容量不足时，我们会调用 runtime.growslice 函数为切片扩容，扩容是为切片分配新的内存空间并拷贝原切片中元素的过程，在分配内存空间之前需要先缺点新的切片容量，运行时根据切片的当前容量选择不同的策略进行扩容 (go 1.18版本及以后更新的扩容机制)：\n如果当前容量扩大两倍后依然小于期望容量就会直接使用期望容量； 如果当前切片的长度小于 256 就会将容量翻倍； 如果当前切片的长度大于 256 就会每次以 newcap += (newcap + 3*256) / 4 的方式缓慢增长，直到新容量大于期望容量；之前版本中超过了阈值之后，基本为恒定的1.25倍增长，而现在超过了阈值之后，增长比例是会动态调整的，增长比例会从2倍逐渐向1.25倍靠拢。 但是这仅仅会确定切片的大致容量，下面还需要根据切片中的元素大小进行内存对齐，runtime.roundupsize 函数会将待申请的内存向上取整，取整时会使用 runtime.class_to_size 数组()，使用该数组中的整数可以提高内存的分配效率并减少内存碎片。\n1 var class_to_size = [_NumSizeClasses]uint16{0, 8, 16, 24, 32, 48, 64, ...} 我们申请内存时，内存管理模块会根据这个数组帮我们匹配到足够大且最接近的规格，比如切片要申请40字节，但是没有这个规模的，就会申请到48字节。\n如果计算新容量时发生了内存溢出或者请求的内存超过了上限就会发生panic 退出程序。\n1 2 var arr []int64 arr = append(arr, 1, 2, 3, 4, 5) 当我们执行上述代码时，会触发 runtime.growslice 函数扩容 arr 切片并传入期望的新容量 5，int64占8字节，所以这时期望分配的内存大小为 40 字节；由于没有这个字节规模，考虑到内存对齐，运行时会调用 runtime.roundupsize向上取整内存的大小到 48 字节，所以新切片的容量为 48 / 8 = 6。\n1 2 3 4 5 6 7 8 9 10 11 12 13 // a 底层是一个slice结构体，此时len=4 cap=8，len和cap只会在make和append时发生修改 a := make([]int, 4, 8) // b 是另一个slice结构体，此时len=7 cap=8，由于没有超过cap，不会扩容，ab指向的依然是相同的底层数组 b := append(a, 3, 3, 3) // a根据a扩容，由于a记录的len为4，因此会从第5个数扩容，那么3就会变成4，而不是从第8位开始 // 源码append函数是根据SliceHeader结构的len来进行扩容赋值的 // newlen = len + 3 // *a.len = newlen // *(ptr+len) = 1 // *(ptr+len+1) = 2 // *(ptr+len+2) = 3 a = append(a, 4, 4, 4) println(b) 拷贝切片 当我们使用 copy(a, b) 的形式对切片进行拷贝时，编译期间的 cmd/compile/internal/gc.copyany 也会分两种情况处理拷贝操作，如果当前 copy 不是在运行时调用的，copy(a, b) 会被直接转换成下面的代码 (常用的拷贝情况)：\n1 2 3 4 5 6 7 n := len(a) if n \u0026gt; len(b) { n = len(b) } if a.ptr != b.ptr { memmove(a.ptr, b.ptr, n*sizeof(elem(a))) } 上述代码中的 runtime.memmove 会负责拷贝内存。而如果拷贝是在运行时发生的，例如：go copy(a, b) (根据go关键字的特性可以知道，copy函数会先获取ab的值，执行到时才会调用具体函数)，编译器会使用 runtime.slicecopy 替换运行期间调用的 copy。\n无论是在编译期间拷贝还是运行时拷贝，两种拷贝方式都会通过 runtime.memmove 函数将整块内存的内容拷贝到目标的内存区域中。相比于依次拷贝元素，runtime.memmove 能够提供更好的性能。不过需要注意的是，整块拷贝内存仍然会占用非常多的资源，对大切片进行拷贝操作时一定要注意对性能的影响。\n扩容切片时如果容量不够也会使用 runtime.memmove 将原切片拷贝至扩容后的新切片，然后再依次加入要新增的元素。\n切片的很多功能都是在运行时执行的，无论是初始化切片，还是对切片进行追加和扩容都是需要运行时的支持的，因此在遇见大切片扩容或者拷贝时就可能会发生大规模的内存拷贝，一定要减少类似操作（如提前设置容量避免扩容时容量不够发生切片拷贝），避免影响程序性能。\n哈希表 哈希是除了数组之外最常见的数据结构，它表示的是键值对之间的映射关系。它是计算机科学中最重要的数据结构之一，这不仅因为它 O(1) 的读写性能非常优秀，还因为它提供了键值之间的映射。想要实现一个性能优异的哈希表，需要注意两个关键点 —— 哈希函数和冲突解决方法。\n设计原理 哈希函数 实现哈希表的关键点在于哈希函数的选择，哈希函数的选择在很大程度上能够决定哈希表的读写性能。在理想情况下，哈希函数应该能够将不同键映射到不同的索引上，这要求 哈希函数的输出范围（取)大于输入范围（存) ，但是由于键的数量会远远大于能映射的范围，所以在实际使用时，这个理想的效果是不可能实现的。\n比较实际的方式是让哈希函数的结果能够尽可能的均匀分布，然后通过工程上的手段解决哈希碰撞的问题。哈希函数映射的结果一定要尽可能均匀，结果不均匀的哈希函数会带来更多的哈希冲突以及更差的读写性能。\n如果使用结果分布较为均匀的哈希函数，那么哈希的增删改查的时间复杂度为 O(1)；但是如果哈希函数的结果分布不均匀，那么所有操作的时间复杂度可能会达到 O(n) （查询任何一个值都会遍历一遍map)，由此看来，使用好的哈希函数是至关重要的。\n解决冲突 在通常情况下，哈希函数输入的范围一定会远远大于输出的范围，所以在使用哈希表时一定会遇到冲突，哪怕我们使用了完美的哈希函数，当输入的键足够多也会产生冲突。然而多数的哈希函数都是不够完美的，所以仍然存在发生哈希碰撞的可能，这时就需要一些方法来解决哈希碰撞的问题，常见方法的就是开放寻址法和拉链法。\n需要注意的是，这里提到的哈希碰撞不一定是多个键对应的哈希完全相等，可能是多个哈希的部分相等，例如：两个键对应哈希的前四个字节相同。\n开放寻址法 这种方法的核心思想是 依次 探测和比较 底层数组中的元素 来判断目标键值对是否存在于哈希表中 ，如果我们使用开放寻址法来实现哈希表，那么实现哈希表底层的数据结构就是数组，不过因为数组的长度有限，向哈希表写入 [ author, draven] 这个键值对时会从如下的索引开始遍历：\n1 index := hash(\u0026#34;author\u0026#34;) % array.len 当我们向当前哈希表写入新的数据时，如果发生了冲突，就会将键值对写入到下一个内存为空的位置。如果向下遍历时发现这个key存在就会修改其值。当我们读取数据时，就会先获取键的哈希并取模，如果获取到的索引对应的key与比对的key不相等，就会继续查找后面的元素，直到找到目标元素或者内存为空。\n开放寻址法中对性能影响最大的是 负载因子 ，它是数组中元素的数量与数组大小的比值。随着负载因子的增加，线性探测的平均用时就会逐渐增加（碰撞几率增加导致每次查询需要向后移动查询的次数增加)，这会影响哈希表的读写性能。当负载率超过 70% 之后，哈希表的性能就会急剧下降，而一旦负载率达到 100%，整个哈希表就会完全失效，这时查找和插入任意元素的时间复杂度都是 O(n) ，这时需要遍历数组中的全部元素，所以在实现哈希表时一定要关注负载因子的变化。\n拉链法 与开放寻址法相比，拉链法才是哈希表最常见的实现方法，大多数编程语言都是用拉链法来实现哈希表，它的实现比较开放寻址法稍微复杂一些，但是平均查找的长度也比较短，各个用于存储节点的内存都是动态申请的，可以节省比较多的存储空间（拉链的形式，发生冲突时才会申请内存存储冲突kv)。\n实现拉链法一般会使用数组加上链表，不过一些编程语言会在拉链法的哈希中引入红黑树以优化性能，拉链法会使用链表数组作为哈希底层的数据结构，我们可以将它看成可以扩展的二维数组： 如上图所示，当我们需要将一个键值对 (Key6, Value6) 写入哈希表时，键值对中的键 Key6 都会先经过一个哈希函数，哈希函数返回的哈希会帮助我们选择一个桶，和开放寻址法一样，选择桶的方式是直接对哈希返回的结果取模：\n1 index := hash(\u0026#34;Key6\u0026#34;) % array.len 选择了 2 号桶后就可以遍历当前桶中的链表了，在遍历链表的过程中会遇到以下两种情况：\n找到键相同的键值对 — 更新键对应的值； 没有找到键相同的键值对 — 在链表的末尾追加新的键值对； 如果要在哈希表中获取某个键对应的值，首先取模命中相应的桶，然后依次遍历桶中的链表，遍历到相同key则返回值，如果遍历到链表的末尾也没有找到期望的键，就表示哈希表中没有该键对应的值。\n在一个性能比较好的哈希表中，每一个桶中都应该有 0 ～ 1 个元素，有时会有 2 ~ 3 个，很少会超过这个数量。计算哈希、定位桶和遍历链表三个过程是哈希表读写操作的主要开销，使用拉链法实现的哈希也有装载因子这一概念：\n装载因子 :=元素数量 ÷桶数量\n与开放地址法一样，拉链法的装载因子越大，哈希的读写性能就越差。在一般情况下使用拉链法的哈希表装载因子都不会超过 1，当哈希表的装载因子较大时会触发哈希的扩容，创建更多的桶来存储哈希中的元素，保证性能不会出现严重的下降。如果有 1000 个桶的哈希表存储了 10000 个键值对，它的性能是保存 1000 个键值对的 1/10，但是仍然比在链表中直接读写好 1000 倍 (从1000个桶里面读取10000个数据，最差读取一个数据需要链表查10次，而直接从链表中读取数据，最差需要链表查10000次，每个数据都要查询，因此比链表直接读写好1000 倍)。\n数据结构 Go语言运行时同时使用了\nGo 语言运行时同时使用了多个数据结构组合来表示哈希表，其中 runtime.hmap 是最核心的结构体：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 type hmap struct { count int flags uint8 B uint8 noverflow uint16 hash0 uint32 buckets unsafe.Pointer oldbuckets unsafe.Pointer nevacuate uintptr extra *mapextra } type mapextra struct { overflow *[]*bmap oldoverflow *[]*bmap nextOverflow *bmap } count 表示当前哈希表中的元素数量； B 表示当前哈希表持有的 buckets 数量，但是因为哈希表中桶的数量都 2 的倍数，所以该字段会存储对数，也就是 len(buckets) == 2^B； hash0 是哈希的种子，它能为哈希函数的结果引入随机性，这个值在创建哈希表时确定，并在调用哈希函数时作为参数传入； oldbuckets 是哈希在扩容时用于保存之前 buckets 的字段； noverflow 为map中溢出桶的数量。当溢出桶太多时，map 会进行 same-size map growth，其实质是避免桶过大导致内存泄露。extra 存储map中的溢出桶； nevacuate 表示扩容进度，小于此地址的 buckets 迁移完成 哈希表 runtime.hmap 的桶是 runtime.bmap。每一个 runtime.bmap 都能存储8个键值对，当哈希表中存储的数据过多时，单个桶已经装满就会使用 extra.nextOverflow 中的桶来存储溢出的数据。\n上面两种不同的桶在内存中是连续存储的（即底层是一个数组，正常桶集和排列，然后溢出桶集和排列），它们分别被称为正常桶和溢出桶，溢出桶是在Go语言还使用C语言实现时使用的设计，由于它能够减少扩容的频率所以一直使用至今。\nruntime.bmap 在go源代码中的定义只包含一个简单的 tophash 字段，它存储了键的哈希的高8位，通过比较不同键的哈希的高8位可以减少访问键值对次数以提高性能（从判断键整体的不同变为了先判断高8位，符合再继续判断）。\n1 2 3 type bmap struct { tophash [bucketCnt]uint8 } 随着哈希表存储的数据逐渐增多，我们会扩容哈希表或者使用溢出桶来存储溢出的数据，不会让单个桶中的数据超过8个，不过溢出桶只是临时的解决方案，创建过多的溢出桶最终也会导致哈希的扩容（溢出桶以链表的形式存在，不扩容的话，查询的速率就会变的非常慢）。\n初始化 字面量 现代的编程语言基本都支持使用字面量的方式来初始化哈希，一般都会使用 key: value 的语法来表示键值对，Go 语言中也不例外：\n1 2 3 4 5 hash := map[string]int{ \u0026#34;1\u0026#34;: 2, \u0026#34;3\u0026#34;: 4, \u0026#34;5\u0026#34;: 6, } 我们需要在初始化哈希时声明键值对的类型，这种使用字面量初始化的方式最终都会通过 cmd/compile/internal/gc.maplit 初始化，我们来分析一下该函数初始化哈希的过程：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 func maplit(n *Node, m *Node, init *Nodes) { a := nod(OMAKE, nil, nil) a.Esc = n.Esc a.List.Set2(typenod(n.Type), nodintconst(int64(n.List.Len()))) litas(m, a, init) entries := n.List.Slice() if len(entries) \u0026gt; 25 { ... return } // Build list of var[c] = expr. // Use temporaries so that mapassign1 can have addressable key, elem. ... } 当哈希表中的元素数量少于或者等于25个时，编译器会将字面量初始化的结构体转换成以下的代码，将所有的键值对一次性加入到哈希表中：\n1 2 3 4 hash := make(map[string]int, 3) hash[\u0026#34;1\u0026#34;] = 2 hash[\u0026#34;3\u0026#34;] = 4 hash[\u0026#34;5\u0026#34;] = 6 这种初始化字面量的方式与数组和切片几乎完全相同，由此看来集和类型的初始化在Go 语言中有着相同的处理逻辑。\n但是一旦哈希表中的元素数量超过25个，编译器会创建两个数组分别存储键和值，这些键值对会通过如下所示的for循环加入哈希：\n1 2 3 4 5 6 hash := make(map[string]int, 26) vstatk := []string{\u0026#34;1\u0026#34;, \u0026#34;2\u0026#34;, \u0026#34;3\u0026#34;, ... ， \u0026#34;26\u0026#34;} vstatv := []int{1, 2, 3, ... , 26} for i := 0; i \u0026lt; len(vstak); i++ { hash[vstatk[i]] = vstatv[i] } 这里的 vstatk 和 vstatv 还会根据切片字面量的初始化被编辑器继续展开。无论哈希使用哪种方法，使用字面量初始化的过程都会使用Go 语言中的关键字 make 来创建新的哈希并通过最原始的 [ ] 语法向哈希表中追加元素。\n运行时 当创建的哈希被分配到栈上并且其桶容量小于 BUCKETSIZE = 8 时，Go 语言会在编译阶段使用如下方式快速初始化哈希，这也是编译器对小容量哈希做的优化：\n1 2 3 4 5 6 7 var h *hmap var hv hmap var bv bmap h := \u0026amp;hv b := \u0026amp;bv h.buckets = b h.hash0 = fashtrand0() 除了上述特定的优化之外，无论 make 是从哪里来的，只要我们使用 make 创建哈希，Go 语言编译器都会在类型检查期间将它们转换成 runtime.makemap，使用字面量初始化哈希也只是语言提供的辅助工具，最后调用的都是 runtime.makemap：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 func makemap(t *maptype, hint int, h *hmap) *hmap { mem, overflow := math.MulUintptr(uintptr(hint), t.bucket.size) if overflow || mem \u0026gt; maxAlloc { hint = 0 } if h == nil { h = new(hmap) } h.hash0 = fastrand() B := uint8(0) for overLoadFactor(hint, B) { B++ } h.B = B if h.B != 0 { var nextOverflow *bmap h.buckets, nextOverflow = makeBucketArray(t, h.B, nil) if nextOverflow != nil { h.extra = new(mapextra) h.extra.nextOverflow = nextOverflow } } return h } 这个函数会按照下面的步骤执行：\n计算哈希占用的内存是否会溢出或者超出能分配的最大值； 调用 runtime.fastrand() 获取一个随机的哈希种子； 根据传入的 hint 计算出需要的最小需要的桶的数量； 使用 runtime.makeBucketArray 创建用于保存桶的数组； runtime.makeBucketArray 会根据传入的B 计算出需要创建的桶数量并在内存中分配一片连续的空间用于存储数据。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 func makeBucketArray(t *maptype, b uint8, dirtyalloc unsafe.Pointer) (buckets unsafe.Pointer, nextOverflow *bmap) { base := bucketShift(b) nbuckets := base if b \u0026gt;= 4 { nbuckets += bucketShift(b - 4) sz := t.bucket.size * nbuckets up := roundupsize(sz) if up != sz { nbuckets = up / t.bucket.size } } buckets = newarray(t.bucket, int(nbuckets)) if base != nbuckets { nextOverflow = (*bmap)(add(buckets, base*uintptr(t.bucketsize))) last := (*bmap)(add(buckets, (nbuckets-1)*uintptr(t.bucketsize))) last.setoverflow(t, (*bmap)(buckets)) } return buckets, nextOverflow } 当桶的数量小于 2的四次方时( b =4)，由于数据较少、使用溢出桶的可能性较低，会省略创建溢出桶的过程以减少额外开销； 当桶的数量大于等于 2的四次方时，会额外创建 2的 (b-4) 个溢出桶； 即无论b是否大于4都会创建正常桶，但是如果b 大于等于4则会额外创建多余的溢出桶。正常桶和溢出桶在内存中的空间是连续的，只是被 runtime.hmap 中的不同字段引用，当溢出桶数量较多时则会通过 runtime.newobject 来扩容创建新的溢出桶。\n读写操作 哈希表的访问一般都是通过下标或者遍历进行的：\n1 2 3 4 5 _ = hash[key] for k, v := range hash { // k, v } 这两种方式虽然都能读取哈希表的数据，但是使用的函数和底层原理完全不同，前者需要知道哈希的键并且一次只能获取单个键对应的值，而后者可以遍历哈希中的全部键值对，访问数据时也不需要预先知道哈希的键，不过它的时间复杂度是ON。\n数据结构的写一般指的都是增加、删除和修改，增加和修改字段都是使用索引和赋值语句，而删除字典中的数据需要使用关键字 delete：\n1 2 3 hash[key] = value hash[key] = newValue delete(hash, key) 访问 在编译的类型检查期间，hash[key] 以及类似的操作都会被转换成哈希的 OINDEXMAP 操作，中间代码生成阶段会将这些 OINDEXMAP 操作转换成如下代码：\n1 2 v := hash[key] // =\u0026gt; v := *mapaccess1(maptype, hash, \u0026amp;key) v, ok := hash[key] // =\u0026gt; v, ok := mapaccess2(maptype, hash, \u0026amp;key) 赋值语句左侧接收的参数个数会决定使用的运行时方法：\n当接收一个参数时，会使用 mapaccess1 ，该函数仅会返回一个指向目标值的指针； 当接收两个参数时，会使用 mapaccess2 ，除了返回目标值之外，它还会返回一个用于表示当前键对应的值是否存在的布尔值； mapaccess1 会先通过哈希表设置的哈希函数、哈希种子获取当前key键对应的哈希，再通过 runtime.bucketMask 和 runtime.add 拿到该键值对所在的桶序号和哈希高位的8位数字。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 func mapaccess1(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer { alg := t.key.alg // 获取hash hash := alg.hash(key, uintptr(h.hash0)) // 获取桶序号 m := bucketMask(h.B) b := (*bmap)(add(h.buckets, (hash\u0026amp;m)*uintptr(t.bucketsize))) // 获取高8位 top := tophash(hash) bucketloop: // 依次遍历 b 这个桶链表 for ; b != nil; b = b.overflow(t) { // 遍历单个桶，寻找高8位哈希相同的，然后取值 for i := uintptr(0); i \u0026lt; bucketCnt; i++ { if b.tophash[i] != top { if b.tophash[i] == emptyRest { break bucketloop } continue } k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) if alg.equal(key, k) { v := add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.valuesize)) return v } } } // 返回对应类型的零值 return unsafe.Pointer(\u0026amp;zeroVal[0]) } 在 bucketloop 循环中，哈希会依次遍历对应桶的正常桶和溢出桶中的数据（之前获取的桶序号是数据所在桶的正常桶，而如果正常桶满了会存储到溢出桶中，正常桶中会有记录溢出桶指针的字段，以链表的形式存在，再满就是溢出桶存有指向溢出桶指针的字段），它会先比较 获取的哈希的高8位和桶中存储的 tophash 是否相等，后比较传入的 和桶中的值以加速数据的读写。\n用于选择桶序号的是哈希的最低几位，而用于加速访问的是哈希的高8位，这种设计能够减少同一个桶中有大量相等 tophash的概率影响性能。\n如上图所示，每一个桶都是一整片内存空间，当发现桶中的 tophash 与传入键的 tophash 匹配之后，我们会通过指针和偏移量获取哈希中存储的 key[i] 与key相比较，如果两者相同就会根据指针和偏移量获取对应值的指针并返回，不同则会继续循环，直到遇见相同或者 正常桶和溢出桶都遍历完。\n另一个同样用于访问哈希表中数据的 runtime.mapaccess2 只是在 mapaccess1 的基础上多返回了一个标识 键值对是否存在的布尔值：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func mapaccess2(t *maptype, h *hmap, key unsafe.Pointer) (unsafe.Pointer, bool) { ... bucketloop: for ; b != nil; b = b.overflow(t) { for i := uintptr(0); i \u0026lt; bucketCnt; i++ { ... if alg.equal(key, k) { v := add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.valuesize)) return v, true } } } return unsafe.Pointer(\u0026amp;zeroVal[0]), false } 上面的过程是在正常情况下，访问哈希表中元素时的表现，然而和数组一样，哈希表可能会在负载因子过高或者溢出桶过多时进行扩容，而哈希表扩容并不是原子过程，它是逐步迁移的，后续会介绍在扩容过程中如何保证哈希的访问。\n写入（新增、修改） 当形如 hash[k] 的表达式出现在赋值符号的左侧时，该表达式也会在编译期间转换成 runtime.mapassign 函数的调用，该函数与 runtime.mapaccess1 比较相似，我们将其分成几部分依次分析，首先是函数会根据传入的键拿到对应的哈希和桶：\n1 2 3 4 5 6 7 8 9 10 func mapassign(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer { alg := t.key.alg hash := alg.hash(key, uintptr(h.hash0)) h.flags ^= hashWriting again: bucket := hash \u0026amp; bucketMask(h.B) b := (*bmap)(unsafe.Pointer(uintptr(h.buckets) + bucket*uintptr(t.bucketsize))) top := tophash(hash) 然后通过遍历比较桶中存储的 tophash 和键的哈希，如果找到了相同结果就会返回目标位置的地址。\n其中 inserti 表示目标元素的在桶中的索引（tophash 的位置），insertk 和 val 分别表示键值对的地址，获得目标地址之后会通过算术计算寻址获得键值对 k 和 val：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 var inserti *uint8 var insertk unsafe.Pointer var val unsafe.Pointer bucketloop: for { for i := uintptr(0); i \u0026lt; bucketCnt; i++ { if b.tophash[i] != top { if isEmpty(b.tophash[i]) \u0026amp;\u0026amp; inserti == nil { inserti = \u0026amp;b.tophash[i] insertk = add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) val = add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.valuesize)) } if b.tophash[i] == emptyRest { break bucketloop } continue } k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) if !alg.equal(key, k) { continue } val = add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.valuesize)) goto done } ovf := b.overflow(t) if ovf == nil { break } b = ovf } 上述的 for 循环会依次遍历对应的正常桶和溢出桶中存储的数据，整个过程会分别判断 tophash 是否相等、key 是否相等，遍历结束后会从循环中跳出。\n而如果新增时桶已经满了，哈希就会调用 runtime.hmap.newoverflow 创建新桶或者使用 runtime.hmap 预先 在 noverflow 中创键好的桶来保存数据（然后之前的桶以链表的形式指向新的溢出桶），新建的桶不仅会被追加到已有的全部桶的末尾，还会增加哈希表的 noverflow 计数器。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // 如果inserti为空则说明高8位没有符合这个哈希的键，属于新增操作 if inserti == nil { newb := h.newoverflow(t, b) inserti = \u0026amp;newb.tophash[0] insertk = add(unsafe.Pointer(newb), dataOffset) val = add(insertk, bucketCnt*uintptr(t.keysize)) } typedmemmove(t.key, insertk, key) *inserti = top h.count++ // 修改操作，key存在 done: return val } 如果当前键值对在哈希中不存在，哈希会为新的键值对规划存储的内存地址，通过 runtime.typedmemmove 将键移动到对应的内存空间中并返回键对应值的地址 val。如果当前键值对在哈希表中存在，那么就会直接返回 val 值的目标区域的内存地址（见上面23行 goto done），哈希并不会在 runtime.mapassign 这个运行时函数中将值拷贝到桶中，这个函数只会计算并返回对应值的内存地址，真正的赋值操作是在编译期间插入的（编译期生成插入代码，然后运行时执行）。\n1 2 3 4 5 6 7 8 // 执行mapassign 00018 (+5) CALL runtime.mapassign_fast64(SB) // 24(SP) 返回了val值地址，即让寄存器DI 指向 val 的内存地址 00020 (5) MOVQ 24(SP), DI ;; DI = \u0026amp;value // 将字符串的地址存储到寄存器AX 中 00026 (5) LEAQ go.string.\u0026#34;88\u0026#34;(SB), AX ;; AX = \u0026amp;\u0026#34;88\u0026#34; // 将字符串 \u0026#34;88\u0026#34; 存储到目标地址上 00027 (5) MOVQ AX, (DI) ;; *DI = AX 扩容 前面介绍哈希的写入过程时其实省略了扩容操作，随着哈希表中元素的逐渐增加，哈希的性能也会逐渐恶化，所以我们需要更多的桶和更大的内存来保证哈希的读写性能：\n1 2 3 4 5 6 7 8 func mapassign(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer { ... if !h.growing() \u0026amp;\u0026amp; (overLoadFactor(h.count+1, h.B) || tooManyOverflowBuckets(h.noverflow, h.B)) { hashGrow(t, h) goto again } ... } runtime.mapassign 函数会在以下两种情况发生时触发哈希的扩容：\n负载因子已经超过6.5； 哈希使用了太多的溢出桶； 由于Go语言哈希的扩容不是一个原子的过程，所以还需要判断当前哈希是否已经处于扩容状态 !h.growing()，避免二次扩容造成混乱。\n1 2 3 4 func (h *hmap) growing() bool { // 只有在扩容时，oldbuckets才不会为空 return h.oldbuckets != nil } 根据触发的条件不同，扩容的方式分成两种，如果这次扩容是溢出的桶太多导致的，那么这次扩容就是等量扩容 sameSizeGrow，它是一种特殊情况下发生的扩容，当我们持续向哈希中插入数据然后后续将它们删除时，由于哈希表中的数据量没有超过阈值，但是又因为插入导致积累了过多的溢出桶，数据分布过于分散，就会造成缓慢的内存泄露。因此一旦哈希中出现了过多的溢出桶，就会创建新桶来保存数据，减少内存占用，垃圾回收会清理掉老的溢出桶并释放内存。\n扩容的入口是 runtime.hashGrow：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 func hashGrow(t *maptype, h *hmap) { bigger := uint8(1) if !overLoadFactor(h.count+1, h.B) { bigger = 0 h.flags |= sameSizeGrow } oldbuckets := h.buckets newbuckets, nextOverflow := makeBucketArray(t, h.B+bigger, nil) h.B += bigger h.flags = flags h.oldbuckets = oldbuckets h.buckets = newbuckets h.nevacuate = 0 h.noverflow = 0 h.extra.oldoverflow = h.extra.overflow h.extra.overflow = nil h.extra.nextOverflow = nextOverflow } 哈希在扩容过程中会通过 runtime.makeBucketArray 来创建一组新桶和预创建的溢出桶，随后将原有的桶数组设置到 oldbuckets 上并将新的空桶设置到 buckets 上，溢出桶也使用了相同的逻辑更新。\n我们在 runtime.hashGrow中还看不出来等量扩容和翻倍扩容的太多区别，等量扩容创建的新桶数量和旧桶一样，该函数中只是创建了新的桶，并没有对数据进行拷贝和转移。无论是等量还是翻倍，数据迁移的过程是在 runtime.evacuate中完成的，它会对传入桶中的元素进行再分配。\n如果是翻倍扩容， runtime.evacuate 会将一个旧桶中的数据分流到两个新桶中，所以它会创建两个用于保存分配上下文的 runtime.evacDst 结构体，这两个结构体分别指向了一个新桶。如果是等量扩容，那么旧桶和新桶之间就是一对一的关系，只会初始化一个 runtime.evacDst 也不会进行分流，只会重新排列哈希让其分布紧凑。\n在分流时，只使用哈希函数是不能定位到具体的一个桶，哈希函数只会返回很长的哈希，我们还需要一些方法将哈希映射到具体的桶上面。一般都会使用取模或者位操作来获取桶的编号，加入当前哈希中包含4个桶，那么它的桶掩码就是0b11(3)，使用位操作就会得到3，那么我们就会在3号桶中存储该数据：\n1 0xb72bfae3f3285244c4732ce457cca823bc189e0b \u0026amp; 0b11 #=\u0026gt; 0 如果新的哈希表有8个桶，在大多数情况下，原来经过桶掩码 0b11 结果为3的数据也会因为桶掩码增加了一位变成 0b111 而分流到新的3号和7号桶中，所有数据也都会被 runtime.typememmove 拷贝到目标桶中，两个桶一个拿一半数据，散布均与。\nruntime.evacuate 最后会调用 runtime.advanceEvacuationMark 来增加哈希的 nevacuate 计数器，并在所有的旧桶都迁移分流完成后清空哈希的 oldbuckets 和 oldoverflow。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 func advanceEvacuationMark(h *hmap, t *maptype, newbit uintptr) { h.nevacuate++ stop := h.nevacuate + 1024 if stop \u0026gt; newbit { stop = newbit } for h.nevacuate != stop \u0026amp;\u0026amp; bucketEvacuated(t, h, h.nevacuate) { h.nevacuate++ } if h.nevacuate == newbit { // newbit == # of oldbuckets h.oldbuckets = nil if h.extra != nil { h.extra.oldoverflow = nil } h.flags \u0026amp;^= sameSizeGrow } } 之前在分析哈希表访问函数 runtime.mapaccess1 时其实省略了扩容期间获取键值对的逻辑，当哈希表的 oldbuckets 存在时，会先定位到旧桶并尝试从中获取对应键值对，然后再从新桶中获取键值对。因为旧桶的元素还没有被分流，其中还保存着我们需要使用的数据，所以旧桶会替代新创建的空桶提供数据。\n在 runtime.mapassign 中也省略了一段逻辑，当哈希表正处于扩容状态时，每次向哈希表写入值都会触发 runtime.growWork 增加拷贝哈希表中的内容。即写入数据时会顺便进行将旧桶数据迁移至新桶的工作，当然这个不是一次性迁移全部。\n当然除了写入操作之外，删除操作也会在哈希表扩容期间触发 runtime.growWork，触发的方式和代码与写的逻辑几乎完全相同，都是计算当前值所在的桶，然后对桶元素进行迁移。\n需要注意的是，为了保证元素在map中的唯一性，当一个元素从旧桶迁移到新桶时，它会被从旧桶中删除。而一旦所有的元素都已经成功迁移到新桶中，旧桶就会被销毁，释放内存。这个销毁操作发生在所有数据都成功迁移后。\n简单总结一下哈希表扩容的设计和原理，哈希在存储元素过多时会触发扩容操作，会将桶的数量翻倍，扩容过程不是原子操作的，而是通过 runtime.growWork 增量触发的，避免一次性扩容带来的性能瞬时抖动，在扩容期间访问哈希表时会使用旧桶，向哈希表写入或删除数据时会触发旧桶元素的分流迁移。除了这种正常的扩容之外，为了解决大量写入、删除造成的内存泄露问题，哈希引入了 sameSizeGrow 等量扩容 这一机制，在出现较多溢出桶时会整理哈希的内存减少空间的占用。\n删除 如果想要删除哈希中的元素，就需要使用go 语言中的 delete 关键字，这个关键字的唯一作用就是将某一个键值对从哈希表中删除，无论该键对应的值是否存在，这个内置函数都不会返回任何结果。\n在编译期间，delete 关键字会被转换成操作为 ODELETE 的节点，而 cmd/compile/internal/gc.walkexpr 会将 ODELETE 节点转换成 runtime.mapdelete 函数簇中的一个，包括 runtime.mapdelete 、mapdelete_faststr、mapdelete_fast32、mapdelete_fast64\n这四个函数的实现都是差不多的。哈希表的删除逻辑与写入逻辑很相似，只是触发哈希的删除需要使用关键字 delete，如果在删除期间遇到了哈希表的扩容，就会先分流扩容桶中的元素，分流结束后会找到桶中的目标元素完成键值对的删除工作。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 func mapdelete(t *maptype, h *hmap, key unsafe.Pointer) { ... if h.growing() { growWork(t, h, bucket) } ... search: for ; b != nil; b = b.overflow(t) { for i := uintptr(0); i \u0026lt; bucketCnt; i++ { if b.tophash[i] != top { if b.tophash[i] == emptyRest { break search } continue } k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) k2 := k if !alg.equal(key, k2) { continue } // kv都设置为空 *(*unsafe.Pointer)(k) = nil v := add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.valuesize)) *(*unsafe.Pointer)(v) = nil b.tophash[i] = emptyOne ... } } } 总结 Go 语言使用拉链法来解决哈希碰撞的问题，它的访问、写入和删除等操作都会在编译期间转换成运行时的函数和方法。哈希在每一个桶中存储键对应哈希的前8位，当对哈希进行操作时，这些 tophash 就成为可以帮助哈希快速遍历桶中元素的缓存（储存高8位是为了减少==操作带来的开销。如果高8位不同，它们的值必不相同，否则可能相同，这时候再用==操作判断)。\n哈希表的每个桶都只能存储8个键值对，一旦当前哈希的某个桶超出8个，新的键值对就会存储到哈希的溢出桶中。随着键值对数量的增加，溢出桶的数量和哈希的负载因子也会逐渐升高，负载因子超过6.5就会触发翻倍扩容，而如果删除键值对过多，溢出桶依然存在就会触发等量扩容，使数据更紧凑。元素的再分配过程也是在调用写和删除操作时增量进行的，不属于原子操作，不会造成性能的瞬时巨大抖动。\n哈希由于扩容只有写操作（插入、删除）才会触发迁移桶，当这类操作不频繁时就有可能在比较长时间内都是处在一个比之前内存大2倍的状态，必须所有桶都迁完之后才会一次性释放掉内存，因此如果一个map读非常多，写操作不多时，建议在初始化时分配好容量，避免一直处于扩容状态。\n字符串 字符串是Go 语言中的基础数据类型，虽然字符串往往被看做一个整体，但是它实际上是一片连续的内存空间。字符串是由字符组成的数组，数组会占用一片连续的内存空间，而内存空间存储的字节共同组成了字符串，Go 语言中的字符串只是一个只读的字节数组。\n如果是代码中存在的字符串，编译器会将其标记成只读数据 SRODATA，只读意味着字符串会被分配到只读的内存空间，Go 语言只是不支持直接去修改 string 类型变量的内存空间，仍然可以通过在 string 和 []byte 类型之间反复转换实现修改这一目的（转换为byte切片即可修改，但是此时是申请了一片新的可读写的内存空间)。\nJava、Python 以及很多编程语言的字符串也都是不可变的，这种不可变的特性可以保证我们不会引用到意外发生改变的值，而因为 Go 语言的字符串可以作为哈希的键，所以如果哈希的键是可变的，不仅会增加哈希实现的复杂度，还可能会影响哈希的比较。\n数据结构 字符串在 Go 语言中的接口其实非常简单，它就像一个另类的切片，每一个字符串在运行时都会使用 reflect.StringHeader 表示，其中包含指向字节数组的指针和数组的大小。\n1 2 3 4 type StringHeader struct { Data uintptr Len int } 字符串在内存中占用16字节，首先Data是一个指针，它占用8字节。而Len是一个int类型，它在64位系统上等价于int64，也占用8字节。\n与切片的结构体相比，字符串只少了一个表示容量的 Cap 字段，而正是因为切片在 Go 语言的运行时表示与字符串高度相似，所以我们经常会说字符串是一个只读的切片类型。\n字符串作为只读类型，我们并不会向字符串直接追加元素改变其本身的内存空间，所有在字符串上的写入操作都是通过拷贝实现的。\n解析过程 解析器会在词法分析阶段解析字符串，词法分析阶段会对源文件中的字符串进行切片和分组，将原有无意义的字符流转换成 Token 序列。我们可以使用两种字面量方式在 Go 语言中声明字符串，即双引号和反引号:\n1 2 3 str1 := \u0026#34;this is a string\u0026#34; str2 := `this is another string` 两种不同的声明方式其实也意味着 Go 语言编译器需要能够区分并且正确解析两种不同的字符串格式。解析字符串使用的扫描器 cmd/compile/internal/syntax.scanner 会将输入的字符串转换成 Token 流，stdString 方法是它用来解析使用双引号的标准字符串：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 func (s *scanner) stdString() { s.startLit() for { r := s.getr() // 遇见双引号跳过处理 if r == \u0026#39;\u0026#34;\u0026#39; { break } // 逃逸双引号 if r == \u0026#39;\\\\\u0026#39; { s.escape(\u0026#39;\u0026#34;\u0026#39;) continue } // 不能有换行 if r == \u0026#39;\\n\u0026#39; { s.ungetr() s.error(\u0026#34;newline in string\u0026#34;) break } if r \u0026lt; 0 { s.errh(s.line, s.col, \u0026#34;string not terminated\u0026#34;) break } } s.nlsemi = true s.lit = string(s.stopLit()) s.kind = StringLit s.tok = _Literal } 从这个方法的实现我们能分析出 Go 语言处理标准字符串的逻辑：\n标准字符串使用双引号表示开头和结尾； 标准字符串需要使用反斜杠 \\ 来逃逸双引号； 标准字符串不能出现换行 \\n，隐式换行也不行； 使用反引号声明的原始字符串的解析规则就非常简单了，syntax.scanner.rawString 会将非反引号的所有字符都划分到当前字符串的范围中，所以我们可以使用它支持复杂的多行字符串：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 func (s *scanner) rawString() { s.startLit() for { r := s.getr() if r == \u0026#39;`\u0026#39; { break } if r \u0026lt; 0 { s.errh(s.line, s.col, \u0026#34;string not terminated\u0026#34;) break } } s.nlsemi = true s.lit = string(s.stopLit()) s.kind = StringLit s.tok = _Literal } 无论是标准字符串还是原始字符串都会被标记成 StringLit 并传递到语法分析阶段。\n在语法分析阶段，与字符串相关的表达式都会由 cmd/compile/internal/gc.noder.basicLit方法处理：\n1 2 3 4 5 6 7 8 9 10 11 func (p *noder) basicLit(lit *syntax.BasicLit) Val { switch s := lit.Value; lit.Kind { case syntax.StringLit: // 删除结束换行符 \\r if len(s) \u0026gt; 0 \u0026amp;\u0026amp; s[0] == \u0026#39;`\u0026#39; { s = strings.Replace(s, \u0026#34;\\r\u0026#34;, \u0026#34;\u0026#34;, -1) } u, _ := strconv.Unquote(s) return Val{U: u} } } 无论是 import 语句中包的路径(双引号)、结构体中的字段标签还是表达式中的字符串都会使用这个方法将原生字符串中最后的换行符删除并对字符串 Token 进行 Unquote，也就是去掉字符串两边的引号等无关干扰，还原其本来的面目。\nstrconv.Unquote 处理了很多边界条件导致实现非常复杂，其中不仅包括引号，还包括 UTF-8 等编码的处理逻辑。\n拼接 Go 语言拼接字符串会使用 + 符号，编译器会将该符号对应的 OADD 节点转换成 OADDSTR 类型的节点，随后在 gc.walkexpr 中调用 gc.addstr 函数生成用于拼接字符串的代码，gc.addstr 能帮助我们在编译期间选择合适的函数对字符串进行拼接，该函数会根据要拼接的字符串数量选择不同的逻辑：\n如果小于或者等于 5 个，那么会调用 concatstring{2,3,4,5} (concatstring2、concatstring3这种) 等一系列函数； 如果超过 5 个，那么会选择 runtime.concatstrings 传入一个数组切片； 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 func addstr(n *Node, init *Nodes) *Node { c := n.List.Len() buf := nodnil() args := []*Node{buf} for _, n2 := range n.List.Slice() { args = append(args, conv(n2, types.Types[TSTRING])) } // 拼接函数名 var fn string if c \u0026lt;= 5 { fn = fmt.Sprintf(\u0026#34;concatstring%d\u0026#34;, c) } else { fn = \u0026#34;concatstrings\u0026#34; t := types.NewSlice(types.Types[TSTRING]) slice := nod(OCOMPLIT, nil, typenod(t)) slice.List.Set(args[1:]) args = []*Node{buf, slice} } cat := syslook(fn) r := nod(OCALL, cat, nil) r.List.Set(args) ... return r } 其实无论使用 concatstring{2,3,4,5} 中的哪一个,最终都会调用 runtime.concatstrings，它会先遍历传入的切片参数，再过滤空字符串并计算拼接后的字符串长度。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 func concatstrings(buf *tmpBuf, a []string) string { idx := 0 l := 0 count := 0 for i, x := range a { n := len(x) // 过滤拼接中的空字符串 if n == 0 { continue } l += n count++ idx = i } // 如果count=0说明+两边都是空字符串，直接返回\u0026#34;\u0026#34; if count == 0 { return \u0026#34;\u0026#34; } if count == 1 \u0026amp;\u0026amp; (buf != nil || !stringDataOnStack(a[idx])) { return a[idx] } s, b := rawstringtmp(buf, l) // 拷贝 for _, x := range a { copy(b, x) b = b[len(x):] } return s } 如果非空字符串且数量为 1 (过滤拼接的空字符串后的数量为1) 并且当前的字符串不在栈上，就可以直接返回该字符串，不需要做出额外操作。\n但是在正常情况下，运行时会调用 copy 将输入的多个字符串拷贝到目标字符串所在的内存空间。新的字符串是一片新的内存空间，长度是计算出来的拼接字符串的总和，与原字符串没有任何关联，一旦需要拼接的字符串非常大，拷贝带来的性能损失是无法忽略的。\n类型转换 当我们使用 Go 语言解析和序列化 JSON 等数据格式时，经常需要将数据在 string 和 []byte 之间来回转换，类型转换的开销并没有想象的那么小，我们经常会看到 runtime.slicebytetostring 等函数出现在火焰图中，成为程序的性能热点。\n从字节切片到字符串的转换需要使用 runtime.slicebytetostring 函数，例如: string(bytes)，该函数在函数体中会先处理两种比较常见的情况，也就是长度为0或者1的字节切片，这两种情况处理起来都非常简单：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 func slicebytetostring(buf *tmpBuf, b []byte) (str string) { l := len(b) if l == 0 { return \u0026#34;\u0026#34; } if l == 1 { stringStructOf(\u0026amp;str).str = unsafe.Pointer(\u0026amp;staticbytes[b[0]]) stringStructOf(\u0026amp;str).len = 1 return } var p unsafe.Pointer if buf != nil \u0026amp;\u0026amp; len(b) \u0026lt;= len(buf) { p = unsafe.Pointer(buf) } else { p = mallocgc(uintptr(len(b)), nil, false) } stringStructOf(\u0026amp;str).str = p stringStructOf(\u0026amp;str).len = len(b) memmove(p, (*(*slice)(unsafe.Pointer(\u0026amp;b))).array, uintptr(len(b))) return } 它会先根据传入的缓冲区大小来决定是否需要为新字符串分配一片内存空间(mallocgc)，runtime.stringStructOf 会将传入的字符串指针转换成 runtime.stringStruct 结构指针（字符串底层就是这个结构体，不转换的话无法修改字符串的大小和内容），然后设置结构体持有的字符串指针str和长度len，最后通过 runtime.memove 将原[]byte 中的字节全部复制到新的内存空间中 memove(目标地址,原地址，要复制多少)。\n当我们想要将字符串转换成 []byte 类型时就需要使用 runtime.stringtoslicebyte 函数，该函数的实现非常容易理解：\n1 2 3 4 5 6 7 8 9 10 11 func stringtoslicebyte(buf *tmpBuf, s string) []byte { var b []byte if buf != nil \u0026amp;\u0026amp; len(s) \u0026lt;= len(buf) { *buf = tmpBuf{} b = buf[:len(s)] } else { b = rawbyteslice(len(s)) } copy(b, s) return b } 上述函数会根据是否传入缓冲区做出不同的处理：\n当传入缓冲区时，它会使用传入的缓冲区来拷贝字符串内容 当没有传入缓冲区时，运行时会调用 runtime.rawbyteslice 创建新的字节切片并将字符串内容拷贝过去 字符串和 []byte 中的内容虽然一样，但是字符串的内容是只读的，我们不能通过下标或者其他形式改变其中的数据，而 []byte 中的内容是可以读写的。由于字符串是只读的，所以无论从哪种类型转换到另一种都需要拷贝数据，而内存拷贝的性能损耗会随着字符串和 []byte 长度的增长而增长。\n字符串是 Go 语言中相对来说比较简单的一种数据结构，作为只读的数据类型，我们无法改变其本身的结构，所以在做拼接和类型转换等操作时一定要注意性能的损耗，遇到需要极致性能的场景一定要尽量减少类型转换的次数。\n语言基础 函数调用 无论是系统级编程语言 C 和 Go，还是脚本语言 Ruby 和 Python，这些编程语言在调用函数时往往都使用相同的语法：\n1 somefunction(arg0, arg1) 虽然它们调用函数的语法很相似，但是它们的调用惯例却可能大不相同。\n调用惯例 调用惯例是调用方和被调用方对于参数和返回值传递的约定。\n当我们在 x86_64 的机器上使用 C 语言调用函数时，参数都是通过寄存器和栈传递的，其中：\n六个以及六个以下的参数会按照顺序分别使用 edi、esi、edx、ecx、r8d 和 r9d 六个寄存器传递； 六个以上的参数会使用栈传递（1-6使用寄存器，7及以后使用栈传递），函数的参数会以从右到左的顺序依次存入栈中； 而函数的返回值是通过 eax 寄存器进行传递的，由于只使用了一个寄存器存储返回值，所以 C 语言的函数不能同时返回多个值。\n反观golang是直接使用栈来传递参数和接收返回值的，所以它只需要在栈上多分配一些内存就可以返回多个值。\n总结：C语言和Go语言在设计函数的调用惯例时选择了不同的实现方式。C语言同时使用了寄存器和栈来传递参数，使用寄存器传递返回值；而Go语言只使用栈传递参数和返回值。我们可以对比一下这两种设计的优缺点：\nC 语言的方式能够极大的减少函数调用的额外开销，但是也增加了实现的复杂度（使用寄存器）； CPU 访问栈的开销要比访问寄存器高几十倍； 需要单独处理函数参数过多的情况； Go 语言的方式能够降低实现的复杂度并支持返回多个值，但是牺牲了函数调用的性能； 不需要考虑超过寄存器数量的参数应该如何传递（不使用寄存器来传递参数）； 不需要考虑不同架构上的寄存器差异； 函数入参和出参的内存空间都需要在栈上进行分配； Go 语言使用栈作为参数和返回值传递的方法是综合考虑后的设计，选择这种设计意味着编译器会更加简单、更容易维护，当然也损失了一定的性能。（在golang的新版本中，也优先使用寄存器来传递参数了)\n参数传递 我们先来介绍一下传值和传引用两者的区别：\n值传递：函数调用时会对参数进行拷贝，被调用方和调用方两者持有不相关的两份数据； 引用传递：函数调用时会传递参数的指针，被调用方和调用方两者持有相同的数据，任意一方做出的修改都会影响另一方。 不同语言会选择不同的方式传递参数，Go 语言选择了传值的方式， 无论是传递基本类型、结构体还是指针，都会对传递的参数进行拷贝 。只是区分了值类型和引用类型，传递只有值传递，引用类型就是拷贝它的指针进行传递。接收方收到参数时会对这些参数进行复制；了解到这一点之后，在传递数组或者内存占用非常大的结构体时，我们应该尽量使用指针作为参数类型来避免发生数据拷贝进而影响性能。\n我们可以简单总结出以下几条规则：\n通过堆栈传递参数，入栈的顺序是从右到左（最后的参数就在栈底了)，而参数的计算是从左到右； 函数返回值通过堆栈传递并由调用者预先分配内存空间（调用方会先将返回值压入栈中，然后再从右到左压入参数，紧接着执行调用，被调用方会复制栈中的参数和返回值进行计算)； 调用函数时都是传值，接收方会对入参进行复制再计算； 接口 Go 语言中的接口是一组方法的签名，它是 Go 语言的重要组成部分。使用接口能够让我们写出易于测试的代码。\n概述 在计算机科学中，接口是计算机系统中多个组件共享的边界，不同的组件能够在边界上交换信息。接口的本质是引入一个新的中间层，调用方可以通过接口与具体实现分离，解除上下游的耦合，上层的模块不再需要依赖下层的具体模块，只需要依赖一个约定好的接口。\n这种面向接口的编程方式有着非常强大的生命力，无论是在框架还是操作系统中我们都能够找到接口的身影。可移植操作系统接口（Portable Operating System Interface，POSIX）就是一个典型的例子，它定义了应用程序接口和命令行等标准，为计算机软件带来了可移植性 — 只要操作系统实现了 POSIX，计算机软件就可以直接在不同操作系统上运行。接口封装了具体的函数方法，虽然在不同系统中具体实现不一样，但是上层调用的都是同一个接口，因此可以实现跨平台使用。\n除了解耦合有依赖关系的上下游，接口还能够帮助我们隐藏底层实现，减少关注点。《计算机程序的构造和解释》中有这么一句话：\n代码必须能够被人阅读，只是机器恰好可以执行\n人能够同时处理的信息非常有限，定义良好的接口能够隔离底层的实现，让我们将重点放在当前的代码片段中。SQL 就是接口的一个例子，当我们使用 SQL 语句查询数据时，其实不需要关心底层数据库的具体实现，具体是哪个数据库，我们只在乎 SQL 返回的结果是否符合预期。\n计算机科学中的接口是比较抽象的概念，但是编程语言中接口的概念就更加具体。Go 语言中的接口是一种内置的类型，它定义了一组方法的签名。\n隐式接口 很多面向对象语言都有接口这一概念，Java 中的类必须通过显式的方式声明实现的接口，但是在 Go 语言中接口的实现都是隐式的。\n1 2 3 4 5 public class MyInterfaceImpl implements MyInterface { public void sayHello() { System.out.println(MyInterface.hello); } } Go语言实现接口的方式与Java是完全不同的，在Java中实现接口需要显式地声明接口并实现所有方法，而在Go中只需要实现接口定义的所有方法就隐式地实现了该接口。\n在使用实现了接口的结构体时，我们并不会关心它实现了哪些接口，Go语言只会在传递参数、返回参数以及变量赋值时才会对某个类型是否实现接口进行检查。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 func main() { var rpcErr error = NewRPCError(400, \u0026#34;unknown err\u0026#34;) // typecheck1 err := AsErr(rpcErr) // typecheck2 println(err) } func NewRPCError(code int64, msg string) error { return \u0026amp;RPCError{ // typecheck3 Code: code, Message: msg, } } func AsErr(err error) error { return err } Go语言在编译期间对代码进行类型检查，上述代码总共触发了三次类型检查（RPCError实现了error接口）：\n将 *RPCError 类型的变量赋值给 error 类型的变量 rpcErr； 将动态类型为 *RPCError 的变量 rpcErr 传递给签名中参数类型为 error 的 AsErr 函数； 将 *RPCError 类型的变量从函数签名的返回值类型为 error 的 NewRPCError 函数中返回； 从类型检查的过程来看，编译器仅在需要时才检查类型，类型实现接口时只需要实现接口中的全部方法，不需要像Java等编程语言中一样需要显式声明。\n类型 接口也是 Go 语言中的一种类型，它能够出现在变量的定义、函数的入参和返回值中并对它们进行约束，不过 Go 语言中有两种略微不同的接口，它使用 runtime.iface 表示带有一组方法的接口，使用 runtime.eface 表示不包含任何方法的空接口 interface{}，两种接口虽然都使用 interface 声明，但是由于后者在Go中很常见，所以在实现时使用了特殊的类型。\n需要注意的是，与 C 语言中的 void * 不同，interface{} 类型 不是任意类型 。如果我们将类型转换成了 interface{} 类型，变量在运行期间的类型也会发生变化，获取变量类型时会得到 interface{}。它的动态类型是原类型，但是它的静态类型是一个 runtime.iface或者 eface类型。\n指针和接口 在 Go 语言中同时使用指针和接口时会发生一些让人困惑的问题，接口在定义一组方法时是没有办法对实现的接收者做限制，所以我们会看到某个类型实现接口有两种方式，即以指针做接收者和以结构体本身做接收者。\n因为结构体类型和指针类型是不同的，就像我们不能向一个接受指针的函数传递结构体一样，在实现接口时这两种类型也不能划等号。虽然两种类型不同，但是两种实现不可以同时存在，Go 语言的编译器会在结构体类型和指针类型都实现一个方法时报错 “method redeclared”。\nGo中是没办法给一个结构体定义两个名称相同的方法的，如果不实现接口的话，结构体本身是可以互相调用的，即接收者是指针，但是结构体可以直接调用，反之也行，这属于是Go的语法糖，如果定义两个相同名称的方法，其中逻辑不同，接收者不同，方法名和参数返回值相同，Go在执行时就不知道调用哪个方法了，因为指针可以调用结构体的，而结构体也能调用指针的。\n需要特别注意的是，如果实现了接口，接收者是结构体，那么其指针也同时实现了接口，当使用指针调用接口的方法时，它能够隐式地获取到指向的结构体，但是如果是接收者为指针的方法实现了接口，其结构体并没有实现该接口。\n原因就在于Go语言在传递参数时都是属于值传递，而方法的调用者其实属于这个方法的第0个参数，使用 c.Func 进行方法的调用时是会对c 进行拷贝的，如果方法的接收者是指针时：\n对于调用者 \u0026amp;c 来说，这意味着会拷贝一个新的 \u0026amp;c 指针，这个指针和原来的指针会指向一个相同并且唯一的结构体，所以编译器可以隐式的对变量解引用寻址获取指针指向的结构体； 对于调用者 c 来说，这意味着会拷贝一个全新的结构体，而如果方法的参数是 *c，此时参数的指针是新结构体对应的指针，它没法寻址找到最初调用该方法的结构体。个人理解是在使用接口调用方法时，它会先根据动态类型强转类型，此时就已经是一个新的结构体了，它和原结构体无法关联，而方法的接收者是指针，要求的是原指针，可以修改原结构体中的数据，因此无法通过编译。如果不实现接口的话，调用者是结构体，而接收者是结构体指针类型，go的语法糖会根据接收者是指针将调用者先改为其指针，然后通过值传递来拷贝指针再进行调用。 当我们使用结构体指针实现接口时，只有结构体指针类型的变量才会实现该接口；当我们使用结构体实现接口时，其指针类型和结构体类型都会实现该接口。当然这并不意味着我们应该一律使用结构体实现接口，要考虑结构体所占内存和是否会修改接收者的属性，从结构体转为指针只需要增加 \u0026amp; ，因此这个问题在实际开发中并没有那么重要。\nnil和 non-nil (非空) 我们可以通过一个例子理解 Go 语言的接口类型不是任意类型这一句话，下面的代码在 main 函数中初始化了一个 *TestStruct 类型的变量，由于指针的零值是 nil，所以变量 s 在初始化之后也是 nil：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 package main type TestStruct struct{} func NilOrNot(v interface{}) bool { return v == nil } func main() { var s *TestStruct fmt.Println(s == nil) // #=\u0026gt; true fmt.Println(NilOrNot(s)) // #=\u0026gt; false } $ go run main.go true false 调用 NilOrNot 函数时发生了 隐式的类型转换 ，在类型转换时，*TestStruct 类型会转换成 interface{} 类型，空接口类型中保存了转换前的变量，并且其动态类型是 *TestStruct ，所以转换后的变量与 nil 不相等，因为接口的 nil 除了判断其值是否为nil之外还要判断动态类型是否为nil。\n数据结构 Go 语言根据接口类型是否包含一组方法将接口分成了两类：\n使用 runtime.iface 结构体表示包含方法的接口 使用 runtime.eface 结构体表示不包含任何方法的 interface{} 类型 1 2 3 4 type eface struct { // 16 字节 _type *_type data unsafe.Pointer } 由于 interface{} 类型不包含任何方法，所以它的结构相对来说比较简单，只包含指向底层数据和类型的两个指针。也正是如此，Go语言的任意类型都可以转换成 interface{} （替换其底层数据和类型的两个指针即可）\n1 2 3 4 type iface struct { // 16 字节 tab *itab data unsafe.Pointer } runtime.iface 中有指向原始数据的指针 data和类型结构体指针 runtime.itab\nruntime._type runtime._type 是Go语言类型的运行时表示，其中包含了很多类型的元信息，例如：类型的大小、哈希、对齐以及种类等。\n1 2 3 4 5 6 7 8 9 10 11 12 13 type _type struct { size uintptr ptrdata uintptr hash uint32 tflag tflag align uint8 fieldAlign uint8 kind uint8 equal func(unsafe.Pointer, unsafe.Pointer) bool gcdata *byte str nameOff ptrToThis typeOff } size 字段存储了类型占用的内存空间，为内存空间的分配提供信息； hash 字段能够帮助我们快速确定类型是否相等； equal 字段用于判断当前类型的两个对象是否相等（a==b），该字段是为了减少 Go 语言二进制包大小从 typeAlg 结构体中迁移过来的 runtime.itab runtime.itab 结构体是接口类型的核心组成部分，每一个 itab 都占32字节（在iface中存的是它的指针，占8字节），我们可以将其看成接口类型和具体类型的组合，它们分别用 inter 和 _type 两个字段表示：\n1 2 3 4 5 6 7 8 type itab struct { // 32 字节 inter *interfacetype _type *_type hash uint32 // 使用_保持内存对齐，这样itab变量的第24字节就是 itab.fun数组开始的位置 _ [4]byte fun [1]uintptr } hash 是对 _type.hash 的拷贝，当我们想将 interface 类型转换成具体类型时，可以使用该字段快速判断目标类型和具体类型 runtime._type 是否一致； fun 是一个动态大小的数组，它是一个用于动态派发的虚函数表，存储了一组函数指针。虽然该变量被声明成大小固定的数组，但是在使用时会通过原始指针获取其中的数据，所以 fun 数组中保存的元素数量是不确定的； 类型转换 实现接口时可以使用指针类型也可以使用结构体类型，这两种不同的接口实现方式会导致Go语言编译器生成不同的汇编代码，进而影响最终的处理过程。\n假设我们要把具体类型转换成接口类型并执行接口中的某个方法。\n定义一个接口，并用结构体或指针实现它，当初始化并用接口执行时，两者的汇编代码并不相同，但流程是相似的：\n结构体或指针的初始化（如果初始化的是结构体，该结构体会先放置在栈上；如果初始化的是结构体指针会直接放在堆上，两者最后的表现形式都是数据在堆上，栈上存储它的8字节指针） 赋值触发类型转换（如果是初始化的是指针，它主要就是生成 runtime.itab 并将其指针放于栈上；而如果是结构体的话，会先生成 runtime.itab 并存放指针到栈上，同时会将栈上的结构体拷贝到堆上，形成一个 runtime.iface 结构体） 调用接口的对应方法（调用方法时，Go 语言的编译器会在编译期间将一些需要动态派发的方法调用改写成对目标方法的直接调用，以减少性能的额外开销） 类型断言 1 2 3 4 5 6 7 8 func main() { var c Duck = \u0026amp;Cat{Name: \u0026#34;draven\u0026#34;} switch c.(type) { case *Cat: cat := c.(*Cat) cat.Quack() } } 类型断言会用到 switch-case 关键字，switch 生成的汇编指令会将 case 的目标类型的 hash 与接口变量中的 iface.itab.hash / eface._type.hash 进行比较：\n如果两者相等则意味着接口变量的具体类型是该分支的目标类型，紧接着就会跳转到该分支内处理分支逻辑 如果具体类型不相等则会恢复栈指针并返回调用方 如果不禁用编译器优化，编译器会省略转换为接口的过程，并且在空接口比较时会直接获取具体类型的 runtime._type 来进行判断，优化了接口hash判断 动态派发 动态派发是在运行期间选择具体多态操作（方法或者函数）执行的过程，它是面向对象语言中的常见特性。Go语言虽然不是严格意义上的面向对象语言，但是接口的引入为它带来了动态派发这一特性，调用接口类型的方法时，如果编译期间不能确定接口的类型，Go语言会在运行期间决定具体调用该方法的哪个实现。即当以接口类型的身份进行调用，调用时需要经过运行时的动态派发。\n动态派发的具体流程就是在运行时会从接口变量 itab中获取保存了具体类型方法的指针 tab.fun[0]，然后将存储在 runtime.iface 中的具体类型数据拷贝到栈顶，然后会将方法指针拷贝到寄存器中并通过汇编指令 CALL 触发执行。\n在通过 interface.Func() 的方式调用方法时就会出现动态派发，动态派发会带来额外的开销（因为需要将接口转换成对应的具体类型），这些额外开销在有低延时、高吞吐量需求的服务中是不能被忽视的（但是普通程序对此可以接收）。\n在关闭编译器优化的情况下(-gcflags=-N)，动态派发生成的指令会带来约18%左右的额外性能开销，这些性能开销在一个复杂的系统中不会带来太多的影响。一个项目不可能只使用动态派发，而且如果我们开启编译器优化后，动态派发的额外开销会降低至约5%左右，这对应用性能的整体影响就更小了，而编译器优化是默认开启的，并且一般程序并没有低延时、高吞吐量的需求，所以与使用接口带来的好处相比，动态派发的额外开销往往是可以忽略的。\n需要注意的是使用结构体实现接口带来的开销会大于使用指针实现，甚至有一倍的差距，因为接口转成具体类型会拷贝一次，然后调用方法又会再拷贝一次。动态派发在结构体上的表现非常差，因此尽量避免使用结构体类型来实现接口。\n使用结构体带来的巨大性能差异不只是接口转换的原因，主要是因为 Go 语言在函数调用时是传值的，动态派发的过程只是放大了参数拷贝带来的影响。\n反射 虽然反射在大多数的应用和服务中并不常见，但是很多框架都依赖go语言的反射机制简化代码。因为Go语言的语法元素很少、设计简单，所以它没有特别强的表达能力，但是Go语言的 reflect 包能够弥补它在语法上 reflect.Type 的一些劣势。\nreflect 实现了运行时的反射能力，能够让程序操作不同类型的对象。反射包中有两对非常重要的函数和类型，两个函数分别是：\nreflect.TypeOf 能获取类型信息，与 reflect.Type 对应； reflect.ValueOf 能获取数据的运行时表示，与 reflect.Value 对应； 类型 reflect.Type 是反射包定义的一个接口，我们可以使用 reflect.TypeOf 函数获取任意变量的类型，reflect.Type 接口中定义了一些有趣的方法，MethodByName 可以获取当前类型的对应方法的引用、Implements 可以判断当前类型是否实现了某个接口等等。\n反射包中的 reflect.Value 的类型与 reflect.Type 不同，它被声明成了结构体。这个结构体没有对外暴露的字段，但是提供了获取和写入数据的方法。\n反射包中的所有方法基本都是围绕着 reflect.Type 和 reflect.Value 这两个类型来设计的。\n三大法则 运行时反射是程序在运行期间检查其自身结构的一种方式。反射带来的灵活性是一把双刃剑，它作为一种元编程方式可以减少重复代码，但是过量的使用反射会使程序逻辑变得难以理解并且运行缓慢。\n第一法则：从 interface{} 变量可以反射出反射对象 反射的第一法则是能将Go语言的 interface{} 变量转换成反射对象。当我们执行 reflect.ValueOf(1) 时，虽然看起来是获取了基本类型 int 对应的反射类型，但是由于 reflect.TypeOf、reflect.ValueOf 这两个方法的入参都是 interface{}类型，所以在方法执行的过程中发生了类型转换。\n因为Go语言的函数调用都是值传递的，所以变量会在函数调用时进行类型转换。基本类型 int 会转换成 interface{}类型，这也就是为什么第一条法则是从接口到反射对象。如果我们认为Go语言的类型和反射类型处于两个不同的世界，那么 reflect.TypeOf 和 reflect.ValueOf 就是连接这两个世界的桥梁。\n总而言之，使用 reflect.TypeOf 和 reflect.ValueOf 能够获取Go语言中的变量对应的反射对象。而一旦获取了反射对象。我们就能得到跟当前类型相关的数据和操作，并可以使用这些在运行时获取的结构来执行方法。\n第二法则：从反射对象可以获取 interface{} 变量 反射的第二法则是我们可以从反射对象获取到 interface{}变量。既然能够将接口类型的变量转换成反射对象，那么一定需要其他方法将反射对象还原成接口类型的变量。reflect.Value.Interface 就能完成这项工作。调用 reflect.Value.Interface 方法只能获得 interface{} 类型的变量，如果想要将其还原成最原始的状态还需要经过显式类型转换。\n第三法则：要修改反射对象，其值必须可设置 Go语言反射的最后一条法则\n","date":"2023-06-05T00:00:00Z","permalink":"https://xiaonuoz.github.io/p/go-%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E8%AF%BB%E5%90%8E%E7%90%86%E8%A7%A3/","title":"《Go 语言设计与实现》读后理解"},{"content":"k8s 是一个为 容器化 应用提供集群部署和管理的开源工具，由 Google 开发。\n主要特性：\n高可用，不宕机，自动灾难恢复 灰度更新，不影响业务正常运转 一键回滚到历史版本 方便的伸缩扩展（应用伸缩，机器加减）、提供负载均衡 有一个完善的生态 ","date":"2023-04-09T00:00:00Z","permalink":"https://xiaonuoz.github.io/p/kubernetes-%E5%85%A5%E9%97%A8/","title":"Kubernetes 入门"},{"content":"此文章为学习 《Go语言标准库》的总结性文章\n对于程序员而言，标准库与语言本身同样重要，它好比一个百宝箱，能为各种常见的任务提供完美的解决方案。\n输入输出 (Input/Output) 一般的计算机程序都是输入(input)内容 -\u0026gt; 算法处理 -\u0026gt; 输出 (Output)\nGo语言中，为了方便开发者使用，将IO操作封装在了如下几个包中：\nio 为 IO 原语（I/O primitives）提供基本的接口 io/ioutil 封装了一些实用的 I/O 函数 fmt 实现格式化 I/O，类似 C 语言中的 printf 和 scanf bufio 实现带缓冲I/O IO 库 ReaderFrom 和 WriterTo 接口 用于一次性从某个地方读或写到某个地方去，ioutil.ReadFile其实底层用的也是ReaderFrom方法实现\nSeeker 接口 Seek 方法是用于设置偏移量的，这样可以从某个特定位置开始操作数据流。听起来和 ReaderAt/WriteAt 接口有些类似，不过 Seeker 接口更灵活，可以更好的控制读写数据流的位置。\n1 2 3 type Seeker interface { Seek(offset int64, whence int) (ret int64, err error) } whence 的值，在 io 包中定义了相应的常量，应该使用这些常量\n1 2 3 4 5 const ( SeekStart = 0 // 相对于文件的开头进行偏移 SeekCurrent = 1 // 相对于当前偏移量进行偏移 SeekEnd = 2 // 相对于文件的结尾进行偏移 ) 而原先 os 包中的常量已经被标注为Deprecated\n1 2 3 4 5 6 // Deprecated: Use io.SeekStart, io.SeekCurrent, and io.SeekEnd. const ( SEEK_SET int = 0 // seek relative to the origin of the file SEEK_CUR int = 1 // seek relative to the current offset SEEK_END int = 2 // seek relative to the end ) SectionReader 类型 SectionReader 是一个 struct（没有任何导出的字段），实现了 Read, Seek 和 ReadAt，同时，内嵌了 ReaderAt 接口。结构定义如下：\n1 2 3 4 5 6 type SectionReader struct { r ReaderAt // 该类型最终的 Read/ReadAt 最终都是通过 r 的 ReadAt 实现 base int64 // NewSectionReader 会将 base 设置为 off off int64 // 从 r 中的 off 偏移处开始读取数据 limit int64 // limit - off = SectionReader 流的长度 } 从名称我们可以猜到，该类型读取数据流中部分数据。\n1 func NewSectionReader(r ReaderAt, off int64, n int64) *SectionReader LimitedReader 类型 1 2 3 4 type LimitedReader struct { R Reader // underlying reader，最终的读取操作通过 R.Read 完成 N int64 // max bytes remaining } 从 R 读取数据但将返回的数据总量限制为 N 字节。每调用一次 Read 都将更新 N 来反应新的剩余数量，每读取m字节，n就等于n-m。\n也就是说，你可以读取很多次，但是最多总共只能返回 N 字节数据。\n1 2 3 4 5 6 7 8 content := \u0026#34;This Is LimitReader Example\u0026#34; reader := strings.NewReader(content) limitReader := \u0026amp;io.LimitedReader{R: reader, N: 8} for limitReader.N \u0026gt; 0 { tmp := make([]byte, 2) limitReader.Read(tmp) fmt.Printf(\u0026#34;%s\u0026#34;, tmp) } 可见，通过该类型可以达到 只允许读取一定长度数据 的目的。\n在 io 包中，LimitReader 函数的实现其实就是调用 LimitedReader：\n1 func LimitReader(r Reader, n int64) Reader { return \u0026amp;LimitedReader{r, n} } PipeReader 和 PipeWrite 类型 PipeReader（一个没有任何导出字段[字段全小写]的 struct）是管道的读取端。从管道中读取数据。该方法会堵塞，直到管道写入端开始写入数据或写入端被关闭。如果写入端关闭时带有 error（即调用 CloseWithError 关闭），该Read返回的 err 就是写入端传递的error；否则 err 为 EOF。\nPipeWriter（一个没有任何导出字段的 struct）是管道的写入端。写数据到管道中。该方法会堵塞，直到管道读取端读完所有数据或读取端被关闭。如果读取端关闭时带有 error（即调用 CloseWithError 关闭），该Write返回的 err 就是读取端传递的error；否则 err 为 ErrClosedPipe。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 func main() { // io.Pipe() 用于创建一个同步的内存管道 pipeReader, pipeWriter := io.Pipe() go PipeWrite(pipeWriter) go PipeRead(pipeReader) time.Sleep(30 * time.Second) } func PipeWrite(writer *io.PipeWriter){ data := []byte(\u0026#34;Go语言中文网\u0026#34;) for i := 0; i \u0026lt; 3; i++{ n, err := writer.Write(data) if err != nil{ fmt.Println(err) return } fmt.Printf(\u0026#34;写入字节 %d\\n\u0026#34;,n) } writer.CloseWithError(errors.New(\u0026#34;写入段已关闭\u0026#34;)) } func PipeRead(reader *io.PipeReader){ buf := make([]byte, 128) for{ fmt.Println(\u0026#34;接口端开始阻塞5秒钟...\u0026#34;) time.Sleep(5 * time.Second) fmt.Println(\u0026#34;接收端开始接受\u0026#34;) n, err := reader.Read(buf) if err != nil{ fmt.Println(err) return } fmt.Printf(\u0026#34;收到字节: %d\\n buf内容: %s\\n\u0026#34;,n,buf) } } 有点类似于无缓冲channel\n它将 io.Reader 连接到 io.Writer。一端的读取匹配另一端的写入，直接在这两端之间复制数据；它没有内部缓存。它对于并行调用 Read 和 Write 以及其它函数或 Close 来说都是安全的(一方调用close，另一方就无法再写入或读取)。一旦等待的 I/O 结束，Close 就会完成。并行调用 Read 或并行调用 Write 也同样安全：同种类的调用将按顺序进行控制。\n正因为是同步的，因此不能在一个 goroutine 中进行读和写（会死锁，就像channel一样）。\n另外，对于管道的 close 方法（非 CloseWithError 时），err 会被置为 EOF。\nReadAtLeast 和 ReadFull 函数 ReadAtLeast 函数的签名：\n1 func ReadAtLeast(r Reader, buf []byte, min int) (n int, err error) ReadAtLeast 将 r 读取到 buf 中，直到读了最少 min 个字节为止。它返回复制的字节数，如果读取的字节较少，还会返回一个错误。若没有读取到字节，错误就只是 EOF。如果一个 EOF 发生在读取了少于 min 个字节之后，ReadAtLeast 就会返回 ErrUnexpectedEOF。若 min 大于 buf 的长度，ReadAtLeast 就会返回 ErrShortBuffer。对于返回值，当且仅当 err == nil 时，才有 n \u0026gt;= min。\nReadFull 函数的签名：\n1 func ReadFull(r Reader, buf []byte) (n int, err error) ReadFull 精确地从 r 中将 len(buf) 个字节读取到 buf 中。它返回复制的字节数，如果读取的字节较少，还会返回一个错误。若没有读取到字节，错误就只是 EOF。如果一个 EOF 发生在读取了一些但不是所有的字节后，ReadFull 就会返回 ErrUnexpectedEOF。对于返回值，当且仅当 err == nil 时，才有 n == len(buf)。\n注意该函数和 ReadAtLeast 的区别：ReadFull 将 buf 读满；而 ReadAtLeast 是最少读取 min 个字节。\nMultiReader 和 MultiWriter 函数 这两个函数的定义分别是：\n1 2 func MultiReader(readers ...Reader) Reader func MultiWriter(writers ...Writer) Writer 它们接收多个 Reader 或 Writer，返回一个 Reader 或 Writer。我们可以猜想到这两个函数就是操作多个 Reader 或 Writer 就像操作一个。\nMultiReader 的使用：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 readers := []io.Reader{ strings.NewReader(\u0026#34;from strings reader\u0026#34;), bytes.NewBufferString(\u0026#34;sssfrom bytes buffer\u0026#34;), } reader := io.MultiReader(readers...) data := make([]byte, 0, 128) buf := make([]byte, 15) for { n, err := reader.Read(buf) if err != nil { fmt.Println(err) break } fmt.Println(string(buf[:n])) data = append(data, buf[:n]...) } fmt.Printf(\u0026#34;%s\\n\u0026#34;, data) 输出：\n1 2 3 4 5 6 from strings re ader sssfrom bytes b uffer EOF from strings readersssfrom bytes buffer 代码中首先构造了一个 io.Reader 的 slice，由 strings.Reader 和 bytes.Buffer 两个实例组成，然后通过 MultiReader 得到新的 Reader，循环读取新 Reader 中的内容。从输出结果可以看到，第一次调用 Reader 的 Read 方法获取到的是 slice 中第一个元素的内容(第一个元素读完后才会读第二个元素)……也就是说，MultiReader 只是逻辑上将多个 Reader 组合起来，并不能通过调用一次 Read 方法获取所有 Reader 的内容，而是顺序读取Reader切片中的所有Reader。在所有的 Reader 内容都被读完后，Reader 会返回 EOF。\nMultiWriter 的使用：\n1 2 3 4 5 6 7 8 9 10 11 file, err := os.Create(\u0026#34;tmp.txt\u0026#34;) if err != nil { panic(err) } defer file.Close() writers := []io.Writer{ file, os.Stdout, } writer := io.MultiWriter(writers...) writer.Write([]byte(\u0026#34;Go语言中文网\u0026#34;)) 这段程序执行后先生成 tmp.txt 文件，然后同时在文件和屏幕中都输出：Go语言中文网。这和 Unix 中的 tee 命令类似。\nMultiWriter 可以通过调用一次Write同时向多个I/O写入内容，因为写入可以通过句柄直接从尾部写入；MultiReader 只能顺序读取切片中的每一个元素数据，只有上一个元素读取完后才能读取下一个元素。\nTeeReader函数 函数签名如下：\n1 func TeeReader(r Reader, w Writer) Reader TeeReader 返回一个 Reader，它将从 r 中读到的数据写入 w 中。所有经由它处理的从 r 的读取都匹配于对应的对 w 的写入。它没有内部缓存，当读取 r 中的内容时，会无缓冲的将读取内容写入到 Writer 中。任何在写入时遇到的错误都将作为读取错误返回。\n这种功能的实现其实挺简单，无非是在 Read 完后执行 Write。\n1 2 3 4 5 file, _ := os.OpenFile(\u0026#34;tmp.txt\u0026#34;, os.O_WRONLY|os.O_APPEND, 0666) reader := io.TeeReader(strings.NewReader(\u0026#34;Go语言中文网\u0026#34;), file) reader.Read(make([]byte, 20)) // close() 要在读取完成后再关闭 (不能往已经关闭的file中写内容) file.Close() 可以使用TeeReader来计算文件hash值，通过 TeeReader 和 io.ReadAll() 读取文件后再 sha256 占用的内存来看， TeeReader 占用的内存更少\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 // 创建临时文件 out, err := os.Create(\u0026#34;test.tmp\u0026#34;) if err != nil { log.Fatal(err) } defer out.Close() // 要读取的文件 resp, err := http.Get(\u0026#34;https://golang.google.cn/dl/go1.18.2.src.tar.gz\u0026#34;) if err != nil { log.Fatal(err) } defer resp.Body.Close() h := sha256.New() // 将read和write绑定 tee := io.TeeReader(resp.Body, h) // 读取tee就相当于从resp.Body读取内容，会同时将内容写入h中 io.Copy(out, tee) fmt.Printf(\u0026#34;%x\u0026#34;, h.Sum(nil)) 还可以用此方法实现简易的下载进度条，只需要定义一个结构体struct，并让它实现io.Writer，然后在自己的Write()方法中实现每次读多少字节就增加多少字节数即可。\n1 2 3 Download Started Downloading... 21097206 B complete Download Finished ioutil ioutil这个包在go1.16就被废弃了，它的大部分方法都是直接去调用io或os包，比如 ioutil.ReadAll() 点开源码就会发现它的实现逻辑就是直接调用 io.ReadAll()\n1 2 3 4 // As of Go 1.16, this function simply calls io.ReadAll. func ReadAll(r io.Reader) ([]byte, error) { return io.ReadAll(r) } NopCloser 函数 有时候我们需要传递一个 io.ReadCloser 的实例，而我们现在只有一个 io.Reader 的实例，比如：strings.Reader ，这个时候 NopCloser 就派上用场了。它包装一个io.Reader，返回一个 io.ReadCloser ，而相应的 Close 方法啥也不做，只是返回 nil。\n比如，在标准库 net/http 包中的 NewRequest，接收一个 io.Reader 的 body，而实际上，Request 的 Body 的类型是 io.ReadCloser，因此，代码内部进行了判断，如果传递的 io.Reader 也实现了 io.ReadCloser 接口，则转换，否则通过ioutil.NopCloser 包装转换一下。\nReadFile 和 WriteFile 函数 ReadFile 的实现和ReadAll 类似，不过，ReadFile 会先判断文件的大小，给 bytes.Buffer 一个预定义容量，避免额外分配内存。\n按源码中注释的说法是 FileInfo 不会很精确地得到文件大小。\nWriteFile 将data写入filename文件中，当文件不存在时会根据perm指定的权限进行创建一个,文件存在时会先清空文件内容。\nDiscard 变量 Discard 是一个 io.Writer，对它进行的任何 Write 调用都将无条件成功。 是一个用于丢弃数据的地方。同时，为了优化 io.Copy 到 Discard，避免不必要的工作，实现了 io.ReaderFrom 接口(在io.Copy源码中会尝试将dst转换为io.ReaderForm)。\nfmt 对数值而言，宽度为该数值占用区域的最小宽度，不足的话会用空格进行填充；精度为小数点之后的位数。对字符串而言，精度为输出的最大字符数，如果必要的话会直接截断。\nScan、Scanf、Scanln的区别 Scan和Scanln基本相同，唯一区别是当读取多个变量当时候，遇到换行符Scanln会直接结束，未读到输入值的变量为零值；Scan会等待，将换行符视为一个空格，等待下一个输入，直到输入的值满足参数的个数后再遇到换行符后才会结束。\nScanf适用于完全了解输入格式的场景，可以直接把不需要的部分过滤掉。但是如果输入时格式和设置时不同就无法扫描到。\n1 2 3 4 5 6 7 8 9 10 11 12 13 var s string var a string var b string // 输入//a //b //c 可以正常获取，但是去掉/后，当前和之后的值就无法获取，如//a b //c 打印的值为 1 0 0 fmt.Scanf(\u0026#34;//%s //%s //%s\u0026#34;, \u0026amp;s, \u0026amp;a, \u0026amp;b) // 输入s的值后回车不会结束 fmt.Scan(\u0026amp;s, \u0026amp;a, \u0026amp;b) // 输入s的值后回车会提前结束 fmt.Scanln(\u0026amp;s, \u0026amp;a, \u0026amp;b) fmt.Println(len(s), len(a), len(b)) Println和Print拼接字符串 Println输出时会将两个不定参数之间加上一个空格，如 fmt.Println(\u0026quot;sa\u0026quot;, \u0026quot;111\u0026quot;)输出的结果为 sa 111，而Print输出时不会加空格，即它的结果为 sa111。\n因此Print有拼接字符串的作用，而且参数有int类型时都不需要通过 strconv.Itoa()转换。\nSprint/Fprint同理。\nStringer 接口 Stringer接口的定义如下：\n1 2 3 type Stringer interface { String() string } 根据 Go 语言中实现接口的定义，一个类型只要有 String() string 方法，我们就说它实现了 Stringer 接口。如果格式化输出某种类型的值，只要它实现了 String() 方法，那么就会调用 String() 方法进行处理。因此为了避免无限递归循环，在自己实现的String()方法中只要涉及到fmt包输出的代码，一定不要直接输出方法接收者，因为当打印方法接收者时，它又会去调用String()，然后在String()中又出现了打印，就这样子无限递归下去直到爆栈。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 type people struct { Name string } func (p *people) String() string { // fatal error: stack overflow fmt.Println(p) // 返回值的不定参数必须是没有实现String()的类型，return p.Name 是没问题的，但是如果fmt.Sprintf(\u0026#34;print: %v\u0026#34;, p)就会爆栈 return fmt.Sprintf(\u0026#34;print: %v\u0026#34;, p.Name) } func main() { p := \u0026amp;people{} p.Name = \u0026#34;abc\u0026#34; fmt.Println(p) } Formatter 接口 Formatter 接口的定义如下：\n1 2 3 type Formatter interface { Format(f State, c rune) } 官方文档中关于该接口方法的说明：\nFormatter 接口由带有定制的格式化器的值所实现。 Format 的实现可调用 Sprintf、Printf 或 Fprintf(f) 等函数来生成其输出。\n也就是说，通过实现 Formatter 接口可以做到自定义输出格式（自定义占位符）。\n接着上面的例子，我们为 people增加一个方法：\n1 2 3 4 5 6 7 8 func (p *people) Format(f fmt.State, c rune) { if c == \u0026#39;s\u0026#39; { f.Write([]byte(\u0026#34;good_\u0026#34; + p.Name)) } else { // 没有此句，会导致 fmt.Printf(\u0026#34;%s\u0026#34;, p) 啥也不输出 f.Write([]byte(\u0026#34;not found\u0026#34;)) } } 这样，people便实现了Formatter接口。这时再运行：\n1 2 3 4 p := \u0026amp;people{} p.Name = \u0026#34;abc\u0026#34; fmt.Printf(\u0026#34;11 %s 22\\n\u0026#34;, p) fmt.Printf(\u0026#34;11 %d 22\\n\u0026#34;, p) 输出为：\n1 2 11 good_abc 22 11 not found 22 这里需要解释以下几点：\nfmt.State 是一个接口。由于 Format 方法是被 fmt 包调用的，它内部自己会实例化好一个 fmt.State 接口的实例，我们不需要关心该接口； 可以实现自定义占位符，同时 fmt 包中和类型相对应的预定义占位符都会无效。而且其他占位符都会走else路线，去掉else就会发现只要不符合if条件就都会不再输出内容。 实现了 Formatter 接口后，相应的 Stringer 接口就不再起作用了。但实现了 Formatter 接口的类型应该实现 Stringer 接口，这样方便在 Format 方法中调用 String() 方法。 Format 方法的第二个参数是占位符中%后的字母（有精度和宽度会被忽略，只保留字母）； 一般地，我们不需要实现 Formatter 接口。通常可以将其作为一个中间件使用，比如我们有一个结构体，需要序列化后打印，如果不需要打印就不需要进行序列化，这种情况就可以实现Formatter接口，在方法内部进行序列化，这样子就可以实现只在打印时才进行序列化的需求。\nState接口相关说明：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 type State interface { // Write is the function to call to emit formatted output to be printed. // Write 函数用于打印出已格式化的输出。 Write(b []byte) (ret int, err error) // Width returns the value of the width option and whether it has been set. // Width 返回宽度选项的值以及它是否已被设置。 Width() (wid int, ok bool) // Precision returns the value of the precision option and whether it has been set. // Precision 返回精度选项的值以及它是否已被设置。 Precision() (prec int, ok bool) // Flag returns whether the flag c, a character, has been set. // Flag 返回标记 c（一个字符）是否已被设置。 Flag(c int) bool } GoStringer 接口 该接口定义了类型的Go语法格式。专门用于打印(Printf)格式化占位符为 %#v 时的值 (%+v %v 都不适配)。\n1 2 3 func (this *Person) GoString() string { return \u0026#34;\u0026amp;Person{Name is \u0026#34;+this.Name+\u0026#34;, Age is \u0026#34;+strconv.Itoa(this.Age)+\u0026#34;, Sex is \u0026#34;+strconv.Itoa(this.Sex)+\u0026#34;}\u0026#34; } 这个时候再执行\n1 2 p := \u0026amp;Person{\u0026#34;polaris\u0026#34;, 28, 0} fmt.Printf(\u0026#34;%#v\u0026#34;, p) 输出：\n1 \u0026amp;Person{Name is polaris, Age is 28, Sex is 0} 一般的，我们不需要实现该接口。\nbufio bufio 包实现了缓存IO。它包装了 io.Reader 和 io.Writer 对象，创建了另外的Reader和Writer对象，它们也实现了 io.Reader 和 io.Writer 接口，不过它们是有缓存的。\nReader 类型和方法 bufio.Reader 结构包装了一个 io.Reader 对象，提供缓存功能，同时实现了 io.Reader 接口。\nReader 结构没有任何导出的字段，结构定义如下：\n1 2 3 4 5 6 7 8 9 10 type Reader struct { buf []byte // 缓存 rd io.Reader // 底层的io.Reader // r:从buf中读走的字节（偏移）；w:buf中填充内容的偏移； // w - r 是buf中可被读的长度（缓存数据的大小），也是Buffered()方法的返回值 r, w int err error // 读过程中遇到的错误 lastByte int // 最后一次读到的字节（ReadByte/UnreadByte) lastRuneSize int // 最后一次读到的Rune的大小 (ReadRune/UnreadRune) } bufio 包提供了两个实例化 bufio.Reader 对象的函数：NewReader 和 NewReaderSize。其中，NewReader 函数是调用 NewReaderSize 函数实现的：\n1 2 3 4 func NewReader(rd io.Reader) *Reader { // 默认缓存大小：defaultBufSize=4096 return NewReaderSize(rd, defaultBufSize) } ReadSlice、ReadBytes、ReadString 和 ReadLine 方法 ReadSlice方法签名如下：\n1 func (b *Reader) ReadSlice(delim byte) (line []byte, err error) ReadSlice 从输入中读取内容，直到遇到第一个界定符（delim）为止，返回一个指向缓存中字节的 slice 指针，在下次调用读操作（read）时，slice内容会发生变化。举例说明：\n1 2 3 4 5 6 7 reader := bufio.NewReader(strings.NewReader(\u0026#34;http://studygolang.com. \\nIt is the home of gophers\u0026#34;)) line, _ := reader.ReadSlice(\u0026#39;\\n\u0026#39;) fmt.Printf(\u0026#34;the line:%s\\n\u0026#34;, line) // 这里可以换上任意的 bufio 的 Read/Write 操作 n, _ := reader.ReadSlice(\u0026#39;\\n\u0026#39;) fmt.Printf(\u0026#34;the line:%s\\n\u0026#34;, line) fmt.Println(string(n)) 输出：\n1 2 3 4 the line:http://studygolang.com. the line:It is the home of gophers It is the home of gophers 从结果可以看出，第一次ReadSlice的结果（line），在第二次调用读操作后，内容发生了变化。也就是说，ReadSlice 返回的 []byte 是指向 Reader 中的 buffer ，而不是 copy 一份返回。正因为ReadSlice 返回的数据会被下次的 I/O 操作重写，因此许多的客户端会选择使用 ReadBytes 或者 ReadString 来代替。这两个方法就不会出现第二次读取会覆盖第一次的结果。\n注意，这里的界定符可以是任意的字符，可以将上面代码中的\u0026rsquo;\\n\u0026rsquo;改为\u0026rsquo;m\u0026rsquo;试试。同时，返回的结果是包含界定符本身的，上例中，输出结果有一空行就是\u0026rsquo;\\n\u0026rsquo;本身(line携带一个\u0026rsquo;\\n\u0026rsquo;,printf又追加了一个\u0026rsquo;\\n\u0026rsquo;)。\n如果 ReadSlice 在找到界定符之前遇到了 error ，它就会返回缓存中所读到的数据和错误本身（经常是 io.EOF）。\n如果在找到界定符之前缓存已经满了，ReadSlice 会返回 bufio.ErrBufferFull 错误。\n当且仅当返回的结果（line）没有以界定符结束的时候，ReadSlice 返回err != nil，也就是说，如果ReadSlice 返回的结果 line 不是以界定符 delim 结尾，那么返回的 err 就一定 不等于 nil（可能是bufio.ErrBufferFull或io.EOF）。\n上面关于错误的逻辑 ReadBytes ReadSting也是同理。\n通过源码可以发现，ReadSlice() 中返回的是b.buf切片的某一段，即返回的 []byte 和 Reader 中的 buffer共用同一个底层数组；而ReadBytes() 是 copy 一份返回，它和buf已经没关系了；ReadString()则是通过string强转了b.buf，它会在内存开辟新空间。也正因为如此，通常我们会使用 ReadBytes 或 ReadString，它们不会影响到buf的内容。\nReadLine ReadLine 是一个底层的原始行读取命令。效果和 ReadBytes(\u0026rsquo;\\n\u0026rsquo;) 或者 ReadString(\u0026rsquo;\\n\u0026rsquo;) 相当，但是它会去掉换行符，而这两个方法会将换行符保留。\nReadLine 尝试返回单独的行，但不包括行尾的换行符。如果一行大于缓存，isPrefix 会被设置为 true，同时返回该行已经读取的内容（等于缓存大小的部分）。该行剩余的部分就会在下次调用的时候返回。当下次调用返回该行剩余部分时，isPrefix 将会是 false 。由于它底层调用的是 ReadSlice()，因此返回的 line 只是 buffer 的引用，每次读取line的值都会发生改变。\n注意，返回值中，要么 line 不是 nil，要么 err 非 nil，两者不会同时非 nil。\nReadLine 返回的文本不会包含行结尾（\u0026quot;\\r\\n\u0026quot;或者\u0026quot;\\n\u0026quot;）。如果输入中没有行尾标识符，它也不会返回任何指示或者错误。\n从上面的讲解中，我们知道，读取一行，通常会选择 ReadBytes 或 ReadString。不过，正常人的思维，应该用 ReadLine，只是不明白为啥 ReadLine 的实现不是通过 ReadBytes，然后清除掉行尾的\\n（或\\r\\n），它现在的实现，用不好会出现意想不到的问题 (底层使用ReadSlice导致buf改变时，获取的值也会发生改变)，比如丢数据。建议可以这么实现读取一行：\n1 2 line, err := reader.ReadBytes(\u0026#39;\\n\u0026#39;) line = bytes.TrimRight(line, \u0026#34;\\r\\n\u0026#34;) 这样既读取了一行，也去掉了行尾结束符。\nPeek 方法 该方法只是“窥探”一下 Reader 中没有读取的 n 个字节。好比栈数据结构中的取栈顶元素，但不出栈。意思就是它会读取Reader中尚未被读取的n个字节但并不会导致偏移量发生移动。\n1 2 3 4 5 6 7 8 9 10 11 12 13 reader := bufio.NewReader(strings.NewReader(\u0026#34;http://studygolang.com. \\n It is the home of gophers\u0026#34;)) s, _ := reader.Peek(20) fmt.Println(string(s)) reader.ReadBytes(\u0026#39;\\n\u0026#39;) s1, _ := reader.Peek(20) fmt.Println(string(s1)) s2, _ := reader.Peek(20) fmt.Println(string(s2)) // result: // http://studygolang.c // It is the home of g // It is the home of g 观察源码可以发现，它返回的 []byte 只是 buffer 中的引用，在下次IO操作后结果会发生改变，可见该方法对多 goroutine 是不安全的，也就是在多并发环境下，不能依赖其结果。\nPeek 方法如果返回的 []byte 长度小于 n，这时返回的 err != nil ，用于解释为啥会小于 n。如果 Peek 的 n 大于 reader 设定的 buffer 长度，err 会是 ErrBufferFull。\nScanner 类型 对于简单的读取一行，在 Reader 类型中，感觉没有让人特别满意的方法。于是，Go1.1增加了一个类型：Scanner。\n1 2 3 4 5 6 7 8 9 10 11 12 const input = \u0026#34;This is The Golang Standard Library.\\nWelcome you!\u0026#34; scanner := bufio.NewScanner(strings.NewReader(input)) scanner.Split(bufio.ScanWords) count := 0 for scanner.Scan() { count++ } // 读取途中报错会结束for循环，并将错误传递出来 if err := scanner.Err(); err != nil { fmt.Fprintln(os.Stderr, \u0026#34;reading input:\u0026#34;, err) } fmt.Println(count) SplitFunc 类型 1 type SplitFunc func(data []byte, atEOF bool) (advance int, token []byte, err error) SplitFunc 定义了 用于对输入进行分词的 split 函数的签名。参数 data 是还未处理的数据，atEOF 标识 Reader 是否还有更多数据，有则为true（是否到了EOF）。返回值 advance 表示从输入中读取的字节数，token 表示下一个结果数据，err 则代表可能的错误。\n1 \u0026#34;studygolang\\tpolaris\\tgolangchina\u0026#34;，通过\u0026#34;\\t\u0026#34;进行分词，那么会得到三个token，它们的内容分别是：studygolang、polaris 和 golangchina。而 SplitFunc 的功能是：进行分词，并返回未处理的数据中第一个 token。对于这个数据，就是返回 studygolang。 如果 data 中没有一个完整的 token，例如，在扫描行（scanning lines）时没有换行符，SplitFunc 会返回(0,nil,nil)通知 Scanner 读取更多数据到 slice 中，然后在这个更大的 slice 中 同样的读取点处，从输入中重试读取。\n如果 err != nil，扫描停止，同时该错误会返回。\n如果参数 data 为空的 slice，除非 atEOF 为 true，否则该函数永远不会被调用。如果 atEOF 为 true，这时 data 可以非空，这时的数据是没有处理的。\n在 bufio 包中预定义了一些 split 函数，也就是说，在 Scanner 结构中的 split 字段，可以通过这些预定义的 split 进行赋值，同时 Scanner 类型的 Split 方法也可以接收这些预定义函数作为参数。可以说，这些预定义 split 函数都是 SplitFunc 类型的实例。这些函数包括：ScanBytes、ScanRunes、ScanWords 和 ScanLines。\nScanBytes 返回单个字节作为一个 token。 ScanRunes 返回单个 UTF-8 编码的 rune 作为一个 token。返回的 rune 序列（token）和 range string类型 返回的序列是等价的，也就是说，对于无效的 UTF-8 编码会解释为 U+FFFD = \u0026ldquo;\\xef\\xbf\\xbd\u0026rdquo;。 ScanWords 返回通过“空格”分词的单词。如：study golang，调用会返回study。注意，这里的“空格”是 unicode.IsSpace()，即包括：\u0026rsquo;\\t\u0026rsquo;, \u0026lsquo;\\n\u0026rsquo;, \u0026lsquo;\\v\u0026rsquo;, \u0026lsquo;\\f\u0026rsquo;, \u0026lsquo;\\r\u0026rsquo;, \u0026rsquo; \u0026lsquo;, U+0085 (NEL), U+00A0 (NBSP)。 ScanLines 返回一行文本，不包括行尾的换行符。这里的换行包括了Windows下的\u0026quot;\\r\\n\u0026quot;和Unix下的\u0026quot;\\n\u0026quot;。(默认实例化Scanner用的就是这个策略) 一般地，我们不会单独使用这些函数，而是提供给 Scanner 实例使用。可以使用上面介绍的预定义 SplitFunc 实例赋值，也可以自定义 SplitFunc 实例。（要给 split 字段赋值，必须调用 Scanner 的 Split 方法）\nScan 方法 该方法好比 iterator 中的 Next 方法，它用于让 Scanner 获取下一个 token，以便 Bytes 和 Text 方法可用。当扫描停止时，它返回false，这时候，要么是到了输入的末尾要么是遇到了一个错误。注意，当 Scan 返回 false 时，通过 Err 方法可以获取第一个遇到的错误 （但如果错误是 io.EOF，Err 方法会返回 nil）。\n用法 我们经常会有这样的需求：读取文件中的数据，一次读取一行（或者其他读取策略）。在学习了 Reader 类型，我们可以使用它的 ReadBytes 或 ReadString来实现，甚至使用 ReadLine 来实现。但是Scanner比这些方法更简单好用。\n1 2 3 4 5 6 7 8 9 10 11 12 file, err := os.Open(\u0026#34;tmp.txt\u0026#34;) if err != nil { log.Fatalln(err) } defer file.Close() scanner := bufio.NewScanner(file) for scanner.Scan() { fmt.Println(scanner.Text()) // Println will add back the final \u0026#39;\\n\u0026#39; } if err := scanner.Err(); err != nil { fmt.Fprintln(os.Stderr, \u0026#34;reading standard input:\u0026#34;, err) } Writer 类型和方法 相比 bufio.Reader, bufio.Writer 结构定义简单很多。\n1 2 3 4 5 6 type Writer struct { err error // 写过程中遇到的错误 buf []byte // 缓存 n int // 当前缓存中的字节数 wr io.Writer // 底层的 io.Writer 对象 } 如果在写数据到 Writer 的时候出现了一个错误，就不会再允许有数据被写进来了，并且所有随后的写操作都会返回该错误。\n和 Reader 类型一样，bufio 包提供了两个实例化 bufio.Writer 对象的函数：NewWriter 和 NewWriterSize。其中，NewWriter 函数是调用 NewWriterSize 函数实现的。\n方法 Available 方法获取缓存中还未使用的字节数（缓存大小 - 字段 n 的值）\nBuffered 方法获取写入当前缓存中的字节数（字段 n 的值）\nFlush 方法将缓存中的所有数据写入底层的 io.Writer 对象中。使用 bufio.Writer 时，在所有的 Write 操作完成之后，应该调用 Flush 方法使得缓存都写入 io.Writer 对象中。\nWriter 类型其他方法是一些实际的写方法，这些写方法在缓存满时都会调用Flush() 方法，但是defer buf.Flush() 不能省略，因为需要在业务函数结束时将buffer中残留且未存满的缓存写入io.Writer中。\n另外，这些写方法源码开始处，都有这样的代码：\n1 2 3 if b.err != nil { return b.err } 也就是说，只要是在写的过程中遇到了错误，再次调用任何写操作都会直接返回该错误。\n数据结构与算法 sort 排序算法 该包实现了四种基本排序算法：插入排序、归并排序、堆排序和快速排序。 但是这四种排序方法是不公开的，它们只被用于 sort 包内部使用。所以在对数据集合排序时不必考虑应当选择哪一种排序方法，只要实现了 sort.Interface 定义的三个方法：获取数据集合长度的 Len() 方法、比较两个元素大小的 Less() 方法和交换两个元素位置的 Swap() 方法，就可以顺利对数据集合进行排序。sort 包会根据实际数据自动选择高效的排序算法。 除此之外，为了方便对常用数据类型的操作，sort 包提供了对[]int 切片、[]float64 切片和[]string 切片完整支持\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 //Len() func (s StuScores) Len() int { return len(s) } //Less(): 将从低到高排序，因为这里如果s.i\u0026lt;s.j返回的是true，则不会进行swap操作，因此可以得出升序的结果（前一个比后一个小不交换） func (s StuScores) Less(i, j int) bool { return s[i].score \u0026lt; s[j].score } //Swap() func (s StuScores) Swap(i, j int) { s[i], s[j] = s[j], s[i] } 此外，如果临时想进行降序操作时，sort 包提供了 Reverse() 方法，可以允许将数据按 Less() 定义的排序方式逆序排序，而不必修改 Less() 代码。方法定义如下：\n1 2 3 // 进行逆转操作 (重写Less方法),然后进行排序，因为切片是引用类型，因此此处逆转然后排序的其实就是之前的切片 sort.Sort(sort.Reverse(stus)) fmt.Println(stus) 整个 Reverse() 的内部实现比较有趣：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // 定义了一个 reverse 结构类型，嵌入 Interface 接口 type reverse struct { Interface } // reverse 结构类型的 Less() 方法拥有嵌入的 Less() 方法相反的行为 //Len() 和 Swap() 方法则会保持嵌入类型的方法行为 func (r reverse) Less(i, j int) bool { return r.Interface.Less(j, i) } // 返回新的实现 Interface 接口的数据类型 func Reverse(data Interface) Interface { return \u0026amp;reverse{data} } 如果想要查询数组中符合条件的第一个元素时，可以使用Search() 方法：\n1 2 3 sort.Search(len(stus), func(i int) bool { return stus[i].score \u0026gt;= 91 }) 该方法会使用“二分查找”算法来找出能使 f(x) 在(0\u0026lt;=x\u0026lt;n) 索引返回 ture 的最小值 i。 前提条件 : f(x)(0\u0026lt;=x\u0026lt;i) 均返回 false, f(x)(i\u0026lt;=x\u0026lt;n) 均返回 ture。 如果不存在 i 可以使 f(i) 返回 ture, 则返回 n。\n它会返回符合条件的第一个元素的索引。大于等于时会找到和条件相等的元素，即等于91的那个元素，没有比这个元素排在更前面了；大于时则会找到升序数组中第一个大于91这个值的元素，返回其索引。\n当排序为升序，则Search的条件必须为 大于，这是因为升序由小到大，如果是小于的话，那么第一个就会小于设置的值，第一个都不小于，数组就没有小于的了(因为升序排序第一个值最小)。当数组是降序时，Search就必须为 小于\n[]interface 排序与查找 只要实现了 sort.Interface 接口，即可通过 sort 包内的函数完成排序，但是这种用法对于其它数据类型的 slice 不友好，可能我们需要为大量的 struct 定义一个单独的 []struct 类型，再为其实现 sort.Interface 接口。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 type Person struct { Name string Age int } type Persons []Person func (p Persons) Len() int { panic(\u0026#34;implement me\u0026#34;) } func (p Persons) Less(i, j int) bool { panic(\u0026#34;implement me\u0026#34;) } func (p Persons) Swap(i, j int) { panic(\u0026#34;implement me\u0026#34;) } sort.Slice 1 sort.Slice(people, func(i, j int) bool { return people[i].Age \u0026lt; people[j].Age }) // 按年龄升序排序 sort.SliceStable 该函数完成 []interface 的稳定排序，其性能会比sort.Slice略低，通常情况下用Slice() 即可\n1 sort.SliceStable(people, func(i, j int) bool { return people[i].Age \u0026gt; people[j].Age }) // 按年龄降序排序 sort.SliceIsSorted 该函数判断 []interface 是否为有序\n1 sort.SliceIsSorted(people,func(i, j int) bool { return people[i].Age \u0026gt; people[j].Age }) 判断有序的前提是已经按对应逻辑排好序了，如排序方式为降序，那么上面的返回值则是false，需要将大于变为小于符号，因为SliceIsSorted中是判断该slice是否是升序排序。\nsort.Search 该函数判断 []interface 是否存在指定元素 （或返回最接近的元素），其只能用于已经排好序的数组切片。\n1 2 sort.Slice(a, func(i, j int) bool { return a[i] \u0026lt; a[j] }) // 升序排序 index := sort.Search(len(a), func(i int) bool { return a[i] \u0026gt;= x }) // 查找元素 container 容器数据类型：heap、list 和 ring 该包实现了三个复杂的数据结构：堆、链表和环，使用这个包就意味着你使用这三个数据结构的时候不需要再费心从头开始写算法了。\n堆 这里的堆使用的数据结构是最小二叉树，即根节点比左边子树和右边子树的所有值都小。\n1 2 3 4 5 type Interface interface { sort.Interface Push(x interface{}) // add x as element Len() Pop() interface{} // remove and return element Len() - 1. } 可以看出，这个堆结构继承自 sort.Interface。需要实现五个方法。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 type IntHeap []int func (h IntHeap) Len() int { return len(h) } func (h IntHeap) Less(i, j int) bool { return h[i] \u0026lt; h[j] } func (h IntHeap) Swap(i, j int) { h[i], h[j] = h[j], h[i] } func (h *IntHeap) Push(x interface{}) { *h = append(*h, x.(int)) } func (h *IntHeap) Pop() interface{} { old := *h n := len(old) x := old[n-1] *h = old[0 : n-1] return x } 使用堆时就需要使用 container/heap 包\n1 2 3 4 h := \u0026amp;IntHeap{2, 1, 5} heap.Init(h) heap.Push(h, 3) heap.Pop(h) 具体说下内部实现，是使用最小堆，索引排序从根节点开始，然后左子树，右子树的顺序方式。索引布局如下：\n1 2 3 4 0 1 2 3 4 5 6 7 8 9 10 11 假设 (heap[1]== 小明 ) 它的左子树 (heap[3]== 小黑 ) 和右子树 (heap[4]== 大黄 ) 且小明 \u0026gt; 小黑 \u0026gt; 大黄 ;\n堆内部实现了 down 和 up 函数 : down 函数用于将索引 i 处存储的值 ( 设 i=1, 即小明 ) 与它的左子树 ( 小黑 ) 和右子树 ( 大黄 ) 相比 , 将三者最小的值 (小明大于小黄) 大黄与小明的位置交换，交换后小明继续与交换后的子树 (heap[9]和 heap[10]) 相比，重复以上步骤，直到小明位置不再发生改变。\nup 函数用于将索引 i 处的值 ( 设 i=3, 即小黑 ) 与他的父节点 ( 小明 ) 比较，将两者较小的值放到父节点上，本例中即交换小黑和小明的位置，之后小黑继续与其父节点比较，重复以上步骤，直到小黑位置不再发生改变。\n从堆中pop时,每次pop的都是堆顶元素,即最小或最大的那个元素，假设 heap[11]== 阿花，当从堆中 Pop 一个元素的时候，会先把堆顶元素和最后一个节点的值 ( 阿花 ) 交换，然后弹出最后一个元素 (交换后的堆顶元素)，这样可以保证堆的性质不被破坏。然后对堆顶的阿花调用 down，将新的堆顶元素下沉，直到满足最小堆的性质。\n当往堆中 Push 一个元素的时候，这个元素插入到最后一个节点，本例中为 heap[12]，即作为 heap[5]的右子树，然后调用 up 函数向上比较。\n链表 链表就是一个有 prev 和 next 指针的数组了。\n1 2 3 4 5 6 7 8 9 10 type Element struct { next, prev *Element // 上一个元素和下一个元素 list *List // 元素所在链表 Value interface{} // 元素 } type List struct { root Element // 链表的根元素 len int // 链表的长度 } 基本使用是先创建 list，然后往 list 中插入值，list 就内部创建一个 Element，并内部设置好 Element 的 next,prev 等。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 package main import ( \u0026#34;container/list\u0026#34; \u0026#34;fmt\u0026#34; ) func main() { list := list.New() list.PushBack(1) list.PushBack(2) fmt.Printf(\u0026#34;len: %v\\n\u0026#34;, list.Len()) fmt.Printf(\u0026#34;first: %#v\\n\u0026#34;, list.Front()) fmt.Printf(\u0026#34;second: %#v\\n\u0026#34;, list.Front().Next()) } list 对应的方法有：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 type Element func (e *Element) Next() *Element func (e *Element) Prev() *Element type List func New() *List func (l *List) Back() *Element // 最后一个元素 func (l *List) Front() *Element // 第一个元素 func (l *List) Init() *List // 链表初始化 func (l *List) InsertAfter(v interface{}, mark *Element) *Element // 在某个元素后插入 func (l *List) InsertBefore(v interface{}, mark *Element) *Element // 在某个元素前插入 func (l *List) Len() int // 在链表长度 func (l *List) MoveAfter(e, mark *Element) // 把 e 元素移动到 mark 之后 func (l *List) MoveBefore(e, mark *Element) // 把 e 元素移动到 mark 之前 func (l *List) MoveToBack(e *Element) // 把 e 元素移动到队列最后 func (l *List) MoveToFront(e *Element) // 把 e 元素移动到队列最头部 func (l *List) PushBack(v interface{}) *Element // 在队列最后插入元素 func (l *List) PushBackList(other *List) // 在队列最后插入接上新队列 func (l *List) PushFront(v interface{}) *Element // 在队列头部插入元素 func (l *List) PushFrontList(other *List) // 在队列头部插入接上新队列 func (l *List) Remove(e *Element) interface{} // 删除某个元素 环 (循环链表) 环的结构有点特殊，环的尾部就是头部，所以每个元素实际上就可以代表自身的这个环。 它不需要像 list 一样保持 list 和 element 两个结构，只需要保持一个结构就行。\n1 2 3 4 type Ring struct { next, prev *Ring Value interface{} } 我们初始化环的时候，需要定义好环的大小，然后对环的每个元素进行赋值。环还提供一个 Do 方法，能遍历一遍环，对每个元素执行一个 function。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 package main import ( \u0026#34;container/ring\u0026#34; \u0026#34;fmt\u0026#34; ) func main() { ring := ring.New(3) for i := 1; i \u0026lt;= 3; i++ { ring.Value = i ring = ring.Next() } // 计算 1+2+3 s := 0 ring.Do(func(p interface{}){ s += p.(int) }) fmt.Println(\u0026#34;sum is\u0026#34;, s) } ring 提供的方法有:\n1 2 3 4 5 6 7 8 9 type Ring func New(n int) *Ring // 初始化环 func (r *Ring) Do(f func(interface{})) // 循环环进行操作 func (r *Ring) Len() int // 环长度 func (r *Ring) Link(s *Ring) *Ring // 连接两个环 func (r *Ring) Move(n int) *Ring // 指针从当前元素开始向后移动或者向前（n 可以为负数） func (r *Ring) Next() *Ring // 当前元素的下个元素 func (r *Ring) Prev() *Ring // 当前元素的上个元素 func (r *Ring) Unlink(n int) *Ring // 从当前元素开始，删除 n 个元素 进程、线程和goroutine 创建进程 os 包及其子包 os/exec 提供了创建进程的方法。\n一般应该优先使用 os/exec 包。因为 os/exec 包依赖 os 包中关键创建进程的 API\n在 Unix 中，创建一个进程，通过系统调用 fork 实现（及其一些变种，如 vfork、clone）。在 Go 语言中，Linux 下创建进程使用的系统调用是 clone。\n很多时候，系统调用 fork、execve、wait 和 exit 会在一起出现\nfork：允许某一进程（父进程）创建一个新进程（子进程）。具体做法是，新的子进程几近于对父进程的翻版：子进程获得父进程的栈、数据段、堆和执行文本段的拷贝。可将此视为把父进程一分为二。 exit(status)：终止一进程，将进程占用的所有资源（内存、文件描述符等）归还内核，交其进行再次分配。**参数 status 为一整型变量，表示进程的退出状态。**父进程可使用系统调用 wait() 来获取该状态。 wait(\u0026amp;status) 目的有二：其一，如果子进程尚未调用 exit() 终止，那么 wait 会挂起父进程（阻塞）直至子进程终止，这样可以确保父进程在子进程完成后再继续执行后续的操作；其二，子进程的终止状态通过 wait 的 status 参数返回。status参数是一个整数指针，通过传入 wait 函数的地址，父进程可以在子进程终止后获取到子进程的终止状态。 execve(pathname, argv, envp) 加载一个新程序（路径名为 pathname，参数列表为 argv，环境变量列表为 envp）到当前进程的内存。这将丢弃现存的程序文本段，并为新程序重新创建栈、数据段以及堆。通常将这一动作称为执行一个新程序（舍弃旧程序）,它是实现进程间切换和程序替换的重要系统调用函数。 在 Go 语言中，没有直接提供 fork 系统调用的封装，而是将 fork 和 execve 合二为一，提供了 syscall.ForkExec。如果想只调用 fork，得自己通过 syscall.Syscall(syscall.SYS_FORK, 0, 0, 0) 实现。\nProcess 及其相关方法 os.Process 存储了通过 StartProcess 创建的进程的相关信息。\n一般通过 StartProcess 创建 Process 的实例，函数声明如下：\nfunc StartProcess(name string, argv []string, attr *ProcAttr) (*Process, error)\n它使用提供的程序名、命令行参数、属性开始一个新进程。StartProcess 是一个低级别的接口。os/exec 包提供了高级别的接口，一般应该尽量使用 os/exec 包。如果出错，错误的类型会是 *PathError。\n其中的参数 attr，类型是 ProcAttr 的指针，用于为 StartProcess 创建新进程提供一些属性。定义如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 type ProcAttr struct { // 如果 Dir 非空，子进程会在创建 Process 实例前先进入该目录。（即设为子进程的当前工作目录） Dir string // 如果 Env 非空，它会作为新进程的环境变量。必须采用 Environ 返回值的格式。 // 如果 Env 为 nil，将使用 Environ 函数的返回值。 Env []string // Files 指定被新进程继承的打开文件对象。 // 前三个绑定为标准输入、标准输出、标准错误输出。 // 依赖底层操作系统的实现可能会支持额外的文件对象。 // nil 相当于在进程开始时关闭的文件对象。 // 通常用法是 Files: []*os.File{os.Stdin, os.Stdout, os.Stderr}, Files []*File // 操作系统特定的创建属性。 // 注意设置本字段意味着你的程序可能会执行异常甚至在某些操作系统中无法通过编译。 // 看 syscall.SysProcAttr 的定义，可以知道用于控制进程的相关属性。 Sys *syscall.SysProcAttr } FindProcess 可以通过 pid 查找一个运行中的进程。该函数返回的 Process 对象可以用于获取关于底层操作系统进程的信息。在 Unix 系统中，此函数总是成功，即使 pid 对应的进程不存在。\nfunc FindProcess(pid int) (*Process, error)\nProcess 提供了四个方法：Kill、Signal、Wait 和 Release。其中 Kill 和 Signal 跟信号相关，而 Kill 实际上就是调用 Signal，发送了 SIGKILL 信号，强制进程退出\nRelease 方法用于释放 Process 对象相关的资源，以便将来可以被再使用（释放资源给其他程序使用）。该方法只有在确定没有调用 Wait 时才需要调用。Unix 中，该方法的内部实现只是将 Process 的 pid 置为 -1。\nfunc (p *Process) Wait() (*ProcessState, error)\n在多进程应用程序的设计中，父进程需要知道某个子进程何时改变了状态 —— 子进程终止或因收到信号而停止。Wait 方法就是一种用于监控子进程的技术。\nWait 方法阻塞父进程直到子进程退出，然后返回一个 ProcessState 描述子进程的状态和可能的错误。Wait 方法会释放绑定到 Process 的所有资源。在大多数操作系统中，Process 必须是当前进程的子进程，否则会返回错误。\n1 2 3 4 5 type ProcessState struct { pid int // The process\u0026#39;s id. status syscall.WaitStatus // System-dependent status info. rusage *syscall.Rusage } ProcessState 保存了 Wait 函数报告的某个进程的信息。status 记录了状态原因，通过 syscal.WaitStatus 类型定义的方法可以判断：\nExited()：是否正常退出，如调用 os.Exit； Signaled()：是否收到未处理信号而终止； CoreDump()：是否收到未处理信号而终止，同时生成 coredump 文件，如 SIGABRT； Stopped()：是否因信号而停止（SIGSTOP）； Continued()：是否因收到信号 SIGCONT 而恢复； syscal.WaitStatus 还提供了其他一些方法，比如获取退出状态、信号、停止信号和中断（Trap）原因。\n因为 Linux 下 Wait 的内部实现使用的是 wait4 系统调用，因此，ProcessState 中包含了 rusage，用于统计进程的各类资源信息。一般情况下，syscall.Rusage 中定义的信息都用不到，如果实际中需要使用，可以查阅 Linux 系统调用 getrusage 获得相关说明 (getrusage(2))。\nProcessState 结构体内部字段都是私有的，我们可以通过它提供的方法来获得一些基本信息，比如：进程是否退出、Pid、进程是否是正常退出、进程 CPU 时间、用户时间等等。\n运行外部命令 通过 os 包可以做到运行外部命令 (os.StartProcess) 。不过，Go 标准库为我们封装了更好用的包： os/exec，运行外部命令，应该优先使用它，它包装了 os.StartProcess 函数以便更容易的重定向标准输入和输出，使用管道连接 I/O，以及作其它的一些调整。\n查找可执行程序 exec.LookPath 函数在 PATH 指定的多个目录中搜索可执行程序。该函数返回完整路径或相对于当前路径的一个相对路径。\nfunc LookPath(file string) (string, error)\n如果在 $PATH 中没有找到可执行文件，则返回 exec.ErrNotFound。\nCmd 及其相关方法 Cmd 结构代表一个正在准备或者在执行中的外部命令，调用了 Run、Output 或 CombinedOutput 后，Cmd 实例不能被重用（已经执行无法重用）。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 type Cmd struct { // Path 是将要执行的命令路径。 // 该字段不能为空（也是唯一一个不能为空的字段），如为相对路径会相对于 Dir 字段。 // 通过 Command 初始化时，会在需要时调用 LookPath 获得完整的路径。 Path string // Args 存放着命令的参数，第一个值是要执行的命令（Args[0])；如果为空切片或者 nil，使用 {Path} 运行。 // 一般情况下，Path 和 Args 都应被 Command 函数设定。 Args []string // Env 指定进程的环境变量，如为 nil，则使用当前进程的环境变量，即 os.Environ()，一般就是当前系统的环境变量。 Env []string // Dir 指定命令的工作目录。如为空字符串，会在调用者的进程当前工作目录下执行。 Dir string // Stdin 指定进程的标准输入，如为 nil，进程会从空设备读取（os.DevNull） // 如果 Stdin 是 *os.File 的实例，进程的标准输入会直接指向这个文件 // 否则，会在一个单独的 goroutine 中从 Stdin 中读数据，然后将数据通过管道传递到该命令中（也就是从 Stdin 读到数据后，写入管道，该命令可以从管道读到这个数据）。在 goroutine 停止数据拷贝之前（停止的原因如遇到 EOF 或其他错误，或管道的 write 端错误），Wait() 方法会一直堵塞，直到数据拷贝的goroutine停止。这种机制确保了在Wait方法返回之前，命令执行过程中的输入数据能够完整地传递给命令。 Stdin io.Reader // Stdout 和 Stderr 指定进程的标准输出和标准错误输出。 // 如果任意一个为 nil，Run 方法会将对应的文件描述符关联到空设备（os.DevNull） // 如果两个字段指向相同，同一时间最多只能有一个线程可以写入。 Stdout io.Writer Stderr io.Writer // ExtraFiles 指定额外被新进程继承的已打开文件，不包括标准输入、标准输出、标准错误输出。 // 如果本字段非 nil，其中的元素 i 会变成文件描述符 3+i。 ExtraFiles []*os.File // SysProcAttr 提供可选的、各操作系统特定的 sys 属性。 // Run 方法会将它作为 os.ProcAttr 的 Sys 字段传递给 os.StartProcess 函数。 SysProcAttr *syscall.SysProcAttr // Process 是底层的，只执行一次的进程。 Process *os.Process // ProcessState 包含一个已经存在的进程的信息，只有在调用 Wait 或 Run 后才可用。 ProcessState *os.ProcessState } Command 一般应该通过 exec.Command 函数产生 Cmd 实例：\nfunc Command(name string, arg ...string) *Cmd\n该函数返回一个 *Cmd，用于使用给出的参数执行 name 指定的程序。返回的 *Cmd 只设定了 Path 和 Args 两个字段。\n**如果 name 不含路径分隔符，将使用 LookPath 获取完整路径；否则直接使用 name。**参数 arg 不应包含命令名。\n得到 *Cmd 实例后，接下来一般有两种写法：\n调用 Start()，接着调用 Wait()，然后会阻塞父进程直到命令执行完成 (这种方式主要用在需要在start和wait之间需要处理其他逻辑的情况)； 调用 Run()，它内部会先调用 Start()，接着调用 Wait()； Start func (c *Cmd) Start() error\n开始执行 c 包含的命令，但并不会等待该命令完成后再返回。Start() 内部调用 os.StartProcess，执行 forkExec。\nWait func (c *Cmd) Wait() error\nWait 会阻塞当前进程直到该命令执行完成，该命令必须是先通过 Start 执行。\n如果命令成功执行，stdin、stdout、stderr 数据传递没有问题，并且返回状态码为 0，方法的返回值为 nil；如果命令没有执行或者执行失败，会返回 *ExitError 类型的错误；否则返回的 error 可能是表示 I/O 问题。\n如果 c.Stdin 不是 *os.File 类型，Wait 会等待，直到数据从 c.Stdin 拷贝到进程的标准输入中。\nWait 方法会返回命令的退出状态码并在命令执行完后释放相关的资源。\nOutput 除了 Run() 是 Start+Wait 的简便写法，Output() 更是 Run() 的简便写法，外加获取外部命令的输出。\nfunc (c *Cmd) Output() ([]byte, error)\n它要求 c.Stdout 必须是 nil (内部第一步就是c.Stdout != nil返回错误)，内部会将 bytes.Buffer 赋值给 c.Stdout，在 Run() 成功返回后，会将 Buffer 的结果返回（stdout.Bytes())。\nCombinedOutput Output() 只返回 Stdout 的结果，而 CombinedOutput 组合了 Stdout 和 Stderr 的输出，即 Stdout 和 Stderr 都赋值为同一个 bytes.Buffer。\nStdoutPipe、StderrPipe 和 StdinPipe 除了上面介绍的 Output 和 CombinedOutput 直接获取命令输出结果外，还可以通过 StdoutPipe 返回的 io.ReadCloser 来获取输出；相应的 StderrPipe 得到错误信息；而 StdinPipe 则可以往命令写入数据。\nfunc (c *Cmd) StdoutPipe() (io.ReadCloser, error)\nStdoutPipe 方法返回一个在命令 Start 执行后与命令标准输出关联的管道。Wait 方法会在命令结束后会自动关闭这个管道，所以一般不需要手动关闭该管道。但是如果在从管道读取完全部数据之前调用 Wait 出错了，则必须手动关闭。因此建议获取后直接defer close，即使已经关闭了再次关闭也没事，只是会因为已经关闭返回失败而已。\nfunc (c *Cmd) StderrPipe() (io.ReadCloser, error) 逻辑和 StdoutPipe 类似，只是关联的是标准错误输出。而 StdinPipe 关联的是标准输入，Wait也会自动关闭这个管道，必要时调用者可以直接通过调用 Close 方法来强行关闭管道。例如，有时候标准输入必须要关闭后，命令执行才能完成，此时调用者需要显式地关闭管道。\n因为这些管道虽然在Start之前定义，但是结果要在 Start之后 Wait 之前才能获取（Wait会关闭管道），所以，要使用这些方法，一般只能使用 Start +Wait 组合，使用 Run就必须需要为管道开一个新的goroutine。\n1 2 3 4 5 6 7 8 9 10 cmd := exec.Command(\u0026#34;ls\u0026#34;) r, _ := cmd.StdoutPipe() gofunc() { // ReadAll会一直循环直到返回EOF，命令结束时就会发送EOF s, _ := io.ReadAll(r) fmt.Println(string(s)) }() // 通过等待5S可以发现ls的结果会在5S后一齐打印 time.Sleep(5 * time.Second) cmd.Run() 进程终止 os.Exit() 函数会终止当前进程，对应的系统调用不是 _exit，而是 exit_group。\nfunc Exit(code int)\nExit 让当前进程以给出的状态码 code 退出。一般来说，状态码 0 表示成功，非 0 表示出错。Exit 会让进程立刻终止，defer 函数都不会被执行。因此如果需要执行defer函数最好使用return来结束函数。\n进程属性和控制 每个进程都有一些属性，os 包提供了一些函数可以获取进程属性。\n每个进程都会有一个进程 ID，可以通过 os.Getpid 获得。同时，每个进程都有创建自己的父进程，通过 os.Getppid 获得。\n进程凭证 Unix 中进程都有一套数字表示用户 ID(UID) 和组 ID(GID)，有时也将这些 ID 称之为进程凭证。Windows 下总是 -1。\n实际用户 ID 和实际组 ID 实际用户 ID（real user ID）和实际组 ID（real group ID）确定了进程所属的用户和组。可以从 /etc/passwd 文件读取用户 ID 和组 ID。当创建新进程时（如 shell 执行程序），将从其父进程中继承这些 ID。\n可通过 os.Getuid() 和 os.Getgid() 获取当前进程的实际用户 ID 和实际组 ID；\n有效用户 ID 和有效组 ID 大多数 Unix 实现中，当进程尝试执行各种操作（即系统调用）时，将结合有效用户 ID、有效组 ID，连同辅助组 ID 一起来确定授予进程哪些权限。内核还会使用有效用户 ID 来决定一个进程是否能向另一个进程发送信号。\n有效用户 ID 为 0（root 的用户 ID）的进程拥有超级用户的所有权限。这样的进程又称为特权级进程（privileged process）。某些系统调用只能由特权级进程执行。\n可通过 os.Geteuid() 和 os.Getegid() 获取当前进程的有效用户 ID（effective user ID）和有效组 ID（effectvie group ID）。\n**通常情况下，有效用户 ID 及组 ID 与其相应的实际 ID 相等，**但有两种方法能够致使二者不同。一是使用相关系统调用；二是执行 set-user-ID 和 set-group-ID 程序，这就会导致实际用户id为运行程序的用户id，而有效用户id为程序本身所属的用户id。\nSet-User-ID 和 Set-Group-ID 程序 set-user-ID 程序会将进程的有效用户 ID 设置为可执行文件所属的用户 ID（实际id和有效id在正常情况下相同，均为执行当前程序的用户，但是set-user-id可以将其改为以文件本身用户权限运行），从而获得常规情况下并不具有的权限。set-group-ID 程序对进程有效组 ID 实现类似任务。（有时也将这程序简称为 set-UID 程序和 set-GID 程序。）\n与其他文件一样，可执行文件的用户 ID 和组 ID 决定了该文件的所有权。文件还拥有两个特别的权限位 set-user-ID 位和 set-group-ID 位，可以使用 os.Chmod 修改这些权限位（非特权用户进程只能修改其自身文件，而特权用户进程能修改任何文件）。\nchmod u+s filename\n文件设置了 set-user-ID 位后，ls -l 显示文件会在属主用户执行权限字段上看到字母 s（有执行权限时） 或 S（无执行权限时）；相应的 set-group-ID 则是在组用户执行位上看到 s 或 S。\n当运行 set-user-ID 程序时，内核会将进程的有效用户 ID 设置为可执行文件自身的用户 ID。set-group-ID 程序对进程有效组 ID 的操作与之类似。通过这种方法修改进程的有效用户 ID 或组 ID，能够使进程（换言之，执行该程序的用户）获得常规情况下所不具有的权限。例如，如果一个可执行文件的属主为 root，且为此程序设置了 set-user-ID 权限位，那么当运行该程序时，进程会取得超级用户权限。\n也可以利用程序的 set-user-ID 和 set-group-ID 机制，将进程的有效 ID 修改为 root 之外的其他用户。例如，为提供一个受保护文件的访问，可采用如下方案：创建一个具有对该文件访问权限的专有用户（组）ID，然后再创建一个 set-user-ID（set-group-ID）程序，将进程有效用户（组）ID 变更为这个专用 ID (即修改文件属主为专有用户)。这样，无需拥有超级用户的所有权限，程序就能访问该文件，避免使用root权限时进行其他非法操作。\n此外，os 还提供了获取辅助组 ID 的函数：os.Getgroups()\n操作系统用户 包 os/user 允许通过名称或 ID 查询用户账号。用户结构定义如下：\n1 2 3 4 5 6 7 type User struct { Uid string // user id Gid string // primary group id Username string Name string HomeDir string } User 结构体代表一个用户帐户。\n在 POSIX 系统中 Uid 和 Gid 字段分别包含代表 uid 和 gid 的十进制数字。在 Windows 系统中 Uid 和 Gid 包含字符串格式的安全标识符（SID）。在 Plan 9 系统中，Uid、Gid、Username 和 Name 字段是 /dev/user 的内容。\nCurrent 函数可以获取当前用户账号。而 Lookup 和 LookupId 则分别根据用户名和用户 ID 查询用户。如果对应的用户不存在，则返回 user.UnknownUserError 或 user.UnknownUserIdError。\n1 2 3 4 5 6 7 8 9 10 func main() { fmt.Println(user.Current()) fmt.Println(user.Lookup(\u0026#34;xiaonuo\u0026#34;)) fmt.Println(user.LookupId(\u0026#34;0\u0026#34;)) } // Output: // \u0026amp;{502 502 xiaonuo /home/xiaonuo} \u0026lt;nil\u0026gt; // \u0026amp;{502 502 xiaonuo /home/xiaonuo} \u0026lt;nil\u0026gt; // \u0026amp;{0 0 root root /root} \u0026lt;nil\u0026gt; 进程的当前工作目录 一个进程的当前工作目录（current working directory）定义了该进程解析相对路径名的起点。新进程的当前工作目录继承自其父进程。\nfunc Getwd() (dir string, err error)\nGetwd 返回一个对应当前工作目录的根路径（即绝对路径）。如果当前目录可以经过多条路径抵达（比如符号链接），Getwd 会返回其中一个。对应系统调用：getcwd。\nfunc Chdir(dir string) error\n相应的，Chdir 将当前工作目录修改为 dir 指定的目录。如果出错，会返回 *PathError 类型的错误。对应系统调用 chdir。\n另外，os.File 有一个方法：Chdir，对应系统调用 fchidr（以文件描述符为参数），也可以改变当前工作目录。\n改变进程的根目录 每个进程都有一个根目录，该目录是解释绝对路径（即那些以 / 开始的目录）时的起点。默认情况下，这是文件系统的真实根目录。新进程从其父进程处继承根目录。有时可能需要改变一个进程的根目录（比如 ftp 服务就是一个典型的例子，ftp一进入，其根就是某个文件夹内容，而不是真实的根）。系统调用 chroot 能改变一个进程的根目录。\n在go的高版本中已经无法通过标准库修改进程的根目录了，可以直接通过syscall去调用系统调用来修改。\n进程环境列表 每个进程都有与其相关的称之为环境列表（environment list）的字符串数组，或简称环境（environment）。其中每个字符串都以 名称 = 值（name=value）形式定义。因此，环境是“名称 - 值”的成对集合，可存储任何信息。常将列表中的名称称为环境变量（environment variables）。\n新进程在创建之时，会继承其父进程的环境副本。这是一种原始的进程间通信方式，却颇为常用。环境（environment）提供了将信息从父进程传递给子进程的方法。创建后，父子进程的环境相互独立，互不影响。\n环境变量的常见用途之一是在 shell 中，通过在自身环境中放置变量值，shell 就可确保把这些值传递给其所创建的进程，并以此来执行用户命令。\n在程序中，可以通过 os.Environ 获取环境列表：\nfunc Environ() []string\n返回的 []string 中每个元素是 key=value 的形式。\nfunc Getenv(key string) string\nGetenv 检索并返回名为 key 的环境变量的值。如果不存在该环境变量会返回空字符串。有时候，可能环境变量存在，只是值刚好是空。为了区分这种情况，提供了另外一个函数 LookupEnv()：\nfunc LookupEnv(key string) (string, bool)\n如果变量名存在，第二个参数返回 true，否则返回 false。\nfunc Setenv(key, value string) error\nSetenv 设置名为 key 的环境变量，值为 value。如果出错会返回错误。（如果值之前存在，会覆盖）\nfunc Unsetenv(key string) error\nUnsetenv 删除名为 key 的环境变量。\nfunc Clearenv()\nClearenv 删除所有环境变量。\n这些函数都是获取和修改当前程序的环境变量，并不会对全局或其他进程的环境变量造成影响，它们都是独立的副本。如果想修改全局的环境变量肯定需要修改文件 /etc/profile，这会由第一个进程执行，其他进程都是从该进程延申出来的，因此会得到其环境变量的副本。\n另外，ExpandEnv 和 Getenv 功能类似，不过，前者使用变量方式，如：\nos.ExpandEnv(\u0026quot;$GODEBUG\u0026quot;) 和 os.Getenv(\u0026ldquo;GODEBUG\u0026rdquo;) 是一样的。\n实际上，os.ExpandEnv 调用的是 os.Expand(s, os.Getenv)。\nfunc Expand(s string, mapping func(string) string) string\nExpand 能够将${var} 或 $var 形式的变量，经过 mapping 处理，得到结果。\n实际开发中使用 os.GetEnv就行了，ExpandEnv基本不会使用，其本质上也是用的os.GetEnv。\n线程 与进程类似，线程是允许应用程序并发执行多个任务的一种机制。一个进程可以包含多个线程。同一个程序中的所有线程均会独立执行相同程序，且共享同一份全局内存区域。\n同一进程中的多个线程可以并发执行。在多处理器环境下，多个线程可以同时并行（不是并发）。如果一个线程因等待 I/O 操作而遭阻塞，其他线程依然可以继续运行。\n在 Linux 中，通过系统调用 clone() 来实现线程的。从前面的介绍，我们知道，该系统调用也可以用来创建进程。实际上，从内核的角度来说，它并没有线程这个概念。Linux 把所有的线程都当作进程来实现。内核并没有准备特别的调度算法或是定义特别的数据结构来表示线程。相反，线程仅仅被内核视为一个使用某些共享资源的进程。所以，在内核中，它看起来就是一个普通的进程（只是该进程和其他一些进程共享某些资源，如地址空间）。\n在 Go 中，通过 clone() 系统调用来创建线程，其中的 clone_flags 为：\n1 2 3 4 5 cloneFlags = _CLONE_VM | /* share memory */ _CLONE_FS | /* share cwd, etc */ _CLONE_FILES | /* share fd table */ _CLONE_SIGHAND | /* share sig handler table */ _CLONE_THREAD /* revisit - okay for now */ 也就是说，父子俩共享了地址空间 (_CLONE_VM)、文件系统资源 (_CLONE_FS)、文件描述符 (_CLONE_FILES) 和信号处理程序 (_CLONE_SIGHAND)。而 _CLONE_THREAD 则会将父子进程放入相同的线程组。这样一来，新建的进程和父进程都叫做线程。\n需要注意的是，直接使用 clone() 系统调用创建线程相对于使用 Go 语言提供的 goroutine 来说较为底层，并且需要手动处理线程的等待和退出。**因此，在一般情况下，推荐使用 goroutine 来实现并发操作。**只有在特定的需求下，才需要使用 clone() 系统调用来创建线程。\n","date":"2023-04-04T00:00:00Z","permalink":"https://xiaonuoz.github.io/p/go-%E8%AF%AD%E8%A8%80%E5%90%84%E7%A7%8D%E6%A0%87%E5%87%86%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/","title":"Go 语言各种标准库的使用"},{"content":"此文介绍从各个方向优化golang代码，让golang运行更流畅，内存消耗更少。\n此文章为学习 极客兔兔 《Go 语言高性能编程》后的总结性文章。\n字符串拼接 在Go语言中，字符串只是可读的，它无法被修改，拼接字符串实际上是创建了一个新的字符串对象。因此如果频繁的拼接字符串，就意味着在频繁的分配内存，会对性能造成严重的影响。\n当使用 + 拼接 2 个字符串时，生成一个新的字符串，那么就需要开辟一段新的空间，新空间的大小是原来两个字符串的大小之和。拼接第三个字符串时，再开辟一段新空间，新空间大小是三个字符串大小之和，以此类推。\n而 strings.Builder，bytes.Buffer，包括切片 []byte 的内存是以倍数申请的。例如，初始大小为 0，当第一次写入大小为 10 byte 的字符串时，则会申请大小为 16 byte 的内存（恰好大于 10 byte 的 2 的指数），第二次写入 10 byte 时，内存不够，则申请 32 byte 的内存，第三次写入内存足够，则不申请新的，以此类推。在实际过程中，超过一定大小，申请策略上会有些许调整。\n官方推荐的字符串拼接方法是strings.Builder,它的性能比bytes.Buffer略快10%,一个比较重要的区别在于,bytes.Buffer 转化为字符串时重新申请了一块空间，存放生成的字符串变量，而 strings.Builder 直接将底层的 []byte 转换成了字符串类型返回了回来。\n速度最快的是用[]byte来拼接，因为它一次性分配完了所需要的内存，通过append来拼接字符串，但是这个有一个很大的缺陷就是，在日常开发中，很多时候并不知道给这个字符串分配有多大的内存。而且strings.Builder也有 Grow() 方法可以进行预分配内存。\nbytes.Buffer 1 2 3 4 5 6 7 8 // To build strings more efficiently, see the strings.Builder type. func (b *Buffer) String() string { if b == nil { // Special case, useful in debugging. return \u0026#34;\u0026lt;nil\u0026gt;\u0026#34; } return string(b.buf[b.off:]) } strings.Builder 1 2 3 4 // String returns the accumulated string. func (b *Builder) String() string { return *(*string)(unsafe.Pointer(\u0026amp;b.buf)) } bytes.Buffer 的注释中还特意提到了：\nTo build strings more efficiently, see the strings.Builder type.\n切片使用陷阱 在已有切片的基础上进行切片，不会创建新的底层数组，它会与原切片共用一个底层数组。因为原来的底层数组没有发生变化，内存会一直占用 (内存逃逸)，直到没有变量引用该数组才会进行垃圾回收。\n因此很可能出现这么一种情况，原切片由大量的元素构成，但是我们在原切片的基础上切片，虽然只使用了很小一段，但底层数组在内存中仍然占据了大量空间，得不到释放。比较推荐的做法，使用 copy 替代 re-slice。copy会创建一个新的底层数据，原数据可以正确的得到回收。\n1 2 3 func lastNumsBySlice(origin []int) []int { return origin[len(origin)-2:] } for 和 range 的性能比较 array/slice 1 2 3 4 5 words := []string{\u0026#34;Go\u0026#34;, \u0026#34;语言\u0026#34;, \u0026#34;高性能\u0026#34;, \u0026#34;编程\u0026#34;} for i, s := range words { words = append(words, \u0026#34;test\u0026#34;) fmt.Println(i, s) } 变量 words 在循环开始前，仅会计算一次，如果在循环中修改切片的长度不会改变本次循环的次数。 迭代过程中，每次迭代的下标和值被赋值给变量 i 和 s，第二个参数 s 是可选的。 针对 nil 切片，迭代次数为 0。 map 1 2 3 4 5 6 7 8 9 10 m := map[string]int{ \u0026#34;one\u0026#34;: 1, \u0026#34;two\u0026#34;: 2, \u0026#34;three\u0026#34;: 3, } for k, v := range m { delete(m, \u0026#34;two\u0026#34;) m[\u0026#34;four\u0026#34;] = 4 fmt.Printf(\u0026#34;%v: %v\\n\u0026#34;, k, v) } 和切片不同的是，迭代过程中，删除还未迭代到的键值对，则该键值对不会被迭代。尽量不要在遍历map时进行删除操作，因为map是无序的，你根本不知道它的迭代顺序，因此无法确定它会在迭代前删除还是删除前迭代 在迭代过程中，如果创建新的键值对，那么新增键值对，可能被迭代，也可能不会被迭代。 针对 nil 字典，迭代次数为 0 channel 1 2 3 4 5 6 7 8 9 10 11 ch := make(chan string) go func() { ch \u0026lt;- \u0026#34;Go\u0026#34; ch \u0026lt;- \u0026#34;语言\u0026#34; ch \u0026lt;- \u0026#34;高性能\u0026#34; ch \u0026lt;- \u0026#34;编程\u0026#34; close(ch) }() for n := range ch { fmt.Println(n) } 发送给信道(channel) 的值可以使用 for 循环迭代，直到信道被关闭。 如果是 nil 信道，循环将永远阻塞。 如果去除close就会发生死锁panic。 range优化 range在迭代过程中返回的值是对元素的拷贝，每遍历一个都会拷贝一次当前索引元素，修改拷贝的值不会影响到切片中元素值，因此如果每次迭代的元素的内存占用很低，那么 for 和 range 的性能几乎是一样，例如 []int。但是如果元素是一个很大的结构体时range就会严重的影响到性能(因为拷贝需要时间)。\n对应这种情况，建议使用for来进行遍历，通过索引下标去寻找对应的值，不进行拷贝。也可以用for i:=range slice 的方法来忽略拷贝。如果必须迭代值，则需要将切片或者数组的元素改为指针， 这样它虽然会发生拷贝，但是拷贝的是指针，指针相比于具体的值会小很多，而且还可以通过寻址直接修改元素的值。\n反射(reflect)性能 标准库 reflect 为 Go 语言提供了运行时动态获取对象的类型和值以及动态创建对象的能力。反射可以帮助抽象和简化代码，提高开发效率。\n但是相比于硬编码的方式，反射的性能就会慢很多。\n创建对象时，reflect.New的耗时约为new()的1.5倍 修改反射字段值时，FieldByName的性能相比Field劣化10倍以上，而Field的性能相比不使用反射的普通赋值操作劣化100倍左右，因此如果要使用反射修改值，尽量使用Field() FieldByName()和Field()有这么大的性能差距就在于两者的底层逻辑实现不同，在反射的内部，字段是按照定义时的顺序存储的。 FieldByName()底层是使用for循环逐个查找字段名匹配的字段，其查找效率为O(N) Field()是根据下标直接访问对应字段，查找效率为O(1)。 结构体所包含的字段(包括方法)越多，两者之间的效率差距则越大。 提高性能 避免使用反射 使用反射赋值，效率非常低下，如果有替代方案的话，尽量避免使用反射，特别是会被频繁使用的热点代码尤为注意。例如在RPC协议中，需要对结构体进行序列化和反序列化，这个时候避免使用 Go 语言自带的 json 的 Marshal 和 Unmarshal 方法，因为标准库中的 json 序列化和反序列化是利用反射实现的。可选的替代方案有 easyjson，在大部分场景下，相比标准库，有 5 倍左右的性能提升。\n缓存Map 因为FieldByName()相比于Field()有一个数量级的性能劣化。那么在实际应用中就要避免之间调用FieldByName。因此就可以通过map来实现，先使用for循环遍历typ.NumField()，将所有的字段通过Field()的方法存到一个map中，然后后续使用时再从map中取。\nmap的value尽量存储对应字段的索引,直接存储对应结构体的话，它为Type的StructField结构体，无法获取到对应具体实例该字段的值。\n使用map可以让FieldByName消耗的时间从原本的10倍缩小到2倍左右。\n使用空结构体节省内存 空结构体 struct{} 实例不占据任何的内存空间，可以通过unsafe.Sizeof 计算出一个数据类型实例需要占用的字节数。\n因为空结构体不占据内存空间，因此被广泛作为各种场景下的占位符使用。一是节省资源，二是空结构体本身就具备很强的语义，即这里不需要任何值，仅作为占位符\n将空结构体作为map的值使用，实现set类型。 作为不发送数据的信道，只用来通知子协程(goroutine)执行任务，或只用来控制协程并发度。 作为仅包含方法的结构体，主要用来给一系列函数进行分组。 内存对齐 CPU访问内存时并不是逐个字节的访问，而是以字长为单位进行访问。而32位的cpu字长为(32/8) 4字节，即cpu每次访问内存的单位也是4字节。\n这么设计的目的主要是减少CPU访问内存的次数，加大CPU访问内存的吞吐量。\n如此图中，如果不进行内存对齐，访问b变量时就需要进行2次内存访问。第一次访问得到b变量的第1个字节，第二次访问得到b变量的后两个字节。\n并且内存对齐对实现变量的原子性操作也是有好处的，如果变量的大小不超过字长，那么内存对齐后，对该变量的访问就是原子的。\n总结：合理的内存对齐可以提高内存读写的性能，并且便于实现变量操作的原子性。\nunsafe.Alignof() 可以根据此方法计算内存对齐遵守的规律。它会返回一个类型的对齐值，也可以叫做对齐系数或者对齐倍数。\n其实例占据的空间必须是对齐值的整数倍，例如一个结构体有一个int16和int32成员，那么它的内存就是2+4=6，但是通过Alignof()得到它的对齐系数为4，因此最终的内存占用值就为8。\n它的对齐系数取决于其结构体中占用内存最大的那个成员的对齐系数。\n对于任意类型的变量 x ，unsafe.Alignof(x) 至少为 1 对于 struct 结构体类型的变量 x，计算 x 每一个字段 f 的 unsafe.Alignof(x.f)，unsafe.Alignof(x) 等于其中的最大值 对于 array 数组类型的变量 x，unsafe.Alignof(x) 等于构成数组的元素类型的对齐倍数(例如切片组成为 cap、len和指向底层数组的指针ptr)，对齐倍数取三者中占用内存最大的那个成员的对齐系数即可 合理布局减少内存占用 对齐系数只是用来推算结构体内存占用必须为系数的整数倍，它无法直接得出该结构体占用内存的值，还需要根据具体排列情况来获取。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 type demo1 struct { a int8 b int16 c int32 } type demo2 struct { a int8 c int32 b int16 } func main() { fmt.Println(unsafe.Sizeof(demo1{})) // 8 fmt.Println(unsafe.Sizeof(demo2{})) // 12 } unsafe.Alignof(x)只会返回最终的对齐系数，。每个字段会按照自身的对齐系数来确定在内存中的偏移量，字段排列顺序不同，上一个字段因为偏移而浪费的大小也不同。\ndemo1: a是第一个字段，它的对齐系数为8/8=1，自身就是8bit，即占满第0个字节。 b是第二个字段，它的对齐系数为16/8=2，那么偏移量必须为2的倍数，即占据第2和第3个字节，第1个字节被留空。 c是第三个字段，它的对齐系数为32/8=4，那么偏移量必须为4的倍数，即占据第4-7字节。 因此demo1内存占用为8字节。\ndemo2: a是第一个字段，和demo1一样占据第0字节。 b是第二个字段,对齐系数为4，4的倍数即必须从第4字节开始，占据4-7字节。 c是第三个字段，对齐系数为2，即占用8-9字节。 demo2的最终对齐倍数为占据内存最大的成员对齐系数值，即它的对齐系数为4，而abc加起来占据了10字节，并不是4的整数倍，因此demo2最终内存占用为12字节。 因此，在对内存特别敏感的结构体的设计上，可以通过调整字段的顺序来减少内存的占用。只需要将结构体的成员按照各自内存占用大小升序排列即可，这样成员的对齐系数是逐渐增大的，排列的会更加紧凑，内存占用也就越小。\n空结构体struct{} 的对齐 空 struct{} 大小为 0，当作为其他 struct 的字段时，一般不需要内存对齐(某个结构体中存在空结构体,只要不是放置在最后，都不会占用空间)。\n但是有一种情况除外：即当 struct{} 作为结构体最后一个字段时，会被填充对齐到前一个字段的大小，地址偏移对齐规则不变。\n因为如果有指针指向该字段, 返回的地址将在结构体之外，如果此指针一直存活不释放对应的内存，就会有内存泄露的问题（该内存不因结构体释放而释放，因为空结构不占用内存，导致这个指针无法指向相对应的值）。\n因此，当 struct{} 作为其他 struct 最后一个字段时，需要填充额外的内存保证安全。我们做个试验，验证下这种情况。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 type demo3 struct { c int32 a struct{} } type demo4 struct { a struct{} c int32 } func main() { fmt.Println(unsafe.Sizeof(demo3{})) // 8 fmt.Println(unsafe.Sizeof(demo4{})) // 4 } 互斥锁和读写锁的性能比较 互斥锁： 互斥即不可同时运行。即使用了互斥锁的两个代码片段互相排斥，只有其中一个代码片段执行完成后，另一个才能执行。在一个 Go 协程调用 Lock 方法获得锁后，其他请求锁的协程都会阻塞在 Lock 方法，直到锁被释放。 读写锁： 保证读操作的安全，那只要保证并发读时没有写操作在进行就行。在这种场景下我们需要一种特殊类型的锁，其允许多个只读操作并行执行，但写操作会完全互斥。这种锁称之为 多读单写锁，简称读写锁，读写锁分为读锁和写锁，读锁是允许同时执行的，但写锁是互斥的。一般来说，有如下几种情况： 读锁之间不互斥，没有写锁的情况下，读锁是无阻塞的，多个协程可以同时获得读锁。 写锁之间是互斥的，存在写锁，其他写锁阻塞。 写锁与读锁是互斥的，如果存在读锁，写锁阻塞，如果存在写锁，读锁阻塞。 读写锁的存在是为了解决读多写少时的性能问题，读场景较多时，读写锁可有效地减少锁阻塞的时间。读写锁一般作用在读远远大于写的情况，最好在读某个共享变量时加锁，读完立马解锁，不要在锁里面处理业务逻辑，这会造成锁住的时间增长。\n互斥锁如何实现公平 互斥锁有两种状态： 正常状态和饥饿状态。 在正常情况下，所有等待锁的goroutine会按照FIFO(先进先出) 顺序等待。唤醒的goroutine不会直接拥有锁，而是会和新请求获取锁的goroutine竞争 锁的拥有。新请求锁的goroutine具有优势，因为它正在CPU上执行，所以刚刚唤醒的goroutine有很大可能在锁的竞争中失败。在这种情况下，这个被唤醒的goroutine会加入到等待队列的前面，但是如果一个等待的goroutine超过1ms 没有获取锁，那么它会将锁转变为饥饿模式。 在饥饿模式下，锁的分配模式会变成 根据等待队列的顺序依次给予，新来的goroutine不会再去尝试获取锁，即使锁可能是unlock解锁状态，也不会去尝试自旋操作，它会直接放在等待队列尾部等待获取锁。 如果一个等待的goroutine获取了锁，并且满足以下其中任何一个条件:\n它是队列中的最后一个，不再有新的goroutine排在后面，这说明这个锁即将变成空闲状态。 它等待的时间小于1ms。 那么锁就会从饥饿状态转换为正常状态。\n正常状态有很好的性能表现，饥饿模式也是非常重要的，因为它能阻止尾部延迟的现象。\n协程的退出 超时返回时的陷阱 超时控制在网络编程中是非常常见的，利用 context.WithTimeout 和 time.After 都能够很轻易地实现。\n超时退出后，子协程依然存在，导致内存泄漏 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 func doBadthing(done chan bool) { time.Sleep(time.Second) done \u0026lt;- true } func timeout(f func(chan bool)) error { done := make(chan bool) go f(done) select { case \u0026lt;-done: fmt.Println(\u0026#34;done\u0026#34;) return nil case \u0026lt;-time.After(time.Millisecond): return fmt.Errorf(\u0026#34;timeout\u0026#34;) } } // fmt.Println(timeout(doBadthing)) 在这个代码中出现的问题就是，假如执行了1000次timeout(doBadthing),那么就会多1000个子协程永久存在与内存中，直到程序结束。原因就是超时时间为1ms，而doBadthing的执行为1s，那么每次都会走超时逻辑，这就导致1s后向done中发送信息时却找不到接收者(timeout已经结束),那么它会永久阻塞。\n解决办法：\n将done从无缓冲channel改为缓存为1的有缓冲channel，这样就不会在done处阻塞，而这个有缓冲的channel因为没人使用也会被垃圾回收掉。 在doBadthing中增加select{} 机制，如果向done发送数据失败，则说明缺少接收者，即超时了，那么这个子协程直接退出。 强制kill goroutine 是不能实现的 即时超时返回了，但是子协程仍在继续运行，直到自己退出。那么有可能在超时的时候，就强制关闭子协程吗？\n答案是不能，goroutine 只能自己退出，而不能被其他 goroutine 强制关闭或杀死。\ngoroutine 被设计为不可以从外部无条件地结束掉，只能通过 channel 来与它通信。也就是说，每一个 goroutine 都需要承担自己退出的责任。\n因为 goroutine 不能被强制 kill，在超时或其他类似的场景下，为了 goroutine 尽可能正常退出，建议如下：\n尽量使用非阻塞 I/O（非阻塞 I/O 常用来实现高性能的网络库），阻塞 I/O 很可能导致 goroutine 在某个调用一直等待，而无法正确结束。 业务逻辑总是考虑退出机制，避免死循环。 任务是分段执行时，超时后立马退出，避免 goroutine 无用的执行过多，浪费资源(在复杂的业务逻辑中，超时可能和部分业务是耦合在一起的，在这种情况下就很难使用非阻塞 I/O 来设计超时，因为如果是非阻塞I/O，它不会进行阻塞，就无法判断有没有超时，会继续向后执行很多不应该执行的业务代码，因此在设计超时机制时，尝试考虑使用select{}是否可行) channel 忘记关闭的陷阱 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 func do(taskCh chan int) { for { select { case t := \u0026lt;-taskCh: time.Sleep(time.Millisecond) fmt.Printf(\u0026#34;task %d is done\\n\u0026#34;, t) } } } func sendTasks() { taskCh := make(chan int, 10) go do(taskCh) for i := 0; i \u0026lt; 1000; i++ { taskCh \u0026lt;- i } } func TestDo(t *testing.T) { t.Log(runtime.NumGoroutine()) sendTasks() time.Sleep(time.Second) t.Log(runtime.NumGoroutine()) } 通过测试结果发现，子协程多了一个，也就是说，有一个协程一直没有得到释放。原因就是子协程go do(taskCh) 的select一直处于阻塞状态，等待接收任务，因此直到程序结束协程都没有释放。 解决办法就是发送完成后通过close(chan)来关闭通道，此时接收者获取的值就是对应类型的零值，然后通过t, beforeClosed := \u0026lt;-taskCh的beforeClosed来判断通道是否关闭，当它为false时表示channel已经被关闭，并且channel里面的数据为空，直接返回。也可以将select{}改为for range的方式，它也会一直读取channel的数据，当close执行后，for循环会退出。\n关于通道和协程的垃圾回收 注意，一个通道被其发送数据协程队列和接收数据协程队列中的所有协程引用着。因此，如果一个通道的这两个队列只要有一个不为空，则此通道肯定不会被垃圾回收。另一方面，如果一个协程处于一个通道的某个协程队列之中，则此协程也肯定不会被垃圾回收，即使此通道仅被此协程所引用。事实上，一个协程只有在退出后才能被垃圾回收。\n通道关闭原则 一个常用的使用Go通道的原则是不要在数据接收方或者在有多个发送者的情况下关闭通道。换句话说，我们只应该让一个通道唯一的发送者关闭此通道 (因为对一个已经关闭的channel再次关闭会panic)。\n可以使用sync.One() 或者互斥锁来确保channel 只被关闭一次。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 type MyChannel struct { C chan T once sync.Once } func NewMyChannel() *MyChannel { return \u0026amp;MyChannel{C: make(chan T)} } func (mc *MyChannel) SafeClose() { mc.once.Do(func() { close(mc.C) }) } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 type MyChannel struct { C chan T closed bool mutex sync.Mutex } func NewMyChannel() *MyChannel { return \u0026amp;MyChannel{C: make(chan T)} } func (mc *MyChannel) SafeClose() { mc.mutex.Lock() defer mc.mutex.Unlock() if !mc.closed { close(mc.C) mc.closed = true } } func (mc *MyChannel) IsClosed() bool { mc.mutex.Lock() defer mc.mutex.Unlock() return mc.closed } 优雅的关闭通道 M 个接受者，1 个发送者。这是最简单的情况，只需让发送者在不想再发送数据的时候关闭数据通道，直接在发送者方close(ch)即可。 1 个接收者，N 个发送者。这个情况比上面的要复杂一点。我们不能让接收者关闭数据通道，不然就会违反了通道关闭原则。但是可以在接收者处创建一个额外的channel,通过关闭额外的信号通道来通知发送者不要再发送数据了 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 // 在例子中，通道 dataCh 不曾被关闭过。如果一个通道不会再有 goroutine 去使用它，它最终会被垃圾回收，不管它是否被关闭。所以在这里优雅的关闭通道就是不要去关闭通道。 wgReceivers := sync.WaitGroup{} wgReceivers.Add(1) dataCh := make(chan int, 100) stopCh := make(chan struct{}) // stopCh 是一个信号通道。用于通知发送方不要继续发送了 // 它的发送者是 dataCh 的接收者。 // 它的接收者是 dataCh 的发送者。 // 发送者，设置1000个发送者 for i := 0; i \u0026lt; 1000; i++ { go func() { for { select { case \u0026lt;-stopCh: return default: } // 即使 stopCh 已经关闭，如果发送给 dataCh 没有阻塞，那么在第二个 select 中第一个分支可能会在一些循环中不会执行(因为select是随机执行一个不阻塞的分支)。 // 如果这是不可接受的，则上面的第一个select代码块是必需的。 select { // 判断能否从stopCh中获取停止发送的信号,因为stopCh在正常情况下没有发送方，导致这一个分支一直处于阻塞状态，直到接收方执行close(ch)后， // 从已关闭的channel中接收数据，如果channel中已经没有数据，则永远不会阻塞，每次接收的都是类型的零值。因此此处发送方不阻塞，直接结束协程。 case \u0026lt;-stopCh: return case dataCh \u0026lt;- rand.Intn(100): } } }() } // 接收者 go func() { defer wgReceivers.Done() for value := range dataCh { if value == 99 { // dataCh 通道的接收者也是 stopCh 通道的发送者。 // 在这里关闭停止通道是安全的。. close(stopCh) return } log.Println(value) } }() wgReceivers.Wait() M个接收者和N个发送者。它们中的任何一个协程都可以去通知让一个中间调解协程帮忙发出停止数据传送的信号。这是最复杂的一种情形。我们不能让接收者和发送者中的任何一个关闭用来传输数据的通道，我们也不能让多个接收者之一关闭一个额外的信号通道。 这两种做法都违反了通道关闭原则。 然而，我们可以引入一个中间调解者角色并让其关闭额外的信号通道来通知所有的接收者和发送者结束工作 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 const Max = 100000 const NumReceivers = 10 const NumSenders = 1000 wgReceivers := sync.WaitGroup{} wgReceivers.Add(NumReceivers) dataCh := make(chan int) stopCh := make(chan struct{}) // stopCh是一个额外的信号通道。 // 它的发送者为中间调解者。它的接收者为dataCh数据通道的所有的发送者和接收者。 // 平常都处于阻塞状态，直到执行close(stopCh)后 toStop := make(chan string, 1) // toStop是一个用来通知中间调解者，让其关闭 通道stopCh 的信号通道。 // 此第二个信号通道的发送者为 dataCh数据通道的所有的发送者和接收者， // 它的接收者为中间调解者。它必须为一个缓冲通道， // 不然如果在中间调解者还未准备好的情况下就已经有某个协程向toStop发送信号时，会发送阻塞，走default分支，此信号就会被抛弃。 // 中间调解者 go func() { \u0026lt;-toStop close(stopCh) }() // 发送者 for i := 0; i \u0026lt; NumSenders; i++ { go func(id string) { for { value := rand.Intn(Max) if value == 0 { // 为了防止阻塞，这里使用了一个尝试 // 发送操作来向中间调解者发送信号。 select { case toStop \u0026lt;- \u0026#34;发送者#\u0026#34; + id: default: } return } // 此处的尝试接收操作是为了让此发送协程尽早 // 退出。标准编译器对尝试接收和尝试发送做了 // 特殊的优化，因而它们的速度很快。 select { case \u0026lt;-stopCh: return default: } // 即使stopCh已关闭，如果这个select代码块 // 中第二个分支的发送操作是非阻塞的，则第一个 // 分支仍很有可能在若干个循环步内依然不会被选 // 中。如果这是不可接受的，则上面的第一个尝试 // 接收操作代码块是必需的。 select { case \u0026lt;-stopCh: return case dataCh \u0026lt;- value: } } }(strconv.Itoa(i)) } // 接收者 for i := 0; i \u0026lt; NumReceivers; i++ { go func(id string) { defer wgReceivers.Done() for { // 和发送者协程一样，此处的尝试接收操作是为了 // 让此接收协程尽早退出。 select { case \u0026lt;-stopCh: return default: } // 即使stopCh已关闭，如果这个select代码块 // 中第二个分支的接收操作是非阻塞的，则第一个 // 分支仍很有可能在若干个循环步内依然不会被选 // 中。如果这是不可接受的，则上面尝试接收操作 // 代码块是必需的。 select { case \u0026lt;-stopCh: return case value := \u0026lt;-dataCh: if value == Max-1 { // 为了防止阻塞，这里使用了一个尝试 // 发送操作来向中间调解者发送信号。 select { case toStop \u0026lt;- \u0026#34;接收者#\u0026#34; + id: default: } return } log.Println(value) } } }(strconv.Itoa(i)) } wgReceivers.Wait() 并没有什么情况非得逼得我们违反通道关闭原则。 如果你遇到了此情形，请考虑修改你的代码流程和结构设计\n并发过高导致程序崩溃 如果无限制的开启协程会导致内存不足崩溃，或者对单个 file/socket 的并发操作个数超过系统上限 (比如在协程中打印内容，fmt.Printf也是操作文件描述符，过多的协程会导致系统资源耗尽)\n不同的应用程序所消耗的资源是不一样的。比较推荐的方式是：应用程序来主动限制并发的协程数量。\n利用channel的缓冲区来限制goroutine的数量 每次开启协程前先向一个有缓冲的channel中发送一条消息，当channel满时就会阻塞，不会再创建新的协程。而每个协程结束时都会从channel中接收一条消息，只有一个协程结束，才能新建一个协程，从而控制了程序创建的协程数量。\nruntime.GOMAXPROCS(逻辑CPU数量) 控制的是可以并发执行的最大 P 数量（即逻辑 CPU 数量），，但是它不能控制总协程数量 (GMP模型中，协程可以在P队列中等待)，GOMAXPROCS默认值就是CPU逻辑核心数量，如8核16线程GOMAXPROCS设置的值就是16，可以通过NumCPU()查看，可以设置比核心数量大，但是没意义，因为正在运行的协程依然最大只能有逻辑CPU数，多余的 P 只会浪费资源，不会带来更好的性能。\n使用第三方库 目前有很多第三方库实现了协程池，可以很方便地用来控制协程的并发数量。\nJeffail/tunny panjf2000/ants 以tunny举例:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 package main import ( \u0026#34;log\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/Jeffail/tunny\u0026#34; ) func main() { // 创建协程池，第一个参数是协程池的大小，第二个参数是协程运行的函数 pool := tunny.NewFunc(3, func(i interface{}) interface{} { log.Println(i) time.Sleep(time.Second) return nil }) // 关闭协程池 defer pool.Close() // 将参数传递给协程池定义好的worker处理 for i := 0; i \u0026lt; 10; i++ { go pool.Process(i) } time.Sleep(time.Second * 4) } 调整系统资源的上限 ulimit 有些时候，即使我们有效地限制了协程的并发数量，仍然会出现某一类资源不足的问题。 比如分布式编译加速工具需要解析gcc命令以及依赖的源文件和头文件，有些编译命令依赖的头文件可能有上百个，那这个时候即使我们将协程的并发数限制到 1000，也可能会超过进程运行时并发打开的文件句柄数量 (程序打开的文件数量超过了系统设置的程序打开句柄数量，资源耗尽)，但是分布式编译工具，仅将依赖的源文件和头文件分发到远端机器执行，并不会消耗本机的内存和 CPU 资源，因此 1000 个并发并不高，这种情况下，降低并发数会影响到编译加速的效率，这种时候我们就可以通过设置系统的打开句柄数量来解决。\n操作系统通常会限制同时打开文件数量、栈空间大小等，ulimit -a 可以看到系统当前的设置：\n1 2 3 4 5 6 7 8 9 10 $ ulimit -a -t: cpu time (seconds) unlimited -f: file size (blocks) unlimited -d: data seg size (kbytes) unlimited -s: stack size (kbytes) 8192 -c: core file size (blocks) 0 -v: address space (kbytes) unlimited -l: locked-in-memory size (kbytes) unlimited -u: processes 1418 -n: file descriptors 12800 我们可以使用 ulimit -n 999999，将同时打开的文件句柄数量调整为 999999 来解决这个问题，其他的参数也可以按需调整\n虚拟内存/交换分区 (virtual memory) 虚拟内存是一项非常常见的技术了，即在内存不足时，将磁盘映射为内存使用，比如 linux 下的交换分区(swap space)。设置完交换分区后，内存不足时系统会自动使用交换分区作内存用。 在 linux 上创建并使用交换分区是一件非常简单的事情：\n1 2 3 4 5 sudo fallocate -l 20G /mnt/.swapfile # 创建 20G 空文件 sudo mkswap /mnt/.swapfile # 转换为交换分区文件 sudo chmod 600 /mnt/.swapfile # 修改权限为 600 sudo swapon /mnt/.swapfile # 激活交换分区 free -m # 查看当前内存使用情况(包括交换分区) 关闭交换分区也非常简单：\n1 2 sudo swapoff /mnt/.swapfile rm -rf /mnt/.swapfile 磁盘的 I/O 读写性能和内存条相差是非常大的，例如 DDR3 的内存条读写速率很容易达到 20GB/s，但是 SSD 固态硬盘的读写性能通常只能达到 0.5GB/s，相差 40倍之多。因此，使用虚拟内存技术将硬盘映射为内存使用，显然会对性能产生一定的影响。如果应用程序只是在较短的时间内需要较大的内存，那么虚拟内存能够有效避免 out of memory (内存不足) 的问题。如果应用程序长期高频率读写大量内存，那么虚拟内存对性能的影响就比较明显了。\nsync.Pool 复用对象 通过保存和复用临时对象，减少内存分配，降低GC垃圾回收压力,sync.Pool主要就是复用一个临时变量，避免频繁的创建临时结构体来承载数据，造成极大的GC压力和不必要的内存，它是并发安全的，所以可以多协程共用。\n常用于网络包收取发送的时候，因为收取发送时需要频繁的反序列化，如果每次反序列化时都是一个新的临时变量，在高并发时，会造成极大的GC压力，采用sync.Pool的话，从池中取曾经存在的对象(不存在才new一个)就可以极大的减少GC压力。\n例如json 的反序列化在文本解析和网络通信过程中非常常见，当程序并发度非常高的情况下，短时间内需要创建大量的临时对象来承载反序列化的数据。而这些对象是都是分配在堆上的，会给 GC 造成很大压力，严重影响程序的性能。\nsync.Pool 是可伸缩的，同时也是并发安全的，其大小仅受限于内存的大小。sync.Pool 用于存储那些被分配了但是没有被使用，而未来可能会使用的值。这样就可以不用再次经过内存分配，可直接复用已有对象，减轻 GC 的压力，从而提升系统的性能。\nsync.Pool 的大小在高负载时会动态扩容，存放在池中的对象如果不活跃了会被自动清理。\n使用方法 声明对象池 1 2 3 4 5 var studentPool = sync.Pool{ New: func() interface{} { return new(Student) }, } 只需要实现 New 函数即可。对象池中没有对象时，将会调用 New 函数创建。\n1 2 3 4 5 6 7 #### Get \u0026amp; Put stu := studentPool.Get().(*Student) json.Unmarshal(buf, stu) studentPool.Put(stu) // 池中不止可以放一种对象，可以同时存放不同的对象，但对Get()来说取后就需要判断了 par := new(Parent) studentPool.Put(par) Get() 用于从对象池中获取对象，因为返回值是 interface{}，因此需要类型转换。 Put() 则是在对象使用完毕后，返回对象池。 sync.Pool 作为使用方不能对 Pool 里面的对象个数做假定，同时也无法获取 Pool 池中对象个数。可以往池中Put发送多个对象，但是Get()时是随机取出对象，无法保证以固定的顺序获取Pool池中的存储对象。\n没有配置 New 方法时，如果 Get 操作多于 Put 操作，继续 Get 会得到一个 nil interface{} 对象，所以需要配置New()代码进行兼容。\n配置 New 方法后，Get 获取不到对象时（Pool 池中已经没有对象了），会调用自定义的 New 方法创建一个对象并返回。需要注意的是，sync.Pool 本身数据结构是并发安全的，但是 Pool.New 函数（用户自定义的）不一定是线程安全的。Pool.New 函数可能会被并发调用，如果 New 函数里面的实现逻辑是 非并发安全的，那就会有问题。\nsync.Pool不适合存储带状态的对象，因为获取对象是随机的 (Get 到的对象可能是刚创建的，也可能是之前创建并 cache 住的)，并且缓存对象的释放策略完全由 runtime 内部管理，你无法确定此次获取的数据是否是自己需要的，也许是之前未被取出，还未释放的数据。\nsync.Once 如何提升性能 sync.Once 是 Go 标准库提供的使函数只执行一次的实现，常应用于单例模式，例如初始化配置、保持数据库连接等。作用与 init 函数类似，但有区别。\ninit 函数是当所在的 package 首次被加载时执行，若执行后init()中的全局变量迟迟未被使用，则既浪费了内存，又延长了程序加载时间。 sync.Once 可以在代码的任意位置初始化和调用，因此可以延迟到需要使用时再执行，并发场景下是线程安全的。 在多数情况下，sync.Once 被用于控制变量的初始化，这个变量的读写满足如下三个条件：\n当且仅当第一次访问某个变量时，进行初始化（写）； 变量初始化过程中，其他执行该sync.Once.Do()的协程会发生阻塞，直到初始化完成，保证所有协程都能拿到初始化后的值； 变量仅初始化一次，初始化完成后驻留在内存里。 sync.Once 仅提供了一个方法 Do，参数 f 是对象初始化函数。\n1 func (o *Once) Do(f func()) 如果不使用sync.One，并发运行初始化时，每次都构造出一个新的对象，既浪费内存，又浪费初始化时间。如果初始化时不加锁，初始化全局变量就可能出现并发冲突。这种情况下，使用 sync.Once 既能够保证全局变量初始化时是线程安全的，又能节省内存和初始化时间。\nsync.Once 的原理 首先要保证变量仅被初始化一次，那么就需要有一个标志来判断变量是否已经初始化过，若没有才需要初始化。\n其次就是保证线程安全，支持并发，这无疑需要互斥锁来实现。\n源码 (代码位于 $(dirname $(which go))/../src/sync/once.g)：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 package sync import ( \u0026#34;sync/atomic\u0026#34; ) type Once struct { // 标志位，判断是否已经初始化 done uint32 m Mutex } func (o *Once) Do(f func()) { // 原子操作判断是否初始化过，标志位设置为uint32就是因为atomic的最小单位就是32，没有uint8 if atomic.LoadUint32(\u0026amp;o.done) == 0 { o.doSlow(f) } } func (o *Once) doSlow(f func()) { // 加锁避免并发问题，不再Do函数加锁的原因就是加快速度， // 如果在Do中加锁，那么即使初始化过后，以后执行Do方法依然需要加锁 o.m.Lock() defer o.m.Unlock() // Once都是同一个，因此在原子操作设置完后，这个done就为1了 if o.done == 0 { // 设置标志为已初始化 defer atomic.StoreUint32(\u0026amp;o.done, 1) f() } } done 是在热路径中的，done 放在第一个字段，能够减少 CPU 指令，也就是说，这样做能够提升性能\n热路径(hot path)是程序非常频繁执行的一系列指令，sync.Once 绝大部分场景都会访问 o.done (判断是否执行过)，在热路径上是比较好理解的，如果 hot path 编译后的机器码指令更少，更直接，必然是能够提升性能的 为什么放在第一个字段就能够减少指令呢？ 因为结构体第一个字段的地址和结构体的指针是相同的，如果是第一个字段，直接对结构体的指针解引用即可。 如果是后面的字段，除了结构体指针外，还需要计算与第一个值的偏移(calculate offset)。在机器码中，偏移量是随指令传递的附加值，CPU 需要做一次偏移值与指针的加法运算，才能获取要访问的值的地址。因此，访问第一个字段的机器代码更紧凑，速度更快。 sync.Cond 条件变量 sync.Cond 条件变量 用来协调想要访问共享资源的那些 goroutine，当共享资源的状态发生变化的时候，它可以用来通知被互斥锁阻塞的 goroutine。\nsync.Cond 基于互斥锁/读写锁，但是它和锁是有区别的。\n锁通常用来保护临界区和共享资源，而sync.Cond是用来协调想要访问共享资源的goroutine。\nsync.Cond 经常用在多个goroutine等待，一个goroutine通知（事件发生）的场景。如果是一个通知一个等待，使用互斥锁或channel就能搞定的。\n比如，有一个协程在异步地接收数据，剩下的多个协程必须等待这个协程接收完数据，才能读取到正确的数据。在这种情况下，如果单纯使用 chan 或互斥锁，那么只能有一个协程可以等待，并读取到数据，没办法通知其他的协程也读取数据。\n这个时候，就需要有个全局的变量来标志第一个协程数据是否接受完毕，剩下的协程，反复检查该全局变量的值，直到满足要求。或者创建多个 channel，每个协程阻塞在一个 channel 上，由接收数据的协程在数据接收完毕后，逐个通知。总之，需要额外的复杂度来完成这件事。\nsync.Cond 的四个方法 sync.Cond 的定义如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // Each Cond has an associated Locker L (often a *Mutex or *RWMutex), // which must be held when changing the condition and // when calling the Wait method. // // A Cond must not be copied after first use. type Cond struct { noCopy noCopy // L is held while observing or changing the condition L Locker notify notifyList checker copyChecker } 每个 Cond 实例都会关联一个锁 L（互斥锁 *Mutex，或读写锁 *RWMutex），当修改条件变量或者调用 Wait 方法时，必须加锁。\nNewCond 创建实例 1 func NewCond(l Locker) *Cond NewCond 创建 Cond 实例时，需要通过参数来关联一个锁。\nBroadcast 广播唤醒所有 1 2 3 4 5 // Broadcast wakes all goroutines waiting on c. // // It is allowed but not required for the caller to hold c.L // during the call. func (c *Cond) Broadcast() Broadcast 唤醒所有等待条件变量 c 的 goroutine，无需锁保护。\nSignal 唤醒一个协程 1 2 3 4 5 // Signal wakes one goroutine waiting on c, if there is any. // // It is allowed but not required for the caller to hold c.L // during the call. func (c *Cond) Signal() Signal 只唤醒任意 1 个等待条件变量 c 的 goroutine，无需锁保护。BroadCast和Signal都只是唤醒正在Wait的协程，如果没有，那么就相当于不起作用\nWait 等待 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // Wait atomically unlocks c.L and suspends execution // of the calling goroutine. After later resuming execution, // Wait locks c.L before returning. Unlike in other systems, // Wait cannot return unless awoken by Broadcast or Signal. // // Because c.L is not locked when Wait first resumes, the caller // typically cannot assume that the condition is true when // Wait returns. Instead, the caller should Wait in a loop: // // c.L.Lock() // for !condition() { // c.Wait() // } // ... make use of condition ... // c.L.Unlock() // func (c *Cond) Wait() 调用 Wait 会自动释放锁 c.L，并挂起调用者所在的 goroutine，因此当前协程会阻塞在 Wait 方法调用的地方。如果其他协程调用了 Signal 或 Broadcast 唤醒了该协程，那么 Wait 方法在结束阻塞时，会重新给 c.L 加锁，并且继续执行 Wait 后面的代码。\n对条件变量的检查，要使用 for !condition() 而非 if，是因为当前协程被唤醒时，条件不一定符合要求，需要再次 Wait 等待下次被唤醒。为了保险起见，使用 for 能够确保条件符合要求后，再执行后续的代码。\n有点类似于sync.WaitGroup，只不过WaitGroup是某一个协程等待其他协程全部结束后再继续往下走，sync.Cond是多个协程等待某一个协程执行完成后才能继续执行。\n示例： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 // done 即互斥锁需要保护的条件变量 var done = false // 调用 Wait() 等待通知，直到 done 为 true，调用Wait()时必须加锁 func read(name string, c *sync.Cond) { // c.L.Lock()是为了Wait函数内部并发安全的将当前协程加入Cond的通知队列， // 之后会解锁并挂起等待通知唤醒，唤醒后在Wait()方法结束时会重新给c.L 加锁。 c.L.Lock() // 某个wait协程可能会把资源状态改回不可用， // 所以其他wait协程需要在wait结束后for循环重新判断条件变量是否符合， // 如果被改回不可用就再次阻塞， // 如果确定所有wait协程都是只读不写的话，用if也可以。 for !done { // wait()获得通知后，并不会立即就退出函数了，wait内部会去抢占c.L锁， // 以确保对共享资源(条件变量) 的并发安全访问。 // 只有当前协程抢占到c.L锁后才会从wait()退出。 // 访问完条件变量后需要显式的c.L.Unlock()去解锁wait()退出前的锁定 c.Wait() } // c.L.Unlock()并不是配对上面的那个c.L.Lock()，而是配对的wait()内部的加锁。 c.L.Unlock() log.Println(name, \u0026#34;starts reading\u0026#34;) } // 接收数据，接收完成后，将 done 置为 true，调用 Broadcast() 通知所有等待的协程 func write(name string, c *sync.Cond) { log.Println(name, \u0026#34;starts writing\u0026#34;) // 模拟耗时，另一方面是确保前面的 3 个 read 协程都执行到 time.Sleep(time.Second) // 修改条件变量时必须加锁 c.L.Lock() done = true c.L.Unlock() log.Println(name, \u0026#34;wakes all\u0026#34;) // 广播时，如果没有其他协程正在wait这个cond，那么这个广播相当于就被丢弃了 c.Broadcast() } func main() { cond := sync.NewCond(\u0026amp;sync.Mutex{}) go read(\u0026#34;reader1\u0026#34;, cond) go read(\u0026#34;reader2\u0026#34;, cond) go read(\u0026#34;reader3\u0026#34;, cond) write(\u0026#34;writer\u0026#34;, cond) time.Sleep(time.Second * 3) } channel也能作为广播使用,close(ch)关闭时，其他协程都可以获取它的零值，但是close有一个问题是它不能复用，无法对close的channel再次close，而sync.Cond是可以复用的\n减小编译体积 减小编译后的二进制文件体积，能够加快程序的发布和安装过程。\n编译选项 Go 编译器默认编译出来的程序会带有符号表和调试信息，一般来说 release 版本可以去除调试信息以减小二进制体积。\n1 go build -ldflags=\u0026#34;-s -w\u0026#34; -o server main.go -s：忽略符号表和调试信息。 -w：忽略DWARFv3调试信息，使用该选项后将无法使用gdb进行调试。 使用 upx 减小体积 upx 是一个常用的压缩动态库和可执行文件的工具，通常可减少 50-70% 的体积。\nupx 的安装方式非常简单，我们可以直接从 github 下载最新的 release 版本，支持 Windows 和 Linux，在 Ubuntu 或 Mac 可以直接使用包管理工具安装。\nupx 有很多参数，最重要的则是压缩率，1-9，1 代表最低压缩率，9 代表最高压缩率。\n1 2 3 4 5 6 $ go build -o server main.go \u0026amp;\u0026amp; upx -9 server File size Ratio Format Name -------------------- ------ ----------- ----------- 10253684 -\u0026gt; 5210128 50.81% macho/amd64 server $ ls -lh server -rwxr-xr-x 1 dj staff 5.0M Dec 8 00:45 server 可以看到，使用 upx 后，可执行文件的体积从 9.8M 缩小到了 5M，缩小了 50%。\n然后再此基础上可以再加上编译选项进一步压缩可执行文件的大小。\n1 $ go build -ldflags=\u0026#34;-s -w\u0026#34; -o server main.go \u0026amp;\u0026amp; upx -9 server upx 的原理 upx 压缩后的程序和压缩前的程序一样，无需解压仍然能够正常地运行，这种压缩方法称之为带壳压缩，压缩包含两个部分：\n在程序开头或其他合适的地方插入解压代码； 将程序的其他部分压缩。 执行时，也包含两个部分：\n首先执行的是程序开头插入的解压代码，将原来的程序在内存中解压出来； 再执行解压后的程序。 也就是说，upx 在程序执行时，会有额外的解压动作，不过这个耗时几乎可以忽略不计。\n如果对编译后的体积没什么要求的情况下，可以不使用 upx 来压缩。一般在服务器端独立运行的后台服务，无需压缩体积。\n分析内存逃逸对性能的影响 Go程序会在2个地方为变量分配内存，一个是 全局 的堆(heap)空间用来动态分配内存，另一个是 每一个goroutine 的栈(stack)空间。\nGo语言实现了垃圾回收(Garbage Collector)机制，因此Go语言的内存管理是自动的，通常开发者并不需要关心内存是分配在栈上还是堆上。但是从性能的角度出发，在栈上分配内存和在堆上分配内存，两者的性能差异是非常大的。\n在函数中申请一个对象，如果分配在栈中，函数执行结束时自动回收。但是如果分配在堆中，则在函数结束后某个时间点进行垃圾回收。\n在栈上分配和回收内存的开销很低，只需要2个 CPU指令: PUSH和POP，一个是将数据PUSH到栈空间以完成分配，POP则是释放空间，也就是说在栈上分配内存，消耗的仅仅是将数据拷贝到内存的时间，而内存的I/O通常能够达到 30GB/s，因此在栈上分配内存的效率是非常高的。\n在堆上分配内存，一个很大的额外开销则是垃圾回收。Go语言使用的是标记清除算法，并在此基础上使用了三色标记法和写屏障技术来提高效率。\n1 2 3 4 标记清除收集器是跟踪式垃圾收集器，其执行过程可以分成标记（Mark）和清除（Sweep）两个阶段： * 标记阶段 — 从根对象出发查找并标记堆中所有存活的对象； * 清除阶段 — 遍历堆中的全部对象，回收未被标记的垃圾对象并将回收的内存加入空闲链表。 标记清除算法的一个典型耗时是在标记期间，需要暂停程序（Stop the world，STW），标记结束之后，用户程序才可以继续执行。 因此，堆内存分配由于垃圾回收的原因导致其开销远远大于栈空间分配与释放的开销。\n逃逸分析 在Go语言中，堆内存是通过垃圾回收机制自动管理的。那么Go编译器怎么知道某个变量要分配在堆上还是栈上呢？编译器决定内存分配位置的方式就称之为逃逸分析(Escape Analysis)。逃逸分析由编译器完成，作用于编译阶段。\n指针逃逸 指针逃逸即在函数中创建了一个对象，并将这个对象的指针通过return返回了出去。这种情况下，函数虽然退出了，但是因为指针还存在在函数外部，这个对象的内存不能随着函数的结束而回收(避免外部有使用这个指针指向的对象时，却因为回收出现空指针异常)，因此只能分配在堆上，由垃圾回收机制管理。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 package main import \u0026#34;fmt\u0026#34; type Demo struct { name string } func createDemo(name string) *Demo { d := new(Demo) // 局部变量 d 逃逸到堆 d.name = name return d } func main() { demo := createDemo(\u0026#34;demo\u0026#34;) fmt.Println(demo) } 这个例子中，函数 createDemo 的局部变量 d 发生了逃逸。d 作为返回值，在 main 函数中继续使用，因此 d 指向的内存不能够分配在栈上，随着函数结束而回收，只能分配在堆上。\n编译时可以借助选项 -gcflags=-m，查看变量逃逸的情况\n1 2 3 4 5 6 7 8 9 $ go build -gcflags=-m .\\main.go # command-line-arguments .\\main.go:9:6: can inline createDemo .\\main.go:16:20: inlining call to createDemo .\\main.go:17:13: inlining call to fmt.Println .\\main.go:9:17: leaking param: name .\\main.go:10:10: new(Demo) escapes to heap .\\main.go:16:20: new(Demo) escapes to heap .\\main.go:17:13: ... argument does not escape new(Demo) escapes to heap 即表示 new(Demo) 逃逸到堆上了。\ninterface{} 动态类型逃逸 在Go语言中，空接口 interface{} 可以表示任意的类型，如果函数的参数或返回值为 interface{}，编译期间很难确定其参数的具体类型，也会发生逃逸。\n1 2 3 func main() { fmt.Println(\u0026#34;hello world!\u0026#34;) } 因为 fmt.Println() 接收的参数是空接口类型，Go 编译器无法确定入参变量的具体类型，所以此类情况变量也会逃逸到堆上\n栈空间不足逃逸 操作系统对内核线程使用的栈空间是有大小限制的，64位系统上通常是8MB。可以使用 ulimit -a命令查看机器上栈允许占用的内存的大小。\n1 2 3 $ ulimit -a -s: stack size (kbytes) 8192 -n: file descriptors 12800 由于栈空间通常比较小，因此当递归函数实现不当时，很容易导致栈溢出。\n对应Go语言来说，运行时(runtime) 会尝试在goroutine需要的时候动态地分配栈空间，goroutine的初始栈大小为2KB。当goroutine被调度时，会绑定内核线程执行，栈空间大小也不会超过操作系统的限制。\n对Go编译器而言，超过一定大小的局部变量将逃逸到堆上，不同Go版本的大小限制可能不一样。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 func generate8191() { nums := make([]int, 8191) // \u0026lt; 64KB for i := 0; i \u0026lt; 8191; i++ { nums[i] = rand.Int() } } func generate8192() { nums := make([]int, 8192) // = 64KB 64 * 1024 / 8 for i := 0; i \u0026lt; 8192; i++ { nums[i] = rand.Int() } } func generate(n int) { nums := make([]int, n) // 不确定大小 for i := 0; i \u0026lt; n; i++ { nums[i] = rand.Int() } } func main() { generate8191() generate8192() generate(1) } generate8191() 创建了大小为 8191 的 int 型切片，恰好小于 64 KB(64位机器上，int 占 8 字节)，不包含切片内部字段占用的内存大小。 generate8192() 创建了大小为 8192 的 int 型切片，恰好占用 64 KB。 generate(n)，切片大小不确定，调用时传入。 编译结果如下：\n1 2 3 4 5 $ go build -gcflags=-m main_stack.go # command-line-arguments ./main_stack.go:9:14: generate8191 make([]int, 8191) does not escape ./main_stack.go:16:14: make([]int, 8192) escapes to heap ./main_stack.go:23:14: make([]int, n) escapes to heap make([]int, 8191) 没有发生逃逸，make([]int, 8192) 和make([]int, n) 逃逸到堆上，也就是说，当切片占用内存超过一定大小，或编译时无法确定当前切片长度时，对象占用内存将在堆上分配。\n闭包 1 一个函数和对其周围状态的引用捆绑在一起（或者说函数被引用包围），这样的组合就是闭包（closure）。也就是说，闭包让你可以在一个内层函数中访问到其外层函数的作用域。 例如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 func Increase() func() int { n := 0 return func() int { n++ return n } } func main() { in := Increase() fmt.Println(in()) // 1 fmt.Println(in()) // 2 } Increase() 返回值就是一个闭包函数，该闭包函数访问了外部变量n，那变量n将一直存在，直到in 被销毁。很显然，变量n 占用的内存不能随着函数Increase() 的退出而回收，因此将会逃逸到堆上。\n1 2 3 $ go build -gcflags=-m main_closure.go # command-line-arguments ./main_closure.go:6:2: moved to heap: n 如何利用逃逸分析提升性能 函数参数和返回值 传值时会拷贝整个对象，而传指针只会拷贝指针地址，指向的对象是同一个对象。但是传指针虽然可以减少值的拷贝，却会导致对象逃逸到堆中，增加垃圾回收(GC)的负担。在对象频繁创建和删除的场景下，传递指针导致的GC开销可能会严重影响性能。\n一般情况下，对于需要修改原对象值或者占用内存比较大的结构体会选择指针传递(避免对象拷贝，并且大结构体也可能会因为栈空间不足发生逃逸，既拷贝也要逃逸，那直接使用指针性能会更好)。对于只读的占用内存较小的结构体，直接传值能够获得更好的性能(避免频繁的指针逃逸影响GC性能)。\n死码消除与调试模式 死码消除 死码消除(dead code elimination, DCE) 是一种编译器优化技术，用处是在编译阶段去掉对程序运行结果没有任何影响的代码。\n死码消除有很多好处：减小程序体积，程序运行过程中避免执行无用的指令，缩短运行时间。\nGo 语言中的应用 使用常量提升性能 在某些场景下，将变量替换为常量的话，性能会有很大的提升。\n例如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func max(num1, num2 int) int { if num1 \u0026gt; num2 { return num1 } return num2 } var a, b = 10, 20 func main() { if max(a, b) == a { fmt.Println(a) } } 将var改为const后分别编译会发现两者大小有差距：\n1 2 3 4 5 go build -o maxvar maxvar.go go build -o maxconst maxconst.go ls -l maxvar maxconst -rwxr-xr-x 1 x x 1895424 Jan 10 00:01 maxconst -rwxr-xr-x 1 x x 2120368 Jan 10 00:01 maxvar 可以看到 maxconst 比 maxvar 体积小了约 10% = 0.22 MB。\n为什么会出现 11% 的差异呢？\n我们使用 -gcflags=-m 参数看一下编译器做了哪些优化：\n1 2 3 4 go build -gcflags=-m -o maxvar maxvar.go # command-line-arguments ./maxconst.go:7:6: can inline max ./maxconst.go:17:8: inlining call to max max 函数被内联了，即被展开了，手动展开后如下：\n1 2 3 4 5 6 7 8 9 10 11 func main() { var result int if a \u0026gt; b { result = a } else { result = b } if result == a { fmt.Println(a) } } 如果a和b均为常量的话，那在编译阶段就可以直接进行计算：\n1 2 3 4 5 6 7 8 9 10 11 func main() { var result int if 10 \u0026gt; 20 { result = 10 } else { result = 20 } if result == 10 { fmt.Println(a) } } 计算之后，10 \u0026gt; 20 永远为假，那么分支消除后：\n1 2 3 4 5 func main() { if 20 == 10 { fmt.Println(a) } } 进一步，20 == 10 也永远为假，再次分支消除：\n1 func main() {} 如果全局变量a和b不为常量，编译器并不知道运行过程中ab会不会发生改变，因此不能进行死码消除，这部分代码会被编译到最终的二进制程序中。因此maxvar 比 maxconst 二进制体积大了约 10%。\n如果在if语句中调用了更多的库，死码消除后，体积差距会更大。\n因此在声明全局变量时，如果能够确定这个变量不会发生改变，那么尽量使用const而非var，这样很多运行代码在编译时期就可以执行。死码消除后，既减小了二进制的体积，又可以提高运行时效率，如果这部分代码是 热路径 (hot path)，那么对性能的提升会更加明显。\n可推断的去局部变量 如果ab作为局部变量呢\n1 2 3 4 5 6 7 // maxvarlocal func main() { var a, b = 10, 20 if max(a, b) == a { fmt.Println(a) } } 编译结果如下，大小与 varconst 一致，即 a、b 作为局部变量时，编译器死码消除是生效的。因为ab局部变量只作用在该函数内部，是可以推断出它的值大小的，就可以直接进行死码消除。\n1 2 3 $ go build -o maxvarlocal maxvarlocal.go $ ls -l maxvarlocal -rwxr-xr-x 1 x x 1895424 Jan 10 00:05 maxvarlocal 那如果再修改一下，函数中增加修改 a、b 变量的并发操作。\n1 2 3 4 5 6 7 8 9 func main() { var a, b = 10, 20 go func() { b, a = a, b }() if max(a, b) == a { fmt.Println(a) } } 编译结果如下，大小增加了 10%，此时，a、b 的值不能有效推断，死码消除失效。\n1 2 3 $ go build -o maxvarlocal maxvarlocal.go $ ls -l maxvarlocal -rwxr-xr-x 1 x x 2120352 Jan 10 00:05 maxvarlocal 其实这个结果很好理解，包(package)级别的变量和函数内部的局部变量的推断难度是不一样的。函数内部的局部变量的修改只会发生在该函数中(注意内部这个词，有协程修改它就不属于内部了)。但是如果是包级别的变量，对该变量的修改可能出现在：\n包初始化函数 init() 中，init() 函数可能有多个，且可能位于不同的 .go 源文件。 包内的其他函数。 如果是 public 变量（首字母大写），其他包引用时可修改。 推断 package 级别的变量是否被修改难度是非常大的，从上述的例子看，Go 编译器只对局部变量作了优化，如果这个局部变量被协程引用，即被外部引用，那么它就会像全局变量一样无法进行死码消除。\n由此可知：只有在其为常量(局部常量也是常量，它无法被外部函数修改)和只作用于当前函数内部的局部变量才会进行死码消除，其他情况由于无法在编译时期有效推断出具体的值，因此无法进行死码消除。\n调试(debug)模式 我们可以在源代码中定义全局常量debug，值设置为false，在需要增加调试代码的地方，使用条件语句 if debug 包裹：\n1 2 3 4 5 6 7 const debug = false func main() { if debug { log.Println(\u0026#34;debug mode is enabled\u0026#34;) } } 如果是正常编译，常量debug始终等于false，调试语句在编译过程中就会被当做死码消除，不会影响最终的二进制大小，也不会对运行效率产生任何影响。\n如果想编译出debug版本的二进制程序就只需要将debug 修改为true编译即可。这对于开发者日常调试是非常有帮助的，日常开发过程中，在进行单元测试或者是简单的集成测试时，总会执行一些额外的操作，例如打印日志，或者是修改变量的值。提交代码时再将debug修改为false即可，这样在开发过程中增加的额外的调试代码在编译时期就会被消除，不会对正式版本产生任何影响。\n条件编译 可以结合 build tags 来实现条件编译，可以不修改源代码也能编译出debug版本。\n新建 release.go 和 debug.go ：\ndebug.go 1 2 3 4 5 // +build debug package main const debug = true release.go 1 2 3 4 5 // +build !debug package main const debug = false // +build debug 表示 build tags 中包含 debug 时，该源文件参与编译。 // +build !debug 表示 build tags 中不包含 debug 时，该源文件参与编译。 编译一个 debug 版本并运行：\n1 2 3 $ go build -tags debug -o debug . $ ./debug 2021/01/11 00:10:40 debug mode is enabled 编译 release 版本并运行：\n1 2 3 $ go build -o release . $ ./release # 无输出 除了全局布尔值常量 debug 以外，debug.go和release.go还可以根据需要添加其他代码。比如相同的函数定义，debug 和release 模式下有不同的函数实现。\n一个源文件中可以有多个build tags，同一行的空格隔开的tag之间是逻辑或的关系。不同行之间的tag是逻辑与的关系。\n例如：\n1 2 // +build linux darwin // +build 386 这种写法表示此源文件只能在linux_386 或者 darwin_386 平台下编译。\n数组切片陷阱 数组 1 2 3 4 5 6 7 8 9 func foo(a [2]int) { a[0] = 200 } func main() { a := [2]int{1, 2} foo(a) fmt.Println(a) } 上面代码的输出为 [1,2]，数组a没有发生改变。\n在Go语言中，数组是一种值类型，而且不同长度的数组属于不同的类型。例如 [2]int和[20]int 属于不同的类型。 当值类型作为参数传递时，参数是该值的一个拷贝，因此更改拷贝的值并不会影响原值。 为了避免数组的拷贝，提高性能，建议传递数组的指针作为参数，或者直接使用切片代替数组。\n切片 1 2 3 4 5 6 7 8 9 10 func foo(a []int) { a = append(a, 1, 2, 3, 4, 5, 6, 7, 8) a[0] = 200 } func main() { a := []int{1, 2} foo(a) fmt.Println(a) } 输出仍然是 [1,2]，切片a没有发生改变。 传参时拷贝了切片，包括底层指针，此时两者指向的是同一个底层数组，但是在foo()中切片新增了8个元素，原切片的cap只有2，不够放置这些元素，因此申请了新的空间来放置扩充后的底层数组，导致函数体中的切片底层指针指向新的空间，此时foo中的a切片和外部的a切片就不是同一个了(引用类型拷贝的是指针，两者指针不同，只是最初都指向相同的底层数组罢了)。因此对新切片的修改不会影响到原切片。\n如果希望foo函数的操作能够影响到原切片的话，可以给foo() 设置返回值，将新切片返回并赋值给 main 函数中的变量 a;也可以将参数改为指针传参 foo(a *[]int)。从可读性上来说，更推荐返回值的方式。\n","date":"2023-03-24T00:00:00Z","permalink":"https://xiaonuoz.github.io/p/golang-%E4%BB%A3%E7%A0%81%E4%BC%98%E5%8C%96/","title":"golang 代码优化"},{"content":"适用于mongodb的业务场景：\n数据量大 写入操作频繁（读写都很频繁） 价值较低的数据，对事物要求不高（mongodb对于事物的支持不是很好） 什么时候选择mongodb:\n应用不需要事物及复杂的join支持 新应用，需求会变，数据模型无法确定，想快速迭代开发 应用需要高读写速率，需要2000-3000以上的读写QPS 应用需要TB及PB级别的数据存储（维护成本较mysql低） 应用要求存储的数据不丢失 应用需要99.9999…%高可用 应用需要大量的地理位置查询、文本查询 如果上述有一个符合就可以考虑使用mongodb，2个以上直接选择使用mongodb就行\n基础mongodb命令 1 2 3 4 mongod --dbpath=../data/db // 在debug时可以这样启动 mongod -f ./mongod.conf\t// 以配置文件的方式启动mongodb mongod --config ./mongod.conf 1 2 mongo 或 mongo --host=127.0.0.1 --port=27017 // 命令行连接mongodb，只mongo则是连接本地27017 配置文件内容：\n1 2 3 4 关闭mongodb可以使用kill，但可能会出现数据错误，因此可以进入mongodb来关闭， mongo --port 27017 use admin db.shutdownServer() CRUD 数据库操作 在每个集合中都会存在一个 _id，这个是mongdb的主键，它自己生成的\n显示数据库 1 2 3 show dbs 或者 show databases 显示所有数据库 db 显示当前所处数据库 选择和创建数据库 1 use 数据库名称 如果库不存在则自动创建，有则切换到此库\n要注意的是，在mongodb中，要新建了一个集合后才会真正的创建一个库。\n原因是use后这个库会先存在于内存中，只有当往这个库中写入了数据，mongodb才会持久化这个库到磁盘中。\n删除数据库 1 2 db.dropDatabase() 删除当前所在的库，因此删除前需要先use 默认库的作用 admin: 从权限的角度来看，这就类似于mysql的root数据库，如果将一个用户添加到这个数据库中，这个用户就自动继承所有数据库的权限。一些特定的服务器命令也只能在这个数据库中运行，比如关闭服务器。 local: 这个数据库中的数据永远不会被复制，作用于集群，集群之间是会相互复制彼此的数据，当数据存于这个库中时，数据就不会被集群中的其他库所复制，可以用来存储限于本地单台服务器的任意集合。 config: 当mongodb用于分片设置时，config数据库在内部使用，用于保存分片的相关信息。 集合操作 集合类似于关系型数据库中的表。它可以显式创建也可以隐式创建。\n1 2 db.createCollection(name) 显式创建名称为name的集合 1 show tables 或 show collections 查看当前库中的表 当向一个集合中插入一个文档的时候，如果集合不存在，则会自动创建集合。通常使用隐式创建即可\n1 2 db.table.drop() 删除名为table的集合,删除成功返回true,反之false 文档基本CRUD 文档的数据结构和JSON基本一样，所有存储在集合中的数据都是BSON格式。\n单个文档的插入 使用insert()或save()方法向集合中插入文档，语法如下:\n1 2 3 4 5 6 7 8 # collection代表集合的名称 db.collection.insert( \u0026lt;document or array of ducuments\u0026gt;, { writeConcern:\u0026lt;document\u0026gt;, ordered:\u0026lt;boolean\u0026gt; } ) 1 2 3 4 5 db.comm.insertOne({\u0026#34;name\u0026#34;:\u0026#34;zs\u0026#34;,\u0026#34;age\u0026#34;:11}) 往comm集合中插入一条数据 db.comm.insertMany([{\u0026#34;name\u0026#34;:\u0026#34;zs\u0026#34;,\u0026#34;age\u0026#34;:11},{\u0026#34;name\u0026#34;:\u0026#34;ww\u0026#34;,\u0026#34;age\u0026#34;:12}]) 往comm集合中插入多条数据 要注意的是：当插入多条时如果某条数据插入失败，将会终止插入，但已经插入成功的数据不会回滚掉。\n因此批量插入时最好可以加上try-catch来进行异常捕捉处理\n1 2 3 4 5 try { db.comm.insertMany([{\u0026#34;name\u0026#34;:\u0026#34;zs\u0026#34;,\u0026#34;age\u0026#34;:11},{\u0026#34;name\u0026#34;:\u0026#34;ww\u0026#34;,\u0026#34;age\u0026#34;:12}]) } catch(e){ print(e) } 文档的查询 基本查询 1 2 db.comm.find() 查询comm中的所有数据 此时会发现在文档中会有一个叫_id的字段，这个相当于MySQL的主键ID，在插入文档时没有指定该字段则MongoDB会自动创建，其类型是ObjectID类型。当然在插入文档时也可以指定_id为自己设置的值，建议不要自己指定_id，因为重复后会很麻烦。\n1 2 db.comm.find({name:\u0026#34;ww\u0026#34;}) 根据条件进行查询 1 2 3 4 5 db.comm.findOne(name:\u0026#34;ww\u0026#34;) 只获取第一条数据，相当于mysql的limit mongodb的find().pretty() 使得查询出来的数据在命令行中更加美观的显示,不至于太紧凑。 投影查询 相当于mysql的select id,name from user，即只显示指定的字段，其中value为1则表示显示此key字段，为0则为不显示，_id默认是显示的，所以必须设置为0让其不显示，其他字段只要没有显式设置，默认都是不显示\n1 db.comm.find({},{name:1,_id:0}) 聚合查询 示例 lookup只支持left outer join\n文档的更新 1 2 db.collection.update(query,update,options) query是查询条件，update是修改内容，options是附加的一些选项 覆盖修改 1 db.comm.update({name:\u0026#34;ww\u0026#34;},{age:11}) update字段的age可以为文档中不存在的值，此时这个文档除了age和_id，其他字段都不见了，此为覆盖修改\n局部修改 1 db.comm.update({name:\u0026#34;ww\u0026#34;},{$set:{age:123}}) 只修改指定的字段，其他字段不变\n批量修改 覆盖修改和局部修改都只会修改一个文档，必须使用批量修改来改集合中满足条件的所有文档\n1 2 3 db.comm.updateMany({name:\u0026#34;zs\u0026#34;},{$set:{age:12}}) OR db.comm.update({name:\u0026#34;zs\u0026#34;},{$set:{age:13}},{multi:true}) 列值增长的修改 如果想对某列值在原有的基础上进行增加和减少，可以使用 $inc 运算符来实现。\n1 2 3 db.comm.updateMany({name:\u0026#34;ww\u0026#34;},{$inc:{age:2}}) 将集合中name=ww的所有文档的age加2 文档的删除 1 2 3 4 5 db.comm.deleteOne({agess:165}) 删除comm表中age=165的最前面的一条文档 db.comm.deleteMany({agess:165}) 删除comm表中age=165的所有文档 1 2 db.comm.remove({}) 删除该集合中的所有文档，慎用！ 文档的分页查询 统计查询 1 2 3 4 5 db.comm.count({age:11}) 查询age=11的文档的个数 db.comm.count() 查询该集合总共有多少个文档 分页列表查询 可以使用limit()方法来读取指定数量的数据，使用skip()方法来跳过指定数量的数据。limit默认是20，skip默认是0\n1 db.comm.find().limit(2).skip(1) 排序查询 sort()方法可以通过参数指定排序的字段来对数据进行排序，并用 1和-1来指定排序的方式，其中1为升序排列，-1为降序排列。\n1 2 db.comm.find().sort({age:1}) 查询comm所有文档并按照age升序排列 注意：\nskip(),limit(),sort()三个放一起执行的时候，执行顺序是sort(),skip(),limit()，和命令的编写顺序无关。\n复杂查询 正则的复杂条件查询 mongodb的模糊查询是通过正则表达式的方式实现的。其正则是用js的写法\n1 2 3 4 5 db.comm.find(name:/zs/) 查询name中包含zs的文档 db.comm.find(name:/^zs/) 查询以zs开头的name的文档 name后的value不要加引号，这表示此为字符串，变成等于了\n比较查询 包含查询 条件连接查询 索引 索引支持在mongodb中高效的执行查询。如果没有索引，mongodb必须执行全集合扫描，即扫描集合中的每个文档来选择与查询语句匹配的文档。这种扫描全集合的查询效率是非常低的，尤其是在处理大量数据时，查询可能要花费几十秒甚至几分钟，这对网站的性能是非常致命的。\n如果查询存在适当的索引，mongodb可以使用该索引限制必须检查的文档数。\n索引是特殊的数据结构，它以易于遍历的形式存储集合数据集的一小部分。索引存储特定字段或一组字段的值，按字段值排序。索引项的排序支持有效的相等匹配和基于范围的查询操作。还可以使用索引中的排序返回排序结果。\nmongodb索引使用的是B tree 数据结构，MySQL是B+ tree\n多路动态平衡树，简称b-tree，效率高，尤其是可以最大提高磁盘io效率，所以大部分数据库的存储引擎都用b-tree或者b-tree增强版作为索引的数据存储结构\n索引的查看 返回一个集合中的所有索引的数组。\n1 db.conn.getIndexes() 索引的创建 1 2 3 db.comm.createIndex({userid:1,age:-1},{unique:true}) 创建一个以userid升序，age降序的复合唯一索引，选项options是可以省略的 索引的移除 1 2 3 4 5 db.comm.dropIndex(\u0026#34;agess_1_age_-1\u0026#34;) 删除索引名 agess_1_age_-1的索引，也可以通过索引规则删除 db.comm.dropIndex({userid:1}) 删除仅以userid为升序的索引 1 2 3 db.comm.dropIndexes() 删除comm集合中的所有索引，但是mongodb自建的_id索引例外，它不会被删除 执行计划 1 db.comm.find(query,options).explain(options) 分析查询语句，查看是否用上了索引\n设计模式 分桶设计 每分钟写入一次，如果运行一年则数据量极大，采用分桶模式进行分组，因为展现数据一般不会基于一分钟进行展现，最低级别可能都是一小时，因此将其合为一组即可极大的优化数据容量和读取速度。一般适用于数据点采集频繁，数据量过多的场景中。\n列转行 导致创建新数据时要花费更多的数据，则可以采用列转行的方式，将相似的列变成一个列\n版本字段 近似计算 在所需要的计算非常有挑战性或消耗的资源昂贵（时间、内存、CPU周期）时，如果精度不是首要考虑因素时，那么我们就可以使用近似值模式。\n假设现在有一个相当规模的城市，大约有3.9万人。人口的确切数字是相当不稳定的，人们会搬入搬出、有婴儿会出生、有人会死亡。我们也许要花上整天的时间来得到每天确切的居民数量。在应用程序中，我们不需要每次更改都去更新数据库中的人口数。我们可以构建一个计数器，只在每达到100的时候才去更新数据库，这样只用原来1%的时间。在这个例子里，我们的写操作显著减少了99%。还有一种做法是创建一个返回随机数的函数。比如该函数返回一个0到100之间的数字，它在大约1%的时间会返回0。当这个条件满足时，我们就把计数器增加100。\n我们为什么需要关心这个？当数据量很大或用户量很多时，对写操作性能的影响也会变得很明显。规模越大，影响也越大，而当数据有一定规模时，这通常是你最需要关心的。通过减少写操作以及不必要的“完美”，可以极大地提高性能。\n预聚合字段 因为计算需要一行一行的扫描进行计算，因此可以采用预聚合模式\nmongodb事物 写事物 1 2 db.conn.insert({count:1},{writeConcern:{w:\u0026#34;majority\u0026#34;,wtimeout:3000}}) 执行此写入操作时，要落入到大多数节点上后才算成功。如果在某个节点写入时间超时3S则直接返回写入失败。但是数据已经写入到其他成功的节点中了 读事物 readPreference (选择读哪个节点的数据) 默认是primary，并且这是推荐的做法，因为这样读肯定是读到最新的数据。nearest是根据ping time来决定。\nreadConcern (隔离级别) 1 2 db.conn.find().readConcern(\u0026#34;local\u0026#34;) 以local的隔离级别进行查询,如果是majority,大多数从节点未同步完,查询就会阻塞，直到大多数从节点同步完数据。 如果要使用majority级别必须要在配置文件中配置enableMajorityReadConcern=true (记住所有写操作的位置和状态)\n多文档事物 (重要，平常代码中使用的事物) Change Stream (触发器) mongodb 代码编写规范 ","date":"2021-04-09T00:00:00Z","permalink":"https://xiaonuoz.github.io/p/mongodb%E5%85%A5%E9%97%A8/","title":"MongoDB入门"},{"content":"Golang调度器的由来 N:1的模型是在用户态有多个routine对应核心态一个线程，这导致它并不适应现在的多核情况，只能在一个CPU中进行调度，可能这个CPU很忙，但其他CPU还是空闲状态，因此出现了M:N的情况。而M:N重点在于我们无法对核心态进行优化，那么只能在语言层面优化，因此调度器的优化就显得尤为重要。\n早期的Goroutine调度器比较传统，它只有一个全局的队列，轮询给核心态thread去调用，那假如在这个Goroutine中还有Goroutine那该给哪个thread去调用，给其他的会导致上下文切换消耗，造成额外的系统负载，自己正在运行一个Goroutine无法分配，并且公共的队列会存在thread对队列锁的竞争，以及多个thread处于不同的内核中，频繁的切换，增加阻塞和取消阻塞增加了系统的开销。\nGMP模型 G 协程，很好理解，就是个goroutine的，里面除了存放本goroutine信息外 还有与所在P的绑定等信息。 P 处理器，管理着一组goroutine队列，P里面会存储当前goroutine运行的上下文环境（函数指针，堆栈地址及地址边界），P会对自己管理的goroutine队列做一些调度（比如把占用CPU时间较长的goroutine暂停、运行后续的goroutine等等）当自己的队列消费完了就去全局队列里取，如果全局队列里也消费完了会去其他P的队列里抢任务。 M（machine）内核线程，是Go运行时（runtime）对操作系统内核线程的虚拟， M与内核线程一般是一一映射的关系， 一个groutine最终是要放到M上执行的； 同一时刻并行的G数量就是P的数量，P会从队列中拿出G交给M执行，执行完一个时间片后再放回队列栈顶\nP的本地队列是存放当前P即将要执行的G，且P是有数量限制的，一般不超过256个G\n当新开一个G协程时，会优先分配给任意P的本地队列中，如果P队列满了才会尝试把这个G协程加这个本地队列中的前一半G一起放到全局队列中\nP列表是在程序启动时创建，最多有GOMAXPROCS配置的个数。M列表则是操作系统分配给当前Go程序的内核线程数。\n在同一时刻运行的总Goroutine数量最大等于CPU核心数，这也是为什么GOMAXPROCS默认值就是CPU核心数\n调度器的设计策略 复用线程：避免频繁的创建、销毁线程，主要是对线程的复用。 work stealing 机制：类似于偷取，如有两个P存在，第一个P中有两个G，然而第二个没有G，且全局G队列中没有G，此时1P有G可以运行，而2P是空闲的，为了避免2P的线程被销毁，2P就会尝试从1P中偷取G过来运行，偷取的是1P队列长度的后一半，它会将1P等分取后一半，偷队头万一即将执行呢。要注意的是这里的一半是总长度的后一半，并不是偷取1P中G总数的一半，假如容量为8，有6个G，它等分后偷取的是后两个。\nhand off 机制： 分离机制，如果一个P上的G长久阻塞，那么Go会唤醒一个新的M，将原本的P和它的队列跟新的M绑定。此时阻塞的G直接和M绑定，M处于睡眠/销毁状态。\n如果G在执行过程中阻塞，则会根据hand off 机制重新从休眠M队列(程序启动时分配的)获取，如果休眠M队列中没有M了，则会创建一个M，和原本的P进行绑定。然后将阻塞的那个G和原先的M进行绑定等待阻塞结束。 当G阻塞结束后，与G绑定的M会优先去寻找原本绑定的P是否和其他M绑定，如已和其他M绑定，则会去空闲P队列中寻找有没有空闲的P，两者任意一个满足则会获取P来执行这个恢复阻塞的G。如果都不满足的话，G会重新加入全局队列，全局队列满则会随机加入任意一个P本地队列，而M则放回休眠M队列，M队列满了则进行销毁。而M队列中的M如果长期没有被唤醒也会被销毁。 利用并行：通过GOMAXPROCS来限定P的个数，默认等于CPU的核心数，最好只用CPU核心数的一半，剩下的一半给其他程序运行 抢占：时间片到期则被其他G抢占运行 全局G队列：当P本地队列中没有G时，P会先从全局队列中拿取G，如果全局也没有则会从其他P中偷取。全局G队列是有锁的，拿取需要解锁，速度会慢一点。 在放入时优先放入执行go func函数的那个线程的P的本地队列中，如果是主函数则先放主函数的线程中，如果满了，则会打乱本地队列前一半的G，加这个线程一起放入全局队列中。\nmain函数也是一个协程，它在M0上，但不是M0的G0，G0是只负责调度的。\nM0的G0存放在全局变量中，其他M的G0放在自己的局部变量中。\nGMP可视化调试 G的创建时 开辟G时，它会优先放入本地队列，假如本地队列此时已满，则会将本地队列等分切割。将队首的多个G打乱然后加上新增的G一起放入全局队列中。这样的好处是负载均衡，避免一个P满而其他P为空或极少的情况，全局队列的G大概率会被不同的核运行，打乱顺序，能降低不同核同时修改同一缓存行的概率，且本地队列也会空出来一半。防止饥饿。\n当每次创建一个G时，它会尝试从休眠线程队列中唤醒一个M，如果有的话，M被唤醒，然后M会寻找有没有尚未绑定M的P(没有和M绑定的P会存在于空闲P队列中)，没有就会再次返回休眠，找到P之后两者进行绑定并初始化本地队列，此时称为自旋线程（没有G但为运行状态的线程），它会不断的寻找G，先从全局中找，全局没有则会去其他P队列中偷取。先从全局中取是避免一直偷取其他队列让全局中的G饥饿。虽然自旋也会浪费CPU的资源，但是如果不采用这种策略会导致线程销毁，更浪费资源。\n并且自旋线程的数量要满足 自旋线程+执行中线程\u0026lt;=GOMAXPROCS ，因为GOMAXPROCS指的是P的数量，即使有多余的M被G开辟出来，它也没有多余的P可供使用，即使开辟出M也会被放回休眠线程队列。\nGQ是全局队列的容量值\n","date":"2021-04-03T00:00:00Z","permalink":"https://xiaonuoz.github.io/p/gmp%E6%A8%A1%E5%9E%8B%E7%90%86%E8%A7%A3/","title":"GMP模型理解"},{"content":"UML类图 关联关系 链表就是自关联\n聚合关系 它是整体与部分的关系，且部分可以离开整体而单独存在。如车和轮胎是整体和部分的关系，轮胎离开车仍然可以存在。聚合关系是关联关系的一种，是强的关联关系；关联和聚合在语法上无法区分，必须考察具体的逻辑关系。\n组合关系 它是整体与部分的关系，如没有公司就不存在部门。组合关系是关联关系的一种，是比聚合关系还要强的关系，它要求普通的聚合关系中代表整体的对象负责代表部分的对象的生命周期\n依赖关系 对于两个相对独立的对象，当一个对象负责构造另一个对象的实例，或者依赖另一个对象的服务时，这两个对象之间主要体现为依赖关系。\n继承关系 继承表示是一个类（称为子类、子接口）继承另外的一个类（称为父类、父接口）的功能，并可以增加它自己的新功能的能力。\n实现关系 实现表示一个class类/struct结构体实现interface接口（可以是多个）的功能。\n软件设计原则 在软件开发中，为了提高软件系统的可维护性和可复用性，增加软件的可扩展性和灵活性，程序员要尽量根据6条原则来开发程序，从而提高软件的开发效率、节约软件开发成本和维护成本。\n开闭原则 对扩展开放，对修改关闭。\n**在程序需要进行扩展的时候，**不能去修改原有的代码，实现一个热拔插的效果。简言之，是为了使程序扩展性好，易于维护和升级。\n想要达到这样的效果，我们需要使用接口和抽象类（定义interface）。因为抽象灵活性好，实用性广，只要抽象的合理，可以基本保持软件架构的稳定。而软件中易变的细节可以在实现类中进行扩展，当软件需要发生变化时，只需要根据需求重新派生一个实现类来扩展就可以了。\n总之就是定义接口声明通用的方法。当有新的需求时再次实现对应接口的方法即可\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 // 总结构体，也许有其他属性,不只是皮肤 type Sougou struct { skin skinInterfaceI // ... } // interface : 定义抽象接口 type skinInterfaceI interface { disPlay() } type skinOne struct { Name string } func (skin skinOne) disPlay() { fmt.Println(skin.Name) } type skinTwo struct { Name string } func (skin skinTwo) disPlay() { fmt.Println(skin.Name) } func main() { // 接口赋值为对应实现类，则可以调用对应实现类的方法细节 sougou := Sougou{ skin: skinOne{ Name: \u0026#34;one\u0026#34;, }, } sougou.skin.disPlay() } 里氏代换原则 里氏代换原则是面向对象设计的基本原则之一。\n里氏代换原则：任何父类可以出现的地方，子类一定可以出现(即在代码中将父类替换为子类也可以输出预期的结果)。通俗理解：子类可以扩展父类的功能，但不能改变父类原有的功能。换句话说，子类继承父类时，除添加新的方法完成新增功能外，尽量不要重写父类的方法。只有不重写才能避免程序预期结果不对的可能性\n如果通过重写父类的方法来完成新的功能，这样写起来虽然简单，但是整个继承体系的可复用性会比较差。抽象类的实现类完成的功能与原定功能不一样会导致读代码的时候对功能模糊。\ngolang中的重写与java不同，golang重写后父方法依然存在。且golang属于强类型语言，自己定义的结构体是无法直接转换成其他结构体的。\n依赖倒置原则 依赖倒置原则的原始定义为：高层模块不应该依赖低层模块，两者都应该依赖其抽象；抽象不应该依赖细节，细节应该依赖抽象。其核心思想是：要面向接口编程，不要面向实现编程。\n依赖倒置原则是实现开闭原则的重要途径之一，它降低了客户与实现模块之间的耦合。\n由于在软件设计中，细节具有多变性，而抽象层则相对稳定，因此以抽象为基础搭建起来的架构要比以细节为基础搭建起来的架构要稳定得多。这里的抽象指的是接口或者抽象类，而细节是指具体的实现类。\n使用接口或者抽象类的目的是制定好规范和契约，而不去涉及任何具体的操作，把展现细节的任务交给它们的实现类去完成。\n多数情况下,这三个设计原则会同时出现，开闭原则是目标，里氏代换原则是基础，依赖倒转原则是手段，它们相辅相成，相互补充，目标一致。归根结底就是面向接口编程，上层依赖与接口，然后再传递具体的实现类来指明自己的身份\n单一职责原则 又称单一功能原则，这里的职责是指类变化的原因，单一职责原则规定一个类应该有且仅有一个引起它变化的原因，否则类应该被拆分。\n该原则提出对象不应该承担太多职责，如果一个对象承担了太多的职责，至少存在以下两个缺点：\n一个职责的变化可能会削弱或者抑制这个类实现其他职责的能力； 当客户端需要该对象的某一个职责时，不得不将其他不需要的职责全都包含进来，从而造成冗余代码或代码的浪费。 单一职责原则的核心就是控制类的粒度大小、将对象解耦、提高其内聚性。如果遵循单一职责原则将有以下优点。\n降低类的复杂度。一个类只负责一项职责，其逻辑肯定要比负责多项职责简单得多。 提高类的可读性。复杂性降低，自然其可读性会提高。 提高系统的可维护性。可读性提高，那自然更容易维护了。 变更引起的风险降低。变更是必然的，如果单一职责原则遵守得好，当修改一个功能时，可以显著降低对其他功能的影响 注意：单一职责同样也适用于方法。一个方法应该尽可能做好一件事情。如果一个方法处理的事情太多，其颗粒度会变得很粗，不利于重用 接口隔离原则 要求程序员尽量将臃肿庞大的接口拆分成更小的和更具体的接口，让接口中只包含客户感兴趣的方法。\n客户端不应该被迫依赖于它不使用的方法。该原则还有另外一个定义：一个类对另一个类的依赖应该建立在最小的接口上（接口中不一定是只有一个方法，一定要适度）。\n以上两个定义的含义是：要为各个类建立它们需要的专用接口，而不要试图去建立一个很庞大的接口供所有依赖它的类去调用。\n接口隔离原则和单一职责都是为了提高类的内聚性、降低它们之间的耦合性，体现了封装的思想，但两者是不同的：\n单一职责原则注重的是职责，而接口隔离原则注重的是对接口依赖的隔离。 单一职责原则主要是约束类，它针对的是程序中的实现和细节；接口隔离原则主要约束接口，主要针对抽象和程序整体框架的构建。\n接口隔离原则是为了约束接口、降低类对接口的依赖性，遵循接口隔离原则有以下 5 个优点。\n将臃肿庞大的接口分解为多个粒度小的接口，可以预防外来变更的扩散，提高系统的灵活性和可维护性。 接口隔离提高了系统的内聚性，减少了对外交互，降低了系统的耦合性。 如果接口的粒度大小定义合理，能够保证系统的稳定性；但是，如果定义过小，则会造成接口数量过多，使设计复杂化；如果定义太大，灵活性降低，无法提供定制服务，给整体项目带来无法预料的风险。 使用多个专门的接口还能够体现对象的层次，因为可以通过接口的继承，实现对总接口的定义。 能减少项目工程中的代码冗余。过大的大接口里面通常放置了许多不用的方法，那么当实现这个接口的时候，就会被迫实现冗余的无用代码。 迪米特法则 又叫作最少知识原则，迪米特法则的定义是：只与你的直接朋友交谈，不跟“陌生人”说话。其含义是：如果两个软件实体无须直接通信，那么就不应当发生直接的相互调用，可以通过第三方转发该调用。其目的是降低类之间的耦合度，提高模块的相对独立性。\n迪米特法则中的“朋友”是指：当前对象本身、当前对象的成员对象、当前对象所创建的对象、当前对象的方法参数等，这些对象同当前对象存在关联、聚合或组合关系，可以直接访问这些对象的方法。\n迪米特法则要求限制软件实体之间通信的宽度和深度，正确使用迪米特法则将有以下两个优点。\n降低了类之间的耦合度，提高了模块的相对独立性。 由于亲合度降低，从而提高了类的可复用率和系统的扩展性。 但是，**过度使用迪米特法则会使系统产生大量的中介类，从而增加系统的复杂性，使模块之间的通信效率降低。**所以，在釆用迪米特法则时需要反复权衡，确保高内聚和低耦合的同时，保证系统的结构清晰。\n合成复用原则 合成复用原则又叫组合/聚合复用原则。它要求在软件复用时，要尽量先使用组合或者聚合等关联关系来实现，其次才考虑使用继承关系来实现。\n如果要使用继承关系，则必须严格遵循里氏替换原则。合成复用原则同里氏替换原则相辅相成的，两者都是开闭原则的具体实现规范。\n通常类的复用分为继承复用和合成复用两种，继承复用虽然有简单和易实现的优点，但它也存在以下缺点。\n继承复用破坏了类的封装性。因为继承会将父类的实现细节暴露给子类，父类对子类是透明的，所以这种复用又称为“白箱”复用（子类可以重写覆盖父类）。 子类与父类的耦合度高。父类的实现的任何改变都会导致子类的实现发生变化，这不利于类的扩展与维护。（只要父类的代码实现更改了一点都会给子类带来影响） 它限制了复用的灵活性。从父类继承而来的实现是静态的，在编译时已经定义，所以在运行时不可能发生变化（继承不能像接口一样形成多态，导致只能传一种对象）。 采用组合或聚合复用时，可以将已有对象纳入新对象中，使之成为新对象的一部分，新对象可以调用已有对象的功能（即结构体中包含结构体或接口），它有以下优点。\n它维持了类的封装性。因为成分对象的内部细节是新对象看不见的，所以这种复用又称为“黑箱”复用。（只能进行调用，不能修改或覆盖成员结构体或接口的实现细节） 新旧类之间的耦合度低。这种复用所需的依赖较少，新对象存取成分对象的唯一方法是通过成分对象的接口。 复用的灵活性高。这种复用可以在运行时动态进行，新对象可以动态地引用与成分对象类型相同的对象。（只有在运行时才会去调用对应结构体或接口的方法，且如果是接口可以实现多态，即传不同的结构体有不同的实现细节和不同的方法） 创建者模式（5种） 用于描述“怎样创建对象”，它的主要特点是“将对象结构体的创建与使用分离”。\n创建者模式的主要关注点是怎样创建对象，它的主要特点是将对象的创建与使用分离。\n这样可以降低系统的耦合度，使用者不需要关注对象的创建细节。\n单例模式、工厂方法模式、抽象工厂模式、原型模式、建造者模式\n单例设计模式（系统中仅有一个该类对象，其他地方通过调用这个对象暴露的方法获取它的指针进行使用，它可以在系统启动时生成，也可以在第一次调用时生成，即饿汉式和懒汉式） 这种类型的设计模式属于创建者模式，它提供了一种创建对象的最佳方式。\n这种模式涉及到一个单一的类(仅一个结构体)，并且确保只有单个对象被创建。这个类提供了一种访问其唯一对象的方式(get方法，直接获取指针)，外部不需要实例化该类的对象。\n它只有一个实例，自我实例化并提供全局访问点\n1 2 3 单例设计模式分为两种： 饿汉式：类加载就会导致该单实例对象被创建 懒汉式：类加载不会导致该单实例对象被创建，而是首次使用该对象时才会创建 饿汉式： 1 2 3 4 5 6 7 8 9 type singleton struct{} // 定义一个私有的变量 var ins *singleton = \u0026amp;singleton{} // 通过函数返回它的指针 func GetSingleton() *singleton { return ins } 这种方法的缺点是如果singleton创建初始化比较复杂耗时时，加载时间会延长。如果声明后并没有调用过Get方法，那么这个资源就会造成内存浪费。\n懒汉式： 1 2 3 4 5 6 7 8 9 10 // 定义一个私有的变量 var ins *singleton // 通过函数返回它的指针 func GetSingleton() *singleton { if ins == nil { ins = \u0026amp;singleton{} } return ins } 但是这种模式最大的缺点是非线程安全的，当正在创建时，有线程来访问此时ins = nil就会再创建，单例类就会有多个实例了，可以通过加锁(不能通过读写锁，可能会导致部分获取到的是nil，要更严格的锁才行)或者通过sync.Once来实现线程安全\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // 1.在外部加锁虽然解决并发的问题，但会导致代码从并发变为串行 mu.Lock() defer mu.Unlock() // 2.双重锁，如果不为空就不加锁，避免每次加锁，提高代码效率 // 如果有多个线程同时通过第一层判断，他们会竞争锁，最终依然只有一个加锁成功， // 然后进入第二层判断进行赋值，当时间片轮转到其他当初竞争锁的线程时， // 虽然它会进行加锁，但是它第二层判断会发现ins已经不为空了，直接跳出判断 if ins == nil { mu.Lock() defer mu.Unlock() if ins == nil { ins = \u0026amp;singleton{} return ins } } // 3.sync.Once实现 once.Do(func() { ins = \u0026amp;singleton{} }) 工厂方法设计模式（设定一个工厂，每调用一个它的方法，则会给我们返回一个相应的对象，每个人调用此方法返回的对象都是相同属性的不同对象，避免每次用new生成时，如果原结构体改变，则系统中每个地方的生成都需要修改，实现解耦） 在java中万物皆对象，golang也类似。要使用时都需要进行创建，但如果创建的时候直接new该对象，就会对该对象耦合严重，假如我们要更换对象(如对象改名，新增一个对象作为该对象的另一种选择)，所有new对象的地方都需要修改一遍，这显然违背了软件设计的开闭原则。但如果使用工厂来生产对象，那么我们就只需要和工厂这一个对象进行打交道，彻底和对象解耦，如果要修改对象，直接在工厂里更换该对象即可，达到了与对象解耦的目的。即工厂模式最大的优点就是解耦\n工厂可以把创建和使用分开，分别存放，修改工厂代码时不必修改和重新编译使用的代码\n简单工厂模式 简单工厂不是一种设计模式，反而比较像一种编程习惯\n它包含如下角色：\n抽象产品：定义了产品规范，描述了产品的主要特性和功能。 具体产品：实现或者继承抽象产品的子类。 具体工厂：提供了创建产品的方法，调用者通过该方法来获取产品。 如咖啡、咖啡工厂、咖啡店、拿铁咖啡和美式咖啡的关系，工厂来处理创建对象的细节（是拿铁还是美式），而咖啡店作为工厂的客户，后期需要咖啡对象直接从工厂中获得即可。不需要每次点单都要在自己的代码中new一个新的，因为客户可能有很多个，如果新增了一款咖啡，那么每个客户端都要增加new新咖啡的代码，而使用简单工厂模式的话则只需要修改工厂模式的代码即可。\n简单工厂模式虽然解除了咖啡店和具体咖啡的耦合，但是又产生了咖啡店和咖啡工厂的耦合，咖啡工厂和具体咖啡的耦合，依然违反了开闭原则，但是这种只修改一处代码的情况总比修改客户端多处代码的情况要好解决。\n总结：简单工厂模式封装了创建对象的过程，可以通过参数直接获取对象。把对象的创建和业务逻辑层分开，这样以后就避免了修改客户端代码，如果要实现新的产品直接修改工厂类而不需要在原代码中修改，这样就降低了在客户代码修改的可能性，更容易扩展，但是增加新产品时还是需要修改工厂类代码，违背的开闭原则。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 // 工厂, 用一个函数来作为工厂给外部提供服务传递对象；也可以定义一个工厂空结构体，它的方法是创建对应具体实例 func CooferFactory(types string) (coffer pkg.Coffer) { switch types { case \u0026#34;late\u0026#34;: coffer = pkg.LateCoffer{ Name: \u0026#34;late\u0026#34;, } case \u0026#34;amera\u0026#34;: coffer = pkg.AmercaCoffer{ Name: \u0026#34;amera\u0026#34;, } default: panic(\u0026#34;input error\u0026#34;) } return coffer } // 咖啡店，根据关键字不同定义不同的咖啡 func CofferStore() { coffers := CooferFactory(\u0026#34;late\u0026#34;) fmt.Println(coffers.GetName()) } // 咖啡接口，有getName接口，用接口作为父类，抽取两种咖啡都有的方法 type Coffer interface { GetName() string } // 拿铁咖啡和美式咖啡，咖啡的子类 type LateCoffer struct { Name string } func (l LateCoffer) GetName() string { return l.Name } type AmercaCoffer struct { Name string } func (a AmercaCoffer) GetName() string { return a.Name } 当在代码里看到switch的时候，就可以思考是否能用简单工厂模式。它可以避免修改多处地方代码。\n工厂方法模式 针对简单工厂设计模式的缺点，使用工厂方法模式就可以完美的解决问题，完全遵循开闭原则。\n它的核心思想就是会定义一个创建对象的接口，让子类决定实例化哪个产品类对象。工厂方法使一个产品类的实例化延迟到其工厂的子类\n它包含如下角色：\n抽象工厂：提供了创建产品的接口，调用者通过它访问具体工厂的工厂方法来创建产品。 具体工厂：主要是实现抽象工厂接口中的抽象方法，完成具体产品的创建 抽象产品：定义了产品规范，描述了产品的主要特性和功能。 具体产品：实现了抽象产品角色所定义的接口，由具体工厂来创建，它同具体工厂之间一一对应 这种设计方式会出现多个具体工厂来一一对应具体产品，这样的话如果新增了产品，只需要生产一个具体的工厂和具体产品即可，不会修改原先的代码。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 // 抽象工厂 type Factory interface { CreateCoffer() Coffer } // 实体工厂 type AmercaFactory struct { } func (a AmercaFactory) CreateCoffer() Coffer { return AmercaCoffer{ Name: \u0026#34;amerca\u0026#34;, } } type LateFactory struct { } func (l LateFactory) CreateCoffer() Coffer { return LateCoffer{ Name: \u0026#34;late\u0026#34;, } } // 咖啡店，根据关键字不同定义不同的咖啡 func CofferStore() { var factory pkg.Factory = pkg.LateFactory{} fmt.Println(factory.CreateCoffer().GetName()) } 它就比简单工厂模式多了一个抽象工厂接口，咖啡店不需要用关键字来判断是否是哪种咖啡，只需要给抽象工厂变量赋值上具体的那个工厂，它就会生产对应的咖啡，新增咖啡种类也只需要继承抽象工厂，重新定义一个具体工厂和具体产品即可，不需要修改工厂类的代码了。\n优缺点：\n用户只需要知道具体工厂的名称就可以得到所要的产品，无序知道产品的具体创建过程。 在系统增加新的产品时只需要增加具体产品类和对应的具体工厂类，无须对原工厂进行任何修改，满足开闭原则。 但是它的缺点也同样明显，每增加一个产品就必须要增加一个产品类和工厂类，这增加了系统的复杂度。 抽象工厂模式（通过选择不同的工厂来生成同一产品族的产品，它能保证客户端始终只使用同一个产品族中的对象） 它是一种为访问类提供一个创建一组相关或相互依赖对象的接口（同一产品族），且访问类无须指定所要产品的具体类就能得到同族的不同等级的产品的模式结构。\n抽象工厂模式是工厂方法模式的升级版本，工厂方法模式只生产一个等级的产品，而抽象工厂模式可生产多个等级的产品。\n它包含如下角色：\n抽象工厂：提供了创建产品的接口，它包含多个创建产品的方法，可以创建多个不同等级的产品。 具体工厂：主要是实现抽象工厂接口中的多个抽象方法，完成具体产品的创建 抽象产品：定义了产品规范，描述了产品的主要特性和功能，抽象工厂模式有多个抽象产品。 具体产品：实现了抽象产品角色所定义的接口，由具体工厂来创建，它同具体工厂之间是多对一的关系，一个工厂可以生产不同的产品 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 interface总工厂: // DessertFactory 甜品总工厂，生产咖啡和甜品 type DessertFactory interface { CreateCoffer() Coffer CreateDessert() Dessert } // 咖啡接口，不同产品族但性质相同的东西 type Coffer interface { GetName() string } // 甜品接口 type Dessert interface { Show() string } 具体产品族的factory实体工厂: type AmercaFactory struct { } func (a AmercaFactory) CreateCoffer() Coffer { return AmercaCoffer{ Name: \u0026#34;AmercaCoffer\u0026#34;, } } func (a AmercaFactory) CreateDessert() Dessert { return MatchaMousse{ Name: \u0026#34;MatchaMousse\u0026#34;, } } type ItalyFactory struct { } func (a ItalyFactory) CreateCoffer() Coffer { return LateCoffer{ Name: \u0026#34;LateCoffer\u0026#34;, } } func (a ItalyFactory) CreateDessert() Dessert { return Tiramisu{ Name: \u0026#34;Tiramisu\u0026#34;, } } 具体产品： // 拿铁咖啡和美式咖啡 type LateCoffer struct { Name string } func (l LateCoffer) GetName() string { fmt.Println(\u0026#34;我是LateCoffer\u0026#34;) return l.Name } type AmercaCoffer struct { Name string } func (a AmercaCoffer) GetName() string { fmt.Println(\u0026#34;我是AmercaCoffer\u0026#34;) return a.Name } // 拿铁咖啡和美式咖啡 type Tiramisu struct { Name string } func (t Tiramisu) Show() string { fmt.Println(\u0026#34;提拉米苏\u0026#34;) return t.Name } type MatchaMousse struct { Name string } func (a MatchaMousse) Show() string { fmt.Println(\u0026#34;抹茶慕斯\u0026#34;) return a.Name } main: var factory pkg.DessertFactory func main() { // 通过定义不同的产品族工厂来生产不同类型的产品 factory = pkg.ItalyFactory{} coffer := factory.CreateCoffer() fmt.Println(coffer.GetName()) d := factory.CreateDessert() fmt.Println(d.Show()) } 如果要再加一个产品族的话，只需要加一个对应的实体工厂和具体产品即可，不需要修改其他的类。满足开闭原则\n优缺点：\n当一个产品族中的多个对象被设计成一起工作时，它能保证客户端始终只使用同一个产品族中的对象。（类似于分组，客户端只能使用这一个分组中的对象，另一个分组的无法使用） 但是当一个产品族需要新增一个产品时，所有的工厂类都需要进行修改。因为新增一个产品就意味着总接口需要定义生产这个产品的方法，而实体工厂结构体必须要实现总工厂接口的所有方法才算继承，那么所有实体工厂类都需要进行更改。 使用场景：\n当需要创建的对象是一系列相互关联或相互依赖的产品族时。 系统中有多个产品族，但每次只使用其中的某一族产品（分组）。 系统中提供了产品的类库，且所有产品的接口相同，客户端不依赖产品实例的创建细节和内部结构。 如输入法换皮肤是一整套一起换（logo、背景、输入框等全部一起换）\n原型模式（本质就是复制一个已经存在的对象来生成新对象，当某个对象生成较为复杂，比如有很多参数，并且该对象的不同实例差别也就是一两个参数不同，就可以使用该模式简化生成流程） 用一个已经创建的实例作为原型，通过复制该原型对象来创建一个和原型对象相同的新对象。\n原型模式包含如下角色：\n抽象原型类：规定了具体原型对象必须实现的clone()方法。 具体原型类：实现抽象原型类的clone()方法，它是可被复制的对象。 访问类：使用具体原型中的clone()方法来复制新的对象 原型模式的克隆分为浅克隆和深克隆。\n浅克隆：创建一个新对象，新对象的属性和原来对象完全相同，对于非基本类型属性，仍指向原有属性所指向的对象的内存地址（采用引用存储指针）。浅克隆是指拷贝对象时仅仅拷贝对象本身（包括对象中的基本变量），而不拷贝对象包含的引用指向的对象，只是指向原对象引用对象的指针\n深克隆：创建一个新对象，属性中引用的其他对象也会被克隆，不再指向原有对象地址。\n在golang中，由于结构体在函数之间的传递属于值传递，因此大部分情况下都是深克隆，如果要使用浅克隆则需要特殊对待。\n用原型模式生成“三好学生”奖状：\n同一学习的三好学生奖状除了获奖人姓名不同，其他都相同，可以使用原型模式复制多个奖状出来，然后修改奖状上的姓名即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 //抽象原型类 type Cloneable interface { Clone() Cloneable SetName(name string) GetName() string } // 实体原型类 type Citation struct { Name string } func (c *Citation) Clone() Cloneable { fmt.Println(\u0026#34;克隆成功\u0026#34;) return \u0026amp;Citation{ Name: c.Name, } } func (c *Citation) SetName(name string) { c.Name = name } func (c *Citation) GetName() string { return c.Name } // 访问 var role pkg.Cloneable func main() { role = \u0026amp;pkg.Citation{} cloable1 := role.Clone() cloable1.SetName(\u0026#34;张三\u0026#34;) cloable2 := role.Clone() cloable2.SetName(\u0026#34;李四\u0026#34;) fmt.Println(cloable1) fmt.Println(cloable2) } 使用场景：\n对象的创建非常复杂，有很多个属性且需要对某些属性进行加工，则可以通过原型模式快捷的创建对象，通过clone就不需要再考虑创建的细节 性能和安全要求比较高，创建细节由一个模板生成，可以避免创建的时候发生遗漏造成空指针等问题 但是每种具体实现类型都要有一个克隆自己的操作。在某些场景会比较困难。\n建造者模式(针对一个复杂对象的创建，将它的元素作为配件进行组装，不同的组装顺序或配件内容不同都会形成不同的结构体，我们只需要通过builder来选择组装哪种结构体即可，结构体的具体数据赋值都由具体建造者实现了，简化初始化赋值流程，有新的类型再实现一个具体建造者即可。工厂模式是生成同一种类型的对象，建造者模式是生成不同类型的对象，但结构体必须是同一个结构体，只是不同的属性) 将一个复杂对象的构建与表示分离，使得同样的构建过程可以创建不同的表示。\n分离了部件的构造（由Builder角色负责）和装配（由Director角色负责）。从而可以构造出复杂的对象。这个模式适用于某个对象的构建过程复杂的情况 由于实现了构建和装配的解耦。不同的构建，相同的装配也可以做出不同的对象；相同的构建，不同的装配顺序也可以做出不同的对象。也就是实现了构建算法、装配算法的解耦，实现了更好的复用。如组装台式机，不同型号的设备可以搭配出不同的主机。 建造者模式可以将部件和其组装过程分开，一步步创建一个复杂的对象。用户只需要指定复杂对象的类型就可以得到该对象，无须知道其内部的具体构造细节。 但是这个模式缺点就是所有产品的组成部分必须相同（因为只能对一个结构体进行组装），这限制了其使用范围。且如果产品内部发生变化，则建造者也要同步修改，指挥者也要新增组装方法，后期维护成本较大。 建造者模式包含如下角色：\n抽象建造者类：这个接口规定要实现复杂对象的哪些部分的创建，并不涉及具体的对象部件的创建。 具体建造者类：实现抽象建造者接口，完成复杂产品的各个部件的具体创建方法。在构造过程完成后，提供产品的实例。（这里重点不是部件的创建，它强调的是装配的过程。创建可以用其他创建者模式来获取） 产品类：要创建的复杂对象。 指挥者类：调用具体建造者来创建复杂对象的各个部分，在指导者中不涉及具体产品的信息，只负责保证对象各部分完整创建或按某种顺序创建。 指挥者用来指挥建造顺序，建造者只管造部件\n如生产共享单车\n生产自行车是一个复杂的过程，它包含了车架、车座等组件的生产。而车架又有碳钎维、铝合金等材质的，车座有橡胶、真皮等材质。对于自行车的生产就可以使用建造者模式\n产品角色（Product）：\n1 2 3 4 5 6 7 8 9 10 11 12 // 这里为了方便使用的基本类型，但是建造者模式通常成员是复杂类型 type Bike struct { frame string // 车架 seat string // 车座 } func (b *Bike) SetFrame(frame string) { b.frame = frame } func (b *Bike) SetSeat(seat string) { b.seat = seat } 抽象建造者（Abstract Builder）\n1 2 3 4 5 6 type BuilderBike interface { BuildFrame() //构建车架 BuildSeat() //构建车座 CreateBike() //创建自行车 GetResult() interface{} // 获取结果 } 具体建造者（Concrete Builder）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 type MobikeBuilder struct { Bike *Bike } func (m *MobikeBuilder) BuildFrame() { m.Bike.SetFrame(\u0026#34;碳钎维的车架\u0026#34;) } func (m *MobikeBuilder) BuildSeat() { m.Bike.SetSeat(\u0026#34;橡胶车座\u0026#34;) } func (m *MobikeBuilder) CreateBike() { fmt.Println(\u0026#34;新建一个摩拜单车\u0026#34;) m.Bike = new(Bike) } func (m *MobikeBuilder) GetResult() interface{} { return m.Bike } type OfoBuilder struct { Bike *Bike } func (o *OfoBuilder) BuildFrame() { o.Bike.SetFrame(\u0026#34;铝合金的车架\u0026#34;) } func (o *OfoBuilder) BuildSeat() { o.Bike.SetSeat(\u0026#34;真皮车座\u0026#34;) } func (o *OfoBuilder) CreateBike() { fmt.Println(\u0026#34;新建一个ofo单车\u0026#34;) o.Bike = new(Bike) } func (o *OfoBuilder) GetResult() interface{} { return o.Bike } 指挥者（Director）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 // 指挥者 type Director struct { builder BuilderBike } // 设置构建者，选择某种构建顺序 func (d *Director) SetBuilder(builder BuilderBike) { d.builder = builder } // 可以创建多个Generate来构建成员变量不同的Bike func (d *Director) Generate() *Bike { d.builder.CreateBike() d.builder.BuildSeat() d.builder.BuildFrame() // interface转为指定类型 return d.builder.GetResult().(*Bike) } main.go :\n1 2 3 4 5 6 7 8 // 创建一个指挥者 director := new(pkg.Director) // 选定制造哪种车 builder := new(pkg.OfoBuilder) director.SetBuilder(builder) // 根据某顺序进行组装 bike := director.Generate() fmt.Println(bike) 建造者模式唯一区别于工厂模式的是针对复杂对象的创建。也就是说，如果创建简单对象，通常都是使用工厂模式进行创建，而如果创建复杂对象，就可以考虑使用建造者模式。当需要创建的产品具备复杂创建过程时，可以抽取出相共性创建过程，然后交由具体实现类来自定义创建流程，使得同样的创建行为可以生产出不同的产品，分离了创建与表示，使创建产品的灵活性大大增加。\n建造者模式主要适用于以下应用场景：\n相同的方法，不同的执行顺序，产生不同的结果。 多个部件或零件，都可以装配到一个对象中，但是产生的结果又不相同。 产品类非常复杂，或者产品类中不同的调用顺序产生不同的作用。 初始化一个对象特别复杂，参数多，而且很多参数都具有默认值。 建造者（Builder）模式在应用过程中可以根据需要改变，如果创建的产品种类只有一种，只需要一个具体建造者，这时可以省略掉抽象建造者，甚至可以省略掉指挥者角色。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 type MobikeBuilder struct { Bike *Bike } func (m *MobikeBuilder) BuildFrame() *MobikeBuilder { m.Bike.SetFrame(\u0026#34;碳钎维的车架\u0026#34;) return m } func (m *MobikeBuilder) BuildSeat() *MobikeBuilder { m.Bike.SetSeat(\u0026#34;橡胶车座\u0026#34;) return m } func (m *MobikeBuilder) CreateBike() *MobikeBuilder { fmt.Println(\u0026#34;新建一个摩拜单车\u0026#34;) m.Bike = new(Bike) return m } func (m *MobikeBuilder) GetResult() interface{} { return m.Bike } type OfoBuilder struct { Bike *Bike } func (o *OfoBuilder) BuildFrame() *OfoBuilder { o.Bike.SetFrame(\u0026#34;铝合金的车架\u0026#34;) return o } func (o *OfoBuilder) BuildSeat() *OfoBuilder { o.Bike.SetSeat(\u0026#34;真皮车座\u0026#34;) return o } func (o *OfoBuilder) CreateBike() *OfoBuilder { fmt.Println(\u0026#34;新建一个ofo单车\u0026#34;) o.Bike = new(Bike) return o } func (o *OfoBuilder) GetResult() interface{} { return o.Bike } main.go: // builder := new(pkg.OfoBuilder) builder := new(pkg.MobikeBuilder) mobikeBike := builder.CreateBike().BuildFrame().BuildSeat().GetResult().(*pkg.Bike) 这种省略掉抽象建造者和指挥者的模式是将构建顺序交给了调用端来定，舍弃了指挥者，实现链式编程，提高可读性。甚至可以在build中通过方法传参实现同一个build构建不同类型不同顺序的结构体\n如果希望屏蔽对象的创建过程，只提供一个封装良好的对象，则可以选择工厂方法模式。而建造者模式可以用在复杂产品的组装方面，如通过装配不同的组件或者相同组件的不同顺序，可以产生出一个新的对象，它可以产生一个非常灵活的架构，方便地扩展和维护系统。两者都是创建一个产品，但工厂模式关心整体，建造者模式关心细节。\n结构型模式（7种） 结构型模式描述如何将类或对象按某种布局组成更大的结构。它分为类结构型模式和对象结构型模式，前者采用继承机制来组织接口和类，后者采用组合或聚合来组合对象。\n由于组合关系或聚合关系比继承关系耦合度低，满足“合成复用原则”，所以对象结构型模式比类结构型模式具有更大的灵活性。\n代理模式（在调用者和目标对象中插入一个代理对象，由它去调用目标对象的方法，保护目标对象，并对目标对象的功能进行增强） 由于某些原因需要给某对象提供一个代理（中介）以控制该对象的访问。此时访问对象不适合或者不能直接引用目标对象，代理对象作为访问对象和目标对象之间的中介。\n代理按照生成时机不同又分为静态代理和动态代理。静态代理在编译时期就生成，而动态代理则是运行时动态生成。\n代理模式分为三种角色：\n抽象主题类：通过接口或抽象类声明真实主题和代理对象要实现的业务方法（规范）。 真实主题类：实现了抽象主题中的具体业务，是代理对象所代表的真实对象，是最终要引用的对象。 代理类：提供了与真实主题相同的接口，其内部含有对真实主题的引用，它可以访问、控制或扩展真实主题的功能（一个代理类可以同时代理多个目标对象，如一个地方商可以代理多家厂商） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 抽象主题类： type Computers interface { // 售卖电脑 SellComputers() } 真实主题类： // 联想电脑 type Lenovo struct{} func (l Lenovo) SellComputers() { fmt.Println(\u0026#34;出售了一台联想电脑\u0026#34;) } // 戴尔电脑 type Dell struct{} func (d Dell) SellComputers() { fmt.Println(\u0026#34;出售了一台戴尔电脑\u0026#34;) } 代理类： // 代理商 type ComputerProxy struct { Computers } // 可以对真实主题的参数、方法体、返回值等进行增强 func (c ComputerProxy) SellComputers() { fmt.Println(\u0026#34;收取了手续费\u0026#34;) c.Computers.SellComputers() } main.go: // 静态代理 var ( Lenovo = new(pkg.Lenovo) Dell = new(pkg.Dell) ) func main() { ComputerProxy := new(pkg.ComputerProxy) ComputerProxy.Computers = Dell ComputerProxy.SellComputers() } 一般来说，Go 这种纯静态的编译型语言，想实现像 Spring 那样的动态代理基本上是不可能实现的。动态代理就是在最开始不存在代理者，是在运行时动态生成的，并且一个代理可以代理一个接口的所有方法，摆脱了繁杂的重复工作。Go如果想使用动态代理就需要使用第三方包，并且大多数情况下得不偿失，而且绝大多数情况，我觉得业务代码是不需要动态代理能力的。\n动态代理和静态代理相比较最大的好处就是接口声明的所有方法都被转移到代理类一个集中的方法中处理（InvocationHandler.invoke）。这样，在接口方法数量比较多的时候，我们可以灵活处理，而不需要像静态代理那样对每一个方法进行中转，接口中新增一个方法，代理类也得实现对应的方法。\n如果接口增加一个方法，静态处理模式除了所有实现类需要实现这个方法外，所有代理类也需要实现此方法。增加了代码维护的复杂度，而动态代理不会出现该问题。动态代理将其全部集中处理，如果增强方式不一样就根据方法名字判断增强。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 Java代码： // 动态代理必须继承InvocationHandler public class DynProxyLawyer implements InvocationHandler { // 可以动态指定不同的接口，减少代码量 private Object target;//被代理的对象 public DynProxyLawyer(Object obj){ this.target=obj; } @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { System.out.println(\u0026#34;案件进展：\u0026#34;+method.getName()); Object result=method.invoke(target,args); return result; } } public class ProxyFactory { public static Object getDynProxy(Object target) { InvocationHandler handler = new DynProxyLawyer(target); return Proxy.newProxyInstance(target.getClass().getClassLoader(), target.getClass().getInterfaces(), handler); } } https://www.bilibili.com/video/BV1Np4y1z7BU?p=58 优缺点：\n代理模式在客户端与目标对象之间起到一个中介作用和保护目标对象的作用（客户端不直接访问目标对象）； 代理对象可以扩展目标对象的功能； 代理模式能将客户端与目标对象分离，在一定程度上降低了系统的耦合度； 缺点就是增加了系统的复杂度； 适配器模式（调用者无法调用原有的接口，则新增一个接口，让它去调用原有接口，调用者调用这个新接口，如调用第三方组件，结果自己定义的接口和第三方组件的接口对不上，则可以采用此模式，但是遇到这种情况更建议重构代码） 将一个类的接口转换成客户希望的另一个接口，使得原本由于接口不兼容而不能一起工作的那些类能一起工作。\n适配器模式分为类适配器模式和对象适配器模式，前者类之间的耦合度比后者高（因为是通过继承实现），且要求程序员了解现有组件库中的相关部件的内部结构，所以应用相对较少些。\n适配器模式包含以下主要角色：\n目标接口：当前系统业务所期待的接口，它可以是抽象类或接口。 适配者类：它是被访问和适配的现存组件库中的组件接口（项目中的现有接口） 适配器者类：它是一个转换器，通过继承或引用适配者的对象，把适配者接口转换成目标接口，让客户按目标接口的格式访问适配者。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 // TF卡，适配者接口ITarget type TFCardI interface { ReadTF() WriteTF() } type TFCard struct { } func (t TFCard) ReadTF() { fmt.Println(\u0026#34;read TF\u0026#34;) } // SD卡 ,目标接口Adaptee type SDCardI interface { ReadSD() WriteSD() } type SDCard struct { } func (s SDCard) ReadSD() { fmt.Println(\u0026#34;read SD\u0026#34;) } // 适配器接口Adapter type SDAdapterTF struct { TFCard TFCard } // 适配器在方法内调用另一个接口或结构体的方法 func (sd SDAdapterTF) ReadSD() { sd.TFCard.ReadTF() } main.go: sdAdapterTF := pkg.SDAdapterTF{ TFCard: pkg.TFCard{}, } // 适配器和目标结构体实现了同一个接口 // 表面是调用的readSD,但是本质上调用的是readTF sdAdapterTF.ReadSD() 应用场景：\n以前开发的系统存在满足新系统功能需求的类，但其接口同新系统的接口不一致 使用第三方提供的组件，但组件接口定义的和自己要求的接口定义不同 即代理模式是扩展接口，适配器模式是改变接口且符合开闭原则 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 package adaptor import \u0026#34;fmt\u0026#34; // 我们的接口（新接口）——音乐播放 type MusicPlayer interface { play(fileType string, fileName string) } // 在网上找的已实现好的库 音乐播放 // ( 旧接口） type ExistPlayer struct { } func (*ExistPlayer) playMp3(fileName string) { fmt.Println(\u0026#34;play mp3 :\u0026#34;, fileName) } func (*ExistPlayer) playWma(fileName string) { fmt.Println(\u0026#34;play wma :\u0026#34;, fileName) } // 适配器 type PlayerAdaptor struct { // 持有一个旧接口 existPlayer ExistPlayer } // 实现新接口 func (player *PlayerAdaptor) play(fileType string, fileName string) { switch fileType { case \u0026#34;mp3\u0026#34;: player.existPlayer.playMp3(fileName) case \u0026#34;wma\u0026#34;: player.existPlayer.playWma(fileName) default: fmt.Println(\u0026#34;暂时不支持此类型文件播放\u0026#34;) } } 优点\n可以让两个没有关联的类一起运行。 提高了类的复用 灵活性好，符合开闭原则 缺点\n过多地使用适配器，会让系统非常零乱，不易整体进行把握。比如，明明看到调用的是 A 接口，其实内部被适配成了 B 接口的实现，一个系统如果太多出现这种情况，无异于一场灾难，增加了代码阅读难度。因此如果不是很有必要，可以不使用适配器，而是直接对系统进行重构。 装饰者模式（可以动态给某个对象加职责，不加也不会影响程序的运行。如一碗面我想加鸡蛋加培根等则可以用这种方法，它将原始对象作为一个参数或变量传给装饰者类，装饰者类和原始对象都实现的同一个接口） 在不改变现有对象结构的情况下，动态的给该对象增加一些职责（即增加其额外功能）的模式。\n装饰者（Decorator）模式中的角色：\n抽象构件角色：定义一个抽象接口以规范准备接受附加责任的对象。 具体构件角色：实现抽象构建，通过装饰角色为其添加一些职责。 #抽象装饰角色：继承或实现抽象构建，并包含具体构建的实例，可以通过其子类扩展具体构件的功能（只存在于JAVA，GOLANG没有这个角色，因为装饰和构件要实现同一个接口才能一直嵌套装饰下去，JAVA可以通过继承，但golang没有继承，分开就只能装饰一次，不能嵌套） 具体装饰角色：实现抽象装饰的相关方法，并给具体构件对象添加附加的责任。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 // 规范面条的方法，抽象构件角色 type Noddles interface { GetDescription() string // 描述 GetPrice() float32 //价格 } // 拉面，具体构件角色 type Ramen struct { Name string Price float32 } func (r Ramen) GetDescription() string { return r.Name } func (r Ramen) GetPrice() float32 { return r.Price } // 炒饭 type FriedRice struct { Name string Price float32 } func (r FriedRice) GetDescription() string { return r.Name } func (r FriedRice) GetPrice() float32 { return r.Price } // 加蛋,具体装饰角色。如果分开实现接口，Noddles在第二次就无法获取到 type Egg struct { Noddles Noddles Name string Price float32 } func (e *Egg) SetNoddles(n Noddles) { e.Noddles = n } func (e *Egg) GetDescription() string { return e.Name + \u0026#34;-\u0026#34; + e.Noddles.GetDescription() } func (e *Egg) GetPrice() float32 { return e.Price + e.Noddles.GetPrice() } // 加培根,具体装饰角色 type Bacon struct { Noddles Noddles Name string Price float32 } func (e *Bacon) SetNoddles(n Noddles) { e.Noddles = n } func (e *Bacon) GetDescription() string { return e.Name + \u0026#34;-\u0026#34; + e.Noddles.GetDescription() } func (e *Bacon) GetPrice() float32 { return e.Price + e.Noddles.GetPrice() } main.go: // 点一份面 // ramen := pkg.FriedRice{ ramen := pkg.Ramen{ Price: 10, Name: \u0026#34;拉面\u0026#34;, } // 加蛋 egg := pkg.Egg{ Noddles: ramen, Price: 1, Name: \u0026#34;鸡蛋\u0026#34;, } // 加培根，在加了鸡蛋的基础上 bacon := pkg.Bacon{ Noddles: \u0026amp;egg, Price: 2, Name: \u0026#34;培根\u0026#34;, } fmt.Println(bacon.GetDescription()) 装饰者模式最重要的地方就是在装饰者角色处聚合了它实现的接口，即它既实现了抽象构件角色接口定义的方法又再次聚合了这个接口。又因为实现了相同的接口，可以让它可以多次嵌套。\n装饰者模式可以带来比继承更加灵活的扩展功能，使用更加方便，可以通过组合不同的装饰者对象来获取具有不同行为状态的多样化的结果。装饰者模式比继承更具良好的扩展性，完美的遵循了开闭原则，动态的附加责任（可加可去，动态的）。\n装饰类和被装饰类可以独立发展（新增装饰或被装饰类都不会影响到之前的，仅仅需要新增一个结构体且实现接口方法即可），不会相互耦合（装饰类动态的嵌套在被装饰者类上，不加也不会有任何问题，只是没有装饰罢了），装饰者模式是继承的一个替代模式，可以动态扩展一个实现类的功能\n使用场景：\n在不影响其他对象的情况下，以动态、透明的方式给单个对象添加职责。 当对象的功能要求可以动态的添加，也可以再动态撤销时（改为调用加功能之前的变量即可动态撤销，因为golang中结构体无法强转，无法通过将加功能的变量重新赋值为未加之前的状态，两者结构体不同）。 代理模式与装饰模式的区别：\n代理模式是把当前的行为或功能委托给其他对象执行，代理类负责接口限定：是否可以调用真是角色，以及是否对发送到真是角色的消息进行变形处理，他不对被主题角色的功能做任何处理，保证原汁原味的调用，代理模式使用到机制开发就是AOP。 装饰模式是在要保证接口不变的情况下加强类的功能，他保证的是被修饰的对象功能比原始对象丰富。但不做准入条件和准入参数过滤 装饰者模式主要是增强目标对象，而代理是为了保护和隐藏目标对象（由代理来调用目标对象的方法） 装饰器模式关注于在一个对象上动态的添加方法，然而代理模式关注于控制对对象的访问。换句话说，用代理模式，代理类可以对它的客户隐藏一个对象的具体信息。因此，当使用代理模式的时候，我们常常在一个代理类中创建一个对象的实例（可以直接在内部进行传递，不暴露出去）。但当我们使用装饰器模式的时候，我们通常的做法是将原始对象作为一个参数或变量传给装饰者类。 桥接模式（有多个维度的对象，每个维度的组合都会有不同的结果。如window和linux播放多种格式的视频的实现代码肯定是不同的，就可以采用这种模式，可以提高扩展性，新增一个视频格式或系统只需要增加一个具体的对应角色即可） 将抽象与现实分离，使它们可以独立变化。它是用组合关系代替继承关系来实现，从而降低了抽象和实现这两个可变维度的耦合度。\n桥接模式包含以下主要角色：\n抽象化角色：定义抽象类，并包含一个对实现化对象的引用。 扩展抽象化角色：是抽象化角色的子类，实现父类中的业务方法，并通过组合关系调用实现化角色中的业务方法。 实现化角色：定义实现化角色的接口，供扩展抽象化角色调用。 具体实现化角色：给出实现化角色接口的具体实现。 在java中抽象化角色用abstract，实现化用interface，因此会说是将抽象与现实分离，但golang中没有抽象，所以本质上就是将两个关联的维度(也可以套更多维度，但是维度之间必须要有线性关系)进行解耦合，让他们两者可以进行独立变化。然后广度高的那个作为抽象化角色，其子类包含实现化角色的接口，通过传不同具体实现化角色来调用它的方法。不需要修改原系统，符合开闭原则，并且实现细节对客户透明。\n开发一个可以在不同操作系统平台上播放多种格式的视频文件，不同操作系统是一个维度，不同播放格式是一个维度。一个操作系统可以播放不同格式的视频文件，操作系统的广度更大，因此操作系统作为抽象化角色，其子类包含对应的播放格式接口\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 // 实现化角色,播放格式接口 type VedioFile interface { Decode(string) // 解码 } // 具体实现化角色, 实现不同播放器的解码 type WMVFile struct{} func (w WMVFile) Decode(dir string) { fmt.Println(\u0026#34;使用WMV格式播放了：\u0026#34; + dir) } type RMVBFile struct{} func (r RMVBFile) Decode(dir string) { fmt.Println(\u0026#34;使用RMVB格式播放了：\u0026#34; + dir) } // 抽象化角色, 操作系统接口 type OperatingSystem interface { Play(string) // 播放 } type Windows struct { VedioFile } func (w Windows) Play(dir string) { fmt.Println(\u0026#34;Windows 系统\u0026#34;) w.VedioFile.Decode(dir) } type Linux struct { VedioFile } func (l Linux) Play(dir string) { fmt.Println(\u0026#34;Linux 系统\u0026#34;) l.VedioFile.Decode(dir) } main.go: // 使用windows播放RMVB var windows pkg.OperatingSystem = pkg.Windows{ VedioFile: pkg.RMVBFile{}, } windows.Play(\u0026#34;战狼\u0026#34;) 它和装饰者看似很像，但是有很大的区别，装饰者模式只需要定义一个接口，但这个必须定义两个。并且装饰者模式扩展是可以不加的，但是这里如果不加实现化角色就会导致空指针。\n外观模式（让底层提供一个统一的对外接口，客户调用此接口来访问底层，其实就是定义了一个方法让上层访问，底层所需要的部分数据和调用顺序已经在这个方法里面写好了，我们在日常工作中也经常使用） 又名门面模式，是一种通过为多个复杂的子系统提供一个一致的接口，而使这些子系统更加容易被访问的模式。该模式对外有一个统一接口，外部应用程序不用关系内部子系统的具体的细节，这样会大大降低应用程序的复杂度，提高了程序的可维护性。\n在日常编码工作中，我们都在有意无意的大量使用外观模式。只要是高层模块需要调度多个子系统（2个以上的类对象），我们都会自觉地创建一个新的类封装这些子系统，提供精简的接口，让高层模块可以更加容易地间接调用这些子系统的功能。尤其是现阶段各种第三方SDK、开源类库，很大概率都会使用外观模式。\n外观模式包含以下主要角色：\n外观角色：为多个子系统对外提供一个共同的接口。 子系统角色：实现系统的部分功能，客户可以通过外观角色访问它。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 type ProductSystem struct { } func (p *ProductSystem) GetProductInfo() { fmt.Println(\u0026#34;获取到商品信息\u0026#34;) } type StockSystem struct { } func (s *StockSystem) GetStockInfo() { fmt.Println(\u0026#34;获取到库存信息\u0026#34;) } type PromotionSystem struct { } func (p *PromotionSystem) GetPromotionInfo() { fmt.Println(\u0026#34;获取营销信息\u0026#34;) } // 获取所有信息 func ProductDetail(product ProductSystem, stock StockSystem, promotion PromotionSystem) { product.GetProductInfo() stock.GetStockInfo() promotion.GetPromotionInfo() fmt.Println(\u0026#34;整理完成商品详情页所有数据\u0026#34;) } func main() { product := pkg.ProductSystem{} stock := pkg.StockSystem{} promotion := pkg.PromotionSystem{} pkg.ProductDetail(product, stock, promotion) } 优缺点：\n降低了子系统与客户端之间的耦合度，子系统的变化不会影响客户端。 对客户屏蔽了子系统的组件，减少了客户处理的对象数目，使得子系统使用起来更加容易。 不符合开闭原则，修改麻烦。 但是这样扩展很蠢，很少会有需要多个具体外观的情况，且这样增加了依然需要修改具体外观，还是不符合开闭原则\n使用场景：\n对分层结构系统构件时（高层调用底层），使用外观模式定义子系统中每层的入口点可以简化子系统之家的依赖关系。 当一个复杂系统的子系统很多时，外观模式可以为系统设计一个简单的接口供外界访问。 当客户端与多个子系统之间存在很大的联系时，引入外观模式可以将他们分离，从而提高子系统的独立性和可移植性。 组合模式（对于树型结构的解决方案，它让叶子节点和分支节点都实现同一个接口，通过调用根节点来递归的调用下面所有节点的方法，客户不需要关注细节，只需要调用一个方法就可以遍历所有的节点） 又名部分整体模式，是用于把一组相似的对象当作一个单一的对象。组合模式依据树型结构来组合对象，用来表示部分以及整体层次。这种类型属于结构型模式，它创建了对象组的树型结构。\n它有三个角色：\n抽象根节点：定义系统各层次对象的共有方法和属性，可以预先定义一些默认行为和属性。 树枝节点：定义树枝节点的行为，存储子节点，组合树枝节点和叶子节点形成一个树形结构。 叶子节点：叶子节点对象，其下再无分支，是系统层次遍历的最小单位 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 // 抽象根节点,定义规范，在golang中根节点也属于部分，用struce来创建 type MenuComponent interface { Add(MenuComponent) GetName() string Print() } // 树枝节点 type Menu struct { Name string Level int MenuComponents []MenuComponent // 树枝节点下面还可能有叶子和树枝节点 } // 树枝节点 func (m *Menu) Add(me MenuComponent) { m.MenuComponents = append(m.MenuComponents, me) } func (m *Menu) GetName() string { return m.Name } func (m *Menu) Print() { // ---用于表现层次感 for i := 0; i \u0026lt; m.Level; i++ { fmt.Print(\u0026#34;---\u0026#34;) } fmt.Println(m.Name) for _, v := range m.MenuComponents { // 递归 v.Print() } } // 叶子节点 type MenuItem struct { Name string Level int } // 树枝节点 func (m *MenuItem) Add(me MenuComponent) { } func (m *MenuItem) GetName() string { return m.Name } func (m *MenuItem) Print() { for i := 0; i \u0026lt; m.Level; i++ { fmt.Print(\u0026#34;---\u0026#34;) } fmt.Println(m.Name) } main.go: // 创建根节点 menu := pkg.Menu{ Name: \u0026#34;系统管理\u0026#34;, Level: 1, } // 创建二级节点 menu1 := pkg.Menu{ Name: \u0026#34;菜单管理\u0026#34;, Level: 2, } { menuItem1 := pkg.MenuItem{ Name: \u0026#34;页面访问\u0026#34;, Level: 3, } menuItem2 := pkg.MenuItem{ Name: \u0026#34;展开菜单\u0026#34;, Level: 3, } menu1.Add(\u0026amp;menuItem1) menu1.Add(\u0026amp;menuItem2) } // 创建二级节点 menu2 := pkg.Menu{ Name: \u0026#34;权限配置\u0026#34;, Level: 2, } { menuItem1 := pkg.MenuItem{ Name: \u0026#34;页面访问\u0026#34;, Level: 3, } menuItem2 := pkg.MenuItem{ Name: \u0026#34;提交保存\u0026#34;, Level: 3, } menu2.Add(\u0026amp;menuItem1) menu2.Add(\u0026amp;menuItem2) } menu3 := pkg.Menu{ Name: \u0026#34;角色管理\u0026#34;, Level: 2, } { menuItem1 := pkg.MenuItem{ Name: \u0026#34;页面访问\u0026#34;, Level: 3, } menuItem2 := pkg.MenuItem{ Name: \u0026#34;新增角色\u0026#34;, Level: 3, } menu3.Add(\u0026amp;menuItem1) menu3.Add(\u0026amp;menuItem2) } menu.Add(\u0026amp;menu1) menu.Add(\u0026amp;menu2) menu.Add(\u0026amp;menu3) menu.Print() 优点：\n组合模式可以清楚的定义分层次的复杂对象，表示对象的全部或部分层次，它让客户端忽略了层次的差异，方便对整个层次结构进行控制。 客户端可以一致地使用一个组合结构或其中的单个对象（因为两者都实现了同一个接口，调用都一样），不必关心处理的是单个对象还是整个组合结构，简化了客户端代码。 在组合模式中增加新的树枝节点和叶子节点都很方便，无须对现有类库进行任何修改，符合开闭原则。（但是对叶子节点加子节点就需要将叶子节点改为分支节点） 组合模式为 树形结构的面向对象实现 提供了一种灵活的解决方案，通过叶子节点和分支节点的递归组合，可以形成复杂的树形结构，但对树形结构的控制却非常简单（实现同一个接口，调用接口方法即可控制）。 组合模式正是应树形结构而生，只要出现树形结构的地方就可以使用组合模式来实现。比如文件目录显示、菜单栏等树形结构数据的操作。\n享元模式(享元模式和单例模式类似，但享元模式即可以再次创建对象，也可以取缓存对象，单例就只有一个，它的使用场景是一个结构体有大量相同相似的对象，其中有部分元素不同，其他都相同，则可以通过此模式减少内存损耗，通过外部状态来改变某一个对象的属性) 运用共享技术来有效的支持大量细粒度对象的复用。它通过共享已经存在的对象来大幅度减少需要创建的对象数量，避免大量相似对象的开销，从而提高系统资源的利用率。\n享元模式主要用于减少创建对象的数量，以减少内存占用和提高性能。在有大量对象时，有可能会造成内存溢出，我们把其中共同的部分抽离出来，如果有相同的业务请求，直接返回在内存中已有的对象，避免重新创建。\n它和单例有点相似，但是单例并没有内外部之分，并且单例通常只有Get方法，因此如果用单例来实现俄罗斯方块就需要每个形状每个颜色都写一个单例，依然浪费了过多的内存。\n这个模式最关键在于用map来存储这些对象，用单例模式和工厂模式获取工厂然后获取这些对象，而它的状态大部分可以通过外部化实现。\n享元模式中存在以下两种状态：\n内部状态：即不会随着环境的改变而改变的可共享部分（系统中仅存在单份，如示例中的形状）。 外部状态：指随着环境改变而改变的不可共享的部分。享元模式的实现要领就是区分应用中的这两种状态，并将外部状态外部化（如示例中的颜色，随着游戏进行改变颜色，主要通过方法参数进行传递，又或者连接池的用户名密码ip等在一开始new的时候就已经确定好了，不再进行更改，这部分就是内部状态，而回收时通过参数将这个连接改为可用，这就为外部状态）。 因此在设计时可以将这个结构体不发生改变的部分设为内部状态，时常改变的部分设为外部状态(即一个系统中生成了大量同一类结构体，只是内容不同)。\n享元模式主要有以下角色：\n抽象享元角色：通常是一个接口或抽象类，在抽象享元类中声明了具体的享元类公共的方法，这些方法可以向外界提供享元对象的内部数据（内部状态），同时也可以通过这些方法来设置外部数据（外部状态）。 具体享元角色：它实现了抽象享元类，称为享元对象。在具体享元类中为内部状态提供了存储空间。通常我们可以结合单例模式来设计具体享元类，为每一个具体享元类提供唯一的享元对象。（内部状态） 非享元角色：并不是所有的抽象享元类的子类都需要被共享，不能被共享的子类可设计为非共享具体享元类，当需要一个非共享具体享元类的对象时可以直接通过实例化创建。（外部状态） 享元工厂角色：负责创建和管理享元角色。当客户对象请求一个享元对象时，享元工厂检查系统中是否存在符合要求的享元对象，如果存在则提供给客户；如果不存在的话，则创建一个新的享元对象。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 var factory *BoxFactory var once sync.Once // 抽象享元角色 type Box interface { // 获取图形 GetShape() string // 显示图形和形状，颜色作为外部状态以参数的形式进行传递 DisPlay(color string) } // 具体享元角色 type IBox struct { Color string } func (i *IBox) GetShape() string { return \u0026#34;IBox\u0026#34; + i.Color } func (i *IBox) DisPlay(color string) { i.Color = color } type OBox struct { Color string } func (i *OBox) GetShape() string { return \u0026#34;OBox\u0026#34; + i.Color } func (i *OBox) DisPlay(color string) { i.Color = color } // 享元工厂角色,通过单例来设计工厂类 type BoxFactory struct { Box map[string]Box } func (bf *BoxFactory) GetBox(boxName string) Box { return bf.Box[boxName] } func GetBoxFactory() *BoxFactory { // 第一次调用时初始化存储map once.Do(func() { factory = \u0026amp;BoxFactory{ Box: map[string]Box{}, } factory.Box[\u0026#34;I\u0026#34;] = \u0026amp;IBox{} factory.Box[\u0026#34;O\u0026#34;] = \u0026amp;OBox{} }) return factory } main.go: // 创建工厂 factory := pkg.GetBoxFactory() // map中存储的是指针，因此此时修改box1会影响到box box := factory.GetBox(\u0026#34;O\u0026#34;) box1 := factory.GetBox(\u0026#34;O\u0026#34;) // 为false是因为\u0026amp;box取的是获取的指针的指针，获取的指针存到了两个不同的变量中，其指针肯定也不同 fmt.Println(\u0026amp;box == \u0026amp;box1) box1.DisPlay(\u0026#34;绿色\u0026#34;) fmt.Println(box) 优缺点：\n极大减少内存中相似或相同的数量，节约系统资源，提高系统性能。 享元模式中的外部状态相对独立，且不影响内部状态（如俄罗斯方块中更改颜色不会影响到形状）。 缺点是为了使对象可以共享，需要将享元对象的部分状态外部化，分离内部状态和外部状态，使程序逻辑复杂。 使用场景：\n一个系统中有大量相同或者相似的对象，造成内存的大量耗费（同一个结构体，成员变量不同）。 对象的大部分状态都可以外部化，可以将这些外部状态传入对象中。 在使用享元模式时需要维护一个存储享元对象的享元池，而这需要耗费一定的系统资源，因此，应当在需要多次重复使用同一类对象时才值得使用享元模式。（如果这个对象在系统中使用的次数少，采用享元模式反而会得不偿失） 享元模式和单例模式的区别：\n享元模式可以再次创建对象，也可以取缓存对象。单例模式则是严格控制单个进程中只有一个实例对象 享元模式可以通过自己实现对外部的单例，也可以在需要的使用创建更多的对象。单例模式是自身控制，需要增加不属于该对象本身的逻辑 两者都可以实现节省对象创建的时间 行为型模式（11种） 行为性模式用于描述程序在运行时复杂的流程控制，即描述多个类或对象之间怎样相互协作共同完成单个对象都无法单独完成的任务，它涉及算法与对象间职责的分配。\n行为型模式分为类行为模式和对象行为模式，前者采用继承机制来在类间分派行为，后者采用组合或聚合在对象间分配行为。由于组合关系或聚合关系比继承关系耦合度低，满足“合成复用原则”，所以对象行为模式比类行为模式具有更大的灵活性。\n模板方法模式（当一个系统是按已知的某种流程运行的，只有其中部分方法逻辑不同，则可以使用此模式，使其模板化，而不同的部分则通过子类的匿名内部类来重写其方法，简少重复代码） 在面向对象程序设计过程中，程序员常常会遇到某种情况：设计一个系统时知道了算法所需的关键步骤，而且确定了这些步骤的执行顺序，但某些步骤的具体实现还未知，或者说某些步骤的实现与具体的环境相关。\n模板方法模式就是定义一个操作中的算法骨架，而将算法的一些步骤延迟到子类中，使得子类可以在不改变该算法结构的情况下重新定义该算法的某些特定步骤。\n模板方法模式包含以下主要角色：\n抽象类：负责给出一个算法的轮廓和骨架。它由一个模板方法和若干个基本方法构成。 模板方法：定义了算法的骨架，按某种顺序调用其包含的基本方法（设置 执行顺序 的方法）。 基本方法：是 实现算法各个步骤的方法（单一步骤的详细实现方法），是模板方法的组成部分。基本方法又可以分为三种： 抽象方法：一个抽象方法由抽象类声明，并要求由其具体子类必须实现。 具体方法：一个具体方法由一个抽象类或具体类 声明并实现，其子类可以进行覆盖也可以直接继承（在父类中就已经实现了此方法）。 钩子方法：在抽象类中已经实现，包括用于判断的逻辑方法和需要子类重写空方法两种。（一般钩子方法是用于判断的逻辑方法，这类方法名一般为IsXxx，返回值类型为bool类型，在父类中实现然后子类继承重写【如父类直接返回false，在子类中再根据情况返回布尔值】） 具体子类：实现抽象类中所定义的抽象方法和钩子方法，它们是一个顶级逻辑的组成步骤。 模板模式是OOP编程中的一把神兵利器，用好了能够提高代码的复用程度，大大提高开发效率。例如，我们可以在父类中定义完成一个任务的几个步骤并分别给出一个默认实现，然后子类继承父类，子类只需要重写自己感兴趣的方法即可，剩余逻辑都可以复用父类的代码。Spring源码中就大量充斥着这样的套路。但是在go语言中，连类都没有，更别提继承了，那如何才能使出这种套路呢?答案就是内嵌匿名结构体。\n如果一个struct A中内嵌了另一个匿名的struct B, 那么A就可以【直接】访问B中所有的字段和方法。这就是Go语言间接实现继承的唯一方法，内嵌匿名结构体。如果在定义A时给B进行了命名，比如b, 那调用时就只能 a.b.bField(), a.b.bMethod()了，完全失去了继承的意义。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 // 抽象类，定义流程 type Cooker interface { pourOil() // 倒油 heatOil() // 热油 pourVegetable() // 倒蔬菜 pourSauce() // 倒调料 fry() // 翻炒 // 可以加钩子函数然后有基本方法结构体实现 } // 模板方法，不写成方法是避免被子类重写 func doCook(cook Cooker) { cook.pourOil() cook.heatOil() cook.pourVegetable() cook.pourSauce() cook.fry() } // 基本方法的结构体 type Cookie struct{} // 具体方法 func (c Cookie) pourOil() { fmt.Println(\u0026#34;倒油\u0026#34;) } func (c Cookie) heatOil() { fmt.Println(\u0026#34;热油\u0026#34;) } // 抽象方法，交由下一层重写 func (c Cookie) pourVegetable() { } func (c Cookie) pourSauce() { } func (c Cookie) fry() { fmt.Println(\u0026#34;翻炒\u0026#34;) } // 具体子类 type BaoCai struct { // 由于golang中没有继承，只能通过这种方式实现模板方法 // 要注意的是这里的Cookie不能起名，起了后就无法再重写Cookie的方法了,只有内嵌匿名结构体才能实现重写，不匿名调用时都需要加上成员变量的名称才行 Cookie } // 重写抽象方法 func (b *BaoCai) pourVegetable() { fmt.Println(\u0026#34;放包菜\u0026#34;) } func (b *BaoCai) pourSauce() { fmt.Println(\u0026#34;放辣椒\u0026#34;) } func main() { baocai := \u0026amp;BaoCai{} doCook(baocai) } 但是这样只能是勉强实现了模板模式，并不优雅，问题很多。编译器无法强制\u0026quot;子类\u0026quot;来实现\u0026quot;父类\u0026quot;定义的步骤方法, 编写\u0026quot;子类\u0026quot;有可能会忘记实现，但这一错误要到运行时才能被发现（因为只有结构体实现接口没实现完才会报错，对父类的方法重写不会涉及报错情况）。或者忘记使用doCook也会在运行时才能被发现。这两个问题目前在golang中是完全无解的。\nGo语言的设计哲学是简单和简洁，即使用最少的关键字、最少的语法来实现最常用的功能。也就造成了牺牲抽象能力，牺牲继承等。\n策略模式（达到一个目的有多种办法，如登录时可以微信可以QQ等，此时就可以使用此方法，把微信和QQ的具体细节分别存入不同的函数中，函数存入map,客户端通过map-key来调用对应的方法，可以省略过多的if-else语句，美化代码） 作为一个编程人员，开发肯定需要选择一款开发工具，而可以选择代码开发的工具很多，可以用vscode，也可以用goland，甚至是其他工具。\n该模式定义了一系列算法，并将每个算法封装起来，使它们可以相互替换（就如开发工具），算法的变化不会影响到使用算法的客户（最终结果）。策略模式属于对象行为模式，它通过对算法进行封装，把使用算法的责任和算法的实现分割开来，并委派给不同的对象对这些算法进行管理。\n我们在用GO编程的时候经常碰到多层控制语句(if-else if\u0026hellip;)，一层又一层，既不优雅，也不利于后续维护。\n虽然按这种模式写起来简单快捷，但它也违背了面向对象的两个原则：\n单一职责原则：多个控制语句，意味着拥有多种功能； 开闭原则：当要进行修改时，原有代码不可避免要被修改； 此时就可以采用策略模式来替换这类多层控制语句。又或者，我们在为一个对象添加行为时发现，该对象可以用多种方式去达成同一个目的，区别在于使用场景不同、效率不同。\n主要角色如下：\n抽象策略类：这是一个抽象角色，通常由一个接口或抽象类实现。此角色给出所有的具体策略类所需的接口。 具体策略类：实现了抽象策略定义的接口，提供具体的算法实现或行为。 环境类：持有一个策略类的引用，最终给客户端调用。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 // 抽象策略类，实现此接口，则为一个策略 type Strategy interface { show() } // 具体策略类，提供不同的策略,每个节日的具体促销活动 type StrategyA struct{} func (s *StrategyA) show() { fmt.Println(\u0026#34;买一送一\u0026#34;) } type StrategyB struct{} func (s *StrategyB) show() { fmt.Println(\u0026#34;满200减50\u0026#34;) } type StrategyC struct{} func (s *StrategyC) show() { fmt.Println(\u0026#34;满1000元加一元换购任意200元以下商品\u0026#34;) } // 环境类，具体策略的执行者，这里可以理解为销售员 type SalesMan struct { strategy Strategy } func (s *SalesMan) SalesManShow() { s.strategy.show() } func (s *SalesMan) SetSlesMan(strategy Strategy) { s.strategy = strategy } func main() { salesMan := SalesMan{} strategyFunc := make(map[string]func(v ...interface{})) strategyFunc[\u0026#34;A\u0026#34;] = func(v ...interface{}) { salesMan.SetSlesMan(\u0026amp;StrategyA{}) } strategyFunc[\u0026#34;B\u0026#34;] = func(v ...interface{}) { salesMan.SetSlesMan(\u0026amp;StrategyB{}) } strategyFunc[\u0026#34;C\u0026#34;] = func(v ...interface{}) { salesMan.SetSlesMan(\u0026amp;StrategyC{}) } strategyFunc[\u0026#34;A\u0026#34;]() salesMan.SalesManShow() } 但是策略的选择还是需要人为了解策略实现后进行选择，甚至还是不可避免地需要使用多个if else的嵌套来选择策略。因此可以预先将策略类保存在一个map中，具体的开发时候，通过一个key来获取这个策略的实例。这样的话，策略的选择就通过变成了一个key的选择，甚至可以做到通过文档规范来制约。\n再重申一遍策略模式的精髓是封装一组算法实现以供使用时的调度，golang里面有一个很重要的语法糖就是func() ——方法变量，也因为，golang实现类似策略模式的做法，不需要依赖于对象而进行。\n这样无论是算法的封装还是调度都从业务场景中解耦了。当然，缺点就是如果需要扩展策略，就要到增加一个Entry\u0026lt;K,V\u0026gt;,没有传统的实现方式中直接扩展一个实现了策略接口的对象那么方便（但是传统实现方式依然无法摆脱if-elseif的问题），这两个还得看具体的项目取舍。\n优点：\n策略类之间可以自由切换。由于策略类都实现同一个接口，所有它们之间可以自由切换。 易于扩展。增加一个新的策略只需要添加一个具体的策略类即可，基本不需要改变原有的代码，符合开闭原则 避免使用多重条件选择语句，充分体现面向对象设计思想 缺点：\n客户端必须知道所有的策略类，才能自行决定使用哪一个策略类。即所有策略类都需要对外暴露。 策略模式将造成产生很多的策略类，可以通过使用享元模式在一定程度上减少对象的数量。 使用场景：\n一个系统需要动态地在几种算法中选择一种时，可以将每个算法封装到策略类中 一个类定义了多种行为，并且这些行为在这个类的操作中以多个条件语句的形式出现，可将每个条件分支移入它们各自的策略类中以代替这些条件语句。 系统中各算法彼此完全独立，且要求对客户隐藏具体算法的实现细节时。 系统要求使用算法的客户不应该知道其操作的数据时，可使用策略模式来隐藏与算法相关的数据结构。 多个类只区别在表现行为不同，可以使用策略模式，在运行时动态选择具体要执行的行为。 命令模式（在请求方和执行方中插入命令层，请求方调用命令层，请求方不再关注是哪个执行方，由命令层分配执行方执行，命令层对请求方的数据加工或转存再调用执行方。新增命令调用其他的执行方时，请求方就可以同步使用新的命令，不需要了解执行方的具体代码，屏蔽了底层的复杂实现。可以实现队列的需求，延时调用和顺序调用的功能，还可以实现回滚功能。多个请求者去调用一个命令层，一个命令层再去调用多个执行者） 将一个请求封装为一个对象，使发出请求的责任和执行请求的责任分隔开。这样两者之间通过命令对象进行沟通，这样方便将命令对象进行存储、传递、调用、增加和管理。\n主要包含以下角色：\n抽象命令类角色：定义命令的接口，声明执行的方法。 具体命令类角色：具体的命令，实现命令接口。通常会持有接收者，并调用接收者的功能来完成命令要执行的操作。 实现者/接收者角色：真正执行命令的对象。任何类都可能成为一个接收者，只要它能够实现命令要求实现的相应功能。 调用者/请求者角色：要求命令对象执行请求，通常会持有命令对象，可以持有很多的命令对象。这个是客户端真正触发命令并要求命令执行相应操作的地方，也就是说相当于使用命令对象的入口。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 // 订单 type Order struct { table int // 桌号 foodDir map[string]int // 食物，key为食物名称，value为份数 } func (o *Order) setFood(name string, num int) { if o.foodDir == nil { o.foodDir = make(map[string]int) } o.foodDir[name] = num } // 厨师类，实现者/接收者角色 type SeniorCher struct { Name string } // 制作食物 func (s SeniorCher) makeFood(num int, foodName string) { fmt.Println(\u0026#34;厨师 \u0026#34; + s.Name + \u0026#34;制作了\u0026#34; + strconv.Itoa(num) + \u0026#34;份\u0026#34; + foodName) } // 服务员类，调用者/请求者角色 type Waitor struct { Name string Commands []Command } // 它可以持有多个命令对象,让不同类型的命令都走一个通道(只要实现了抽象命令类即可) func (w *Waitor) setCommand(cmd Command) { w.Commands = append(w.Commands, cmd) } // 发起命令 func (w *Waitor) orderUp() { fmt.Println(w.Name + \u0026#34;服务员接单\u0026#34;) for _, v := range w.Commands { if v != nil { v.execute() } } } // 抽象命令类角色 type Command interface { execute() } // 具体命令类角色 type OrderCommand struct { Order SeniorCher } func (o *OrderCommand) execute() { fmt.Println(\u0026#34;开始制作\u0026#34; + strconv.Itoa(o.Order.table) + \u0026#34;桌的订单\u0026#34;) for i, v := range o.foodDir { o.SeniorCher.makeFood(v, i) } fmt.Println(strconv.Itoa(o.Order.table) + \u0026#34;桌的订单准备完毕\u0026#34;) } func main() { // 一号桌点单 order1 := Order{ table: 1, } order1.setFood(\u0026#34;西红柿炒蛋\u0026#34;, 1) order1.setFood(\u0026#34;可乐\u0026#34;, 2) // 二号桌点单 order2 := Order{ table: 2, } order2.setFood(\u0026#34;青椒肉丝\u0026#34;, 1) order2.setFood(\u0026#34;雪碧\u0026#34;, 1) // 创建命令对象 OrderCommand1 := OrderCommand{ Order: order1, SeniorCher: SeniorCher{\u0026#34;李大厨\u0026#34;}, } OrderCommand2 := OrderCommand{ Order: order2, SeniorCher: SeniorCher{\u0026#34;王助手\u0026#34;}, } // 服务员发起订单请求 waitor := Waitor{ Name: \u0026#34;小美\u0026#34;, } waitor.setCommand(\u0026amp;OrderCommand1) waitor.setCommand(\u0026amp;OrderCommand2) // 发起命令 waitor.orderUp() } 优点：\n通过引入中间件（抽象接口），降低系统的耦合度。命令模式能将调用操作的对象与实现该操作的对象解耦（增加了一个中间件）。 支持命令队列，顺序执行。也可以在现有命令的基础上增加额外功能，如日志记录，结合装饰者模式会更加灵活。 增加或删除命令非常方便。采用命令模式增加与删除命令不会影响其他类，只需要新增一个具体命令类角色即可（即使与厨师功能不相关都可，这样子服务员除了给厨师发送做饭的命令还可以给其他类发送命令）。它满足开闭原则，对扩展比较灵活。 可以实现宏命令。命令模式和组合模式结合，将多个命令装配成一个组合命令，即为宏命令（将命令包装成一棵树，层层向下进行执行） 方便实现Undo和Redo操作（撤销与恢复）。命令模式可以与备忘录模式结合，实现命令的撤销与恢复。 缺点：\n使用命令模式可能会导致某些系统有过多的具体命令类。 系统结构更加复杂。 使用场景：\n系统需要将请求调用者和请求接收者解耦，使得调用者和接收者不直接交互。 现实语义中具备“命令”的操作（如命令菜单、Shell命令等）。 系统需要在不同的时间指定请求、将请求排队（谁先下单谁先做,因为是存在切片中,是有序的）和执行请求。 系统需要支持命令的撤销和恢复操作。 需要支持命令宏（即命令组合操作）。 命令模式（Command Pattern）是对命令的封装。每一个命令都是一个操作，请求方发出请求要求执行一个操作；接收方收到请求，并执行操作。\n责任链模式（通常作用于过滤器，过滤敏感词。一个对象经由多个对象处理，这个对象处理不了则交给下一级） 在现实生活中，常常会出现这样的事例：一个请求有多个对象可以处理，但每个对象的处理条件或权限不同。例如公司员工请假，可批假的领导有部门负责人、副总经理、总经理等，但每个领导能批准的天数不同，员工必须根据自己要请假的天数去找不同的领导签名，也就是说员工必须记住每个领导的姓名、电话和地址等信息，这无疑是增加了难度。\n责任链模式又名职责链模式，为了避免请求发送者与多个请求处理者耦合在一起，将所有请求的处理者通过前一对象记住下一个对象的引用而连成一条链；当有请求发生时，可将请求沿着这条链传递，直到有对象处理它为止。\n主要包含以下角色：\n抽象处理者角色：定义一个处理请求的接口，包含抽象处理方法和一个后继连接。 具体处理者角色：实现抽象处理者的处理方法，判断能否处理本次请求，如果可以处理请求则处理，否则将该请求转给它的后继者。 客户类角色：创建处理链，并向链头的具体处理者对象提交请求，它不关心处理细节和请求的传递过程（即客户端）。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 // 请假条 type Leave struct { name string num int } // 抽象处理者角色 type Handler interface { submit(l Leave) } // 部门领导，具体处理者角色 type GroupLeader struct { nextHandler Handler // 上级领导 } func (g *GroupLeader) submit(le Leave) { fmt.Println(\u0026#34;小组长审批同意\u0026#34;) // 小组长无法处理三天以上的则向后延,但是需要小组长先同意再继续 if g.nextHandler != nil \u0026amp;\u0026amp; le.num \u0026gt; 1 { g.nextHandler.submit(le) } else { fmt.Println(\u0026#34;流程结束\u0026#34;) } } // 副经理 type Manager struct { nextHandler Handler // 上级领导 } func (m *Manager) submit(le Leave) { fmt.Println(\u0026#34;副经理审批同意\u0026#34;) // 小组长无法处理三天以上的则向后延,但是需要小组长先同意再继续 if m.nextHandler != nil \u0026amp;\u0026amp; le.num \u0026gt; 3 { m.nextHandler.submit(le) } else { fmt.Println(\u0026#34;流程结束\u0026#34;) } } // 总经理 type GeneralManager struct { nextHandler Handler // 上级领导 } func (g *GeneralManager) submit(le Leave) { fmt.Println(\u0026#34;总经理审批同意\u0026#34;) // 小组长无法处理三天以上的则向后延,但是需要小组长先同意再继续 if g.nextHandler != nil \u0026amp;\u0026amp; le.num \u0026gt; 7 { g.nextHandler.submit(le) } else { fmt.Println(\u0026#34;流程结束\u0026#34;) } } func main() { // 创建请假条 leave := Leave{ name: \u0026#34;张三\u0026#34;, num: 3, } // 创建领导链 groupLeader := GroupLeader{} manager := Manager{} generalManager := GeneralManager{} groupLeader.nextHandler = \u0026amp;manager manager.nextHandler = \u0026amp;generalManager groupLeader.submit(leave) } 优点\n降低了请求发送者和接收者之间的耦合度 增强了系统的可扩展性。可以根据需要增加新的具体处理类，满足开闭原则。 增强了给对象指派职责的灵活性。当工作流程发生变化，可以动态的改变链内的成员或修改它们的次序，也可动态的新增或删除职责。 责任链简化了对象之间的连接，一个对象只需要保持一个指向其后继者的引用，不需要保持其他所有处理者的引用，这避免了客户端使用众多的if-elseif语句。 每个类只需要处理自己该处理的工作，不能处理的传递给下一处对象完成，明确各类的责任范围，符合类的单一职责原则。 缺点：\n不能保证每一个请求都被处理。由于请求没有明确的接收者，所以不能保证它一定会被处理，该请求可能一直传到链的末端都得不到处理（如上例中请假天数过大就没处理，实际开发中要仔细考虑） 对较长的职责链，请求的处理可能涉及多个处理对象（一直向下传递），系统性能将受到一定的影响（所以链不能太长，要适当）。 职责链建立的合理性要靠客户端来保证，增加了客户端的复杂性，可能会由于职责链的错误设置而导致系统出错，如可能会造成循环调用（最终又指向最初） 使用场景：\n消息过滤器，权限拦截器 用户发帖内容进行广告过滤，涉黄过滤，敏感词过滤等 除了采用显性链式的情况外，也可以将具体处理者存为一个切片，切片本质是一个数组（和链表类似，属于有序的集合），从头到尾，只是增加了一个基础处理者，这样子在客户端就能一目了然的看明白链的顺序。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 // 假设我们现在有个校园论坛，由于社区规章制度、广告、法律法规的原因需要对用户的发言进行敏感词过滤 // 如果被判定为敏感词，那么这篇帖子将会被封禁 // SensitiveWordFilter 敏感词过滤器，判定是否是敏感词 type SensitiveWordFilter interface { Filter(content string) bool } // SensitiveWordFilterChain 职责链 type SensitiveWordFilterChain struct { filters []SensitiveWordFilter } // AddFilter 添加一个过滤器 func (c *SensitiveWordFilterChain) AddFilter(filter SensitiveWordFilter) { c.filters = append(c.filters, filter) } // Filter 执行过滤 func (c *SensitiveWordFilterChain) Filter(content string) bool { for _, filter := range c.filters { // 如果发现敏感直接返回结果 if filter.Filter(content) { return true } } return false } // AdSensitiveWordFilter 广告 type AdSensitiveWordFilter struct{} // Filter 实现过滤算法 func (f *AdSensitiveWordFilter) Filter(content string) bool { if strings.Contains(content, \u0026#34;广告\u0026#34;) { fmt.Println(\u0026#34;广告敏感\u0026#34;) return false } return true } // PoliticalWordFilter 政治敏感 type PoliticalWordFilter struct{} // Filter 实现过滤算法 func (f *PoliticalWordFilter) Filter(content string) bool { if strings.Contains(content, \u0026#34;政治\u0026#34;) { fmt.Println(\u0026#34;政治敏感\u0026#34;) return false } return true } func main() { // 创建责任链 chain := \u0026amp;SensitiveWordFilterChain{} chain.AddFilter(\u0026amp;AdSensitiveWordFilter{}) chain.AddFilter(\u0026amp;PoliticalWordFilter{}) chain.Filter(\u0026#34;广告敏感\u0026#34;) } 状态模式（当对象根据某状态字段进行不同行为时，如QQ在线离线情况。将结构体中的状态字段包装成一个对象，减少庞大的ifelse语句块） 状态模式与有限状态机的概念紧密相关。其主要思想是程序在任意时刻仅可处于几种有限的状态中。 在任何一个特定状态中， 程序的行为都不相同， 且可瞬间从一个状态切换到另一个状态。 不过， 根据当前状态， 程序可能会切换到另外一种状态， 也可能会保持当前状态不变。 这些数量有限且预先定义的状态切换规则被称为转移 。\n状态机通常由众多条件运算符 （ if或 switch ） 实现， 可根据对象的当前状态选择相应的行为。基于条件语句的状态机会暴露其最大的弱点：为了能根据当前状态选择完成相应行为的方法， 绝大部分方法中会包含复杂的条件语句。修改其转换逻辑可能会涉及到修改所有方法中的状态条件语句，导致代码的维护工作非常艰难。\n这个问题会随着项目进行变得越发严重。 我们很难在设计阶段预测到所有可能的状态和转换。 随着时间推移， 最初仅包含有限条件语句的简洁状态机可能会变成臃肿的一团乱麻。\n而状态模式则是解决这一问题。它对有状态的对象，把复杂的判断逻辑提取到不同的状态对象中，允许状态对象在其内部状态发生变化时改变其行为。\n状态模式建议为对象的所有可能状态新建一个类， 然后将所有状态的对应行为抽取到这些类中。\n原始对象被称为上下文 （context）， 它并不会自行实现所有行为， 而是会保存一个指向表示当前状态的状态对象的引用， 且将所有与状态相关的工作委派给该对象。\n这个结构可能看上去与策略模式相似， 但有一个关键性的不同——在状态模式中， 特定状态知道其他所有状态的存在， 且能触发从一个状态到另一个状态的转换； 策略则几乎完全不知道其他策略的存在。\n主要包含以下角色：\n环境角色：也称上下文，它定义了客户程序需要的接口，维护一个当前状态，并将与状态相关的操作委托给当前状态对象来处理。 抽象状态角色：定义一个接口，用以封装环境对象中的特定状态所对应的行为。 具体状态角色：实现抽象状态所对应的行为。通过消除臃肿的状态机条件语句简化上下文代码。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 // 定义状态 var ( stop = stopState{} run = runState{} ) // 抽象状态角色 type LiftState interface { stop(con *liftContext) run(con *liftContext) } // 具体状态角色 type runState struct{} func (r runState) stop(con *liftContext) { fmt.Println(\u0026#34;关闭电梯\u0026#34;) con.liftState = stop } // 运行时无法再运行 func (r runState) run(con *liftContext) { fmt.Println(\u0026#34;已经在运行状态了\u0026#34;) } type stopState struct{} func (s stopState) stop(con *liftContext) { fmt.Println(\u0026#34;停止状态不可以再停止\u0026#34;) } // 运行时无法再运行 func (s stopState) run(con *liftContext) { fmt.Println(\u0026#34;停止状态可以运行电梯\u0026#34;) con.liftState = run } // 环境角色，也就是原对象，里面的status则是原本的状态，是一个基础数据类型，将其换成接口 type liftContext struct { Name string //电梯 liftState LiftState } func main() { liftContext := liftContext{ Name: \u0026#34;XX电梯\u0026#34;, liftState: stopState{}, //设置为停止状态 } // 调用了关闭状态的run方法，此时电梯状态变为了运行 liftContext.liftState.run(\u0026amp;liftContext) // 调用了启动状态的stop方法，此时电梯状态变为了停止 liftContext.liftState.stop(\u0026amp;liftContext) // 后续如果新增了方法，如断电方法，抽象、具体都得修改代码，也得新增一个断电功能 } 优点：\n将所有与某个状态有关的行为放到一个类中，并且可以方便的增加新的状态，只需要改变对象的状态即可改变对象的行为。 允许状态转换逻辑与状态对象合成一体，而不是一个巨大的条件语句块。 缺点：\n状态模式的使用必然会增加系统类和对象的个数。 状态模式的结构与实现都较为复杂，如果使用不当将导致程序结构和代码的混乱。 状态模式对开闭原则的支持不太友好，新增状态有可能会修改之前状态中的代码。对于可以切换状态的状态模式，增加新的状态类需要修改那些负责状态转换的源代码，否则无法切换到新增状态，而且修改某个状态类的行为也需修改对应类的源代码。 如果状态机只有很少的几个状态， 或者很少发生改变， 那么应用该模式可能会显得小题大作 使用场景：\n如果对象需要根据自身当前状态进行不同行为， 同时状态的数量非常多且与状态相关的代码会频繁变更的话， 可使用状态模式。 如果某个类需要根据成员变量的当前值改变自身行为， 从而需要使用大量的条件语句时， 可使用该模式。 观察者模式（发布-订阅模式，一个对象作为主题，其他多个对象观察它的变化，并随它的变化改变自己，比如在按钮按下时执行自己的逻辑。） 它定义了一种一对多的依赖关系，让多个观察者对象同时监听某一个主题对象。这个主题对象在状态发送变化时，会通知所有的观察者对象，使他们能够自动的更新自己。\n主要包含以下角色：\n抽象主题角色：把所有观察者对象保存在一个集合里，每个主题都可以有任意数量的观察者，抽象主题提供一个接口，可以增加和删除观察者对象。 具体主题角色：它将有关状态存入具体观察者对象中，在具体主题的内部状态发送变化时，给所有注册过的观察者发送通知。 抽象观察者角色：它定义了一个更新接口，使得在得到主题更改通知时更新自己。在绝大多数情况下， 该接口仅包含一个 update更新方法。 该方法可以拥有多个参数， 使发布者能在更新时传递事件的详细信息。 具体观察者角色：实现抽象观察者定义的更新接口，以便在得到主题更改通知时更新自身的状态。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 // 抽象主题角色 type Subject interface { attach(Observer) //添加订阅者（观察者对象） notify(message string) } // 具体主题角色 type SubscriptionSubject struct { weiXinUserArr []Observer } func (s *SubscriptionSubject) attach(observer Observer) { s.weiXinUserArr = append(s.weiXinUserArr, observer) } func (s *SubscriptionSubject) notify(message string) { for _, v := range s.weiXinUserArr { v.update(message) } } // 抽象观察者角色 type Observer interface { update(message string) } // 具体观察者角色 type WeiXinUser struct { name string } func (w WeiXinUser) update(message string) { fmt.Println(w.name + \u0026#34;-\u0026#34; + message) } func main() { // 创建主题 subject := new(SubscriptionSubject) // 客户订阅 subject.attach(\u0026amp;WeiXinUser{\u0026#34;张三\u0026#34;}) subject.attach(\u0026amp;WeiXinUser{\u0026#34;李四\u0026#34;}) // 更新内容 subject.notify(\u0026#34;发布了新内容\u0026#34;) } 优点：\n开闭原则。 你无需修改发布者代码就能引入新的订阅者类 （主题也一样）。 降低了目标与观察者之间的耦合关系，两者属于抽象耦合关系（具体主题和抽象观察者耦合） 被观察者发送通知，所有注册的观察者都会收到信息（可以实现广播机制） 缺点：\n如果观察者非常多的话，那么所有的观察者收到被观察者发送的通知会花费很多时间，可以通过开携程解决 如果被观察者有循环依赖，那么被观察者发送通知会使观察者循环调用，导致系统崩溃。尽量不要让主题和观察者双向沟通，观察者只能收主题的消息，不能给主题发消息。 订阅者的通知顺序是随机的。 观察值模式没有相应的机制让观察者知道所观察的目标对象是怎样发生变化的。而仅仅知道观察目标发生了变化。 使用场景：\n对象间存在一对多关系，一个对象的信息发生改变会影响其他对象，即当一个对象状态的改变后需要改变其他对象时。 实际对象是无法预知或者动态变化时。 需要在具体的对象的某种动作中注入代码逻辑，这种在界面类中最为常见，比如定义在按钮按下时执行逻辑。 当一个对象必须“观察”其他对象时。 中介者模式（如果结构体之间的关联过于复杂可以用这个解耦，它的最大作用就是解耦合，集中管理结构体的交流，但是如果滥用会导致中介者结构体十分庞大，维护困难，得不偿失） 中介者模式又叫调停模式，定义一个中介角色来封装一系列对象之间的交互，使原有对象之间的耦合松散，且可以独立地改变它们之间的交互。中介者能使得程序更易于修改和扩展， 而且能更方便地对独立的组件进行复用， 因为它们不再依赖于很多其他的类。\n中介者模式比较简单，就是把多个对象中的公共方法提升到中介者中完成，发送者调用中介者方法，中介者再去调用对象具体方法，体现了依赖倒置的原则。但是同事中介者模式是一个很忌讳滥用的模式，滥用之后不仅不会优化代码，反而使得中介者类十分冗余。\n看样子它和代理模式有点像，但是代理模式是隐藏对象和加强对象的功能，中介者模式不是。这个模式的重点在于转发和处理对象之间的交互。\n主要包含以下角色：\n抽象中介者角色：它是中介者的接口，提供了同事对象注册与转发同事对象信息的抽象方法。 具体中介者角色：实现中介者接口，定义一个List来管理同事对象（管理同事类），协调各个同事之间的交互关系，因此它依赖于同事角色。 抽象同事类角色：定义同事类的接口，保存中介者对象（交互需要双方都存有对方的信息，不然就是单向的），提供同事对象交互的抽象方法，定义所有相互影响的同事类的公共功能。 具体同事类角色：是抽象同事类的实现者，当需要与其他同事对象交互时，由中介者对象负责后续的交互。 比如租房人和中介和房东的关系，租房人无法直接联系到房东，需要经过中介。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 // 抽象中介者角色 type mediator interface { constact(string, person) //中介给某个对象发送消息，p表示发送人 } // 具体中介者角色 type mediatorStructure struct { houseOwner tenant //如果只有一种结构体则可以存为切片 } // 中介给对应对象发送数据 func (m mediatorStructure) constact(msg string, p person) { switch p.(type) { // 如果类型是房东，就需要将这条数据给租房人 case houseOwner: m.tenant.getMessage(msg) case tenant: m.houseOwner.getMessage(msg) } } // 抽象同事类角色 type person interface { constact(string) //对象给中介发消息 getMessage(string) // 获取中介的消息 } // 房东 type houseOwner struct { name string mediator } // 给中介发消息 func (h houseOwner) constact(msg string) { h.mediator.constact(h.name+\u0026#34;: \u0026#34;+msg, h) } func (h houseOwner) getMessage(msg string) { fmt.Println(\u0026#34;房东收到消息：\u0026#34; + msg) } // 租房人 type tenant struct { name string mediator } func (t tenant) constact(msg string) { t.mediator.constact(t.name+\u0026#34;: \u0026#34;+msg, t) } func (t tenant) getMessage(msg string) { fmt.Println(\u0026#34;租房人收到消息：\u0026#34; + msg) } func main() { // 创建中介 mediatorStructure := mediatorStructure{} // 租房人、房东和中介关联 tenant := tenant{ name: \u0026#34;张三租房\u0026#34;, mediator: mediatorStructure, } houseOwner := houseOwner{ name: \u0026#34;李四房东\u0026#34;, mediator: mediatorStructure, } mediatorStructure.houseOwner = houseOwner mediatorStructure.tenant = tenant // 发送消息 tenant.constact(\u0026#34;我要租套一\u0026#34;) houseOwner.constact(\u0026#34;没房\u0026#34;) } 中介者模式简单来说就是在对象之间加了一个新的对象，它负责对信息进行转发。发送者先把需要发送的信息发往中介，中介就像中转站，可以对收到的信息进行加工等操作，然后再发往接收者。因此中介者需要同事类的信息，同事类也要保存中介者的信息。或者说是原本应该是发送者直接调用接收者的方法，现在变成了发送者调用中介者的方法，中介者再去调用接收者的方法，多包了一层。中介者的主要目标是消除一系列系统组件之间的相互依赖\n优点：\n松散耦合。中介者模式通过把多个同事对象之间的交互封装到中介者对象里面，从而使得同事对象之间松散耦合，基本上可以做到互补依赖。这样一来同事对象就可以独立的变化和复用，而不用像以前那样迁一处而动全身了 集中控制交互。多个同事对象的交互被封装在中介者里面集中管理，使得这些交互行为发生变化的时候，只需要修改中介者对象就可以了，当然如果是已经做好的系统，那么扩展中介者对象，而各个同时类不需要修改。 一对多关联转变为一对一关联。没有使用中介者模式的时候，同事对象之间的关系通常是一对多的，引入中介者模式之后，中介者对象和同事对象的关系变成双向的一对一，这让对象的关系更容易理解和实现。 缺点：\n当同事类太多，中介者的职责将非常大。它会变得复杂庞大，以至于系统难以维护。即一段时间后， 中介者可能会演化成为上帝对象。 过分滥用中介者模式会使中介者实现类十分庞大，所有的方法都需要修改中介者实现类。维护复杂。所以这个模式慎用。不应当在职责混乱的时候使用。 使用场景：\n当一些对象和其他对象紧密耦合以致难以对其进行修改时， 可使用中介者模式。 当组件因过于依赖其他组件而无法在不同应用中复用时， 可使用中介者模式。应用中介者模式后， 每个组件不再知晓其他组件的情况。 尽管这些组件无法直接交流， 但它们仍可通过中介者对象进行间接交流。 如果为了能在不同情景下复用一些基本行为， 导致你需要被迫创建大量组件子类时， 可使用中介者模式。由于所有组件间关系都被包含在中介者中， 因此你无需修改组件就能方便地新建中介者类以定义新的组件合作方式。 迭代器模式（为复杂的聚合对象提供遍历方式，可以向上层隐藏对象的表现形式[上层只能知道结果，但是不清楚数据结构是树还是列表等]，将遍历和存储功能分开来，聚合角色专注存储，迭代器专注遍历） 对复杂聚合对象遍历的一种解决方案，比如树之类的聚合，因为range只能遍历map和切片，如果有树的数据结构就没法使用range，就可以手写一套遍历方案。\n提供一个对象来顺序访问聚合对象中的一系列数据，而不暴露聚合对象的内部表现形式。\n不断向集合中添加遍历算法会模糊其 “高效存储数据” 的主要职责。 此外， 有些算法可能是根据特定应用订制的， 将其加入泛型集合类中会显得非常奇怪。\n另一方面， 使用多种集合的客户端代码可能并不关心存储数据的方式。 不过由于集合提供不同的元素访问方式， 你的代码将不得不与特定集合类进行耦合。\n迭代器通常会提供一个获取集合元素的基本方法。 客户端可不断调用该方法直至它不返回任何内容， 这意味着迭代器已经遍历了所有元素。\n所有迭代器必须实现相同的接口。 这样一来， 只要有合适的迭代器， 客户端代码就能兼容任何类型的集合或遍历算法。 如果你需要采用特殊方式来遍历集合， 只需创建一个新的迭代器类即可， 无需对集合或客户端进行修改。\n主要包含以下角色：\n抽象聚合角色：定义存储、添加、删除聚合元素以及创建迭代器对象的接口。 具体聚合角色：实现抽象聚合类，返回一个具体迭代器的实例。 抽象迭代器角色：定义访问和遍历聚合元素的接口，通常包含hasNext()、next()等方法。 具体迭代器角色：实现抽象迭代器中所定义的方法，完成对聚合对象的遍历，记录遍历的当前位置。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 type student struct { name string number int } //抽象迭代器角色 type Iterator interface { hasNext() bool //判断是否有下一个数据 next() *student } // 具体迭代器角色 type StudentIterator struct { index int students []*student } func (s *StudentIterator) hasNext() bool { return s.index \u0026lt; len(s.students) } func (s *StudentIterator) next() *student { if s.hasNext() { stu := s.students[s.index] s.index++ return stu } return nil } // 抽象聚合角色 type Aggregate interface { CreateIterator() Iterator //创建一个迭代器 } type StudentAggregate struct { students []*student } func (s *StudentAggregate) CreateIterator() Iterator { return \u0026amp;StudentIterator{ students: s.students, } } func main() { // 创建一个集合 students := make([]*student, 0) students = append(students, \u0026amp;student{\u0026#34;张三\u0026#34;, 001}, \u0026amp;student{\u0026#34;李四\u0026#34;, 002}, \u0026amp;student{\u0026#34;王五\u0026#34;, 003}) // 将集合放入聚合角色中 studentAggregate := StudentAggregate{ students: students, } //创建迭代器 iterator := studentAggregate.CreateIterator() stu := iterator.next() fmt.Println(stu) } 优点：\n它支持以不同的方式遍历一个聚合对象（遍历方式存在了具体迭代器中，可以定义新的迭代器来访问原数据），在同一个聚合对象上可以定义多种遍历方式。在迭代器模式中只需要用一个不同的迭代器来替换原有迭代器即可改变遍历算法。 由于引入了迭代器，在原有的聚合对象中不需要再自行提供数据遍历等方法（交由迭代器管理），简化了聚合类的设计。 由于引入了抽象层，增加新的聚合类和迭代器都很方便，无须修改原有代码，满足开闭原则。 可以暂停遍历并在需要时继续，也可以并行遍历同一个集合。 缺点：\n增加了类的个数，一定程度上增加了系统的复杂性。 如果你的程序只与简单的集合进行交互， 应用该模式可能会矫枉过正。 对于某些特殊集合， 使用迭代器可能比直接遍历的效率低。 使用场景：\n当需要为聚合对象提供多种遍历方式时。 当需要为遍历不同的聚合结构提供一个统一的接口时。 当访问一个聚合对象的内容无须暴露其内部细节的表示时。 当集合背后为复杂的数据结构， 且你希望对客户端隐藏其复杂性时 （出于使用便利性或安全性的考虑）， 可以使用迭代器模式。迭代器封装了与复杂数据结构进行交互的细节， 为客户端提供了多个访问集合元素的简单方法。 这种方式不仅对客户端来说非常方便， 而且能避免客户端在直接与集合交互时执行错误或有害的操作， 从而起到保护集合的作用。 访问者模式（解决不同角色调用成员结构体元素方法时有不同情况的问题，解决复杂结构体结构不变但成员变量操作处理逻辑易变的问题，把对数据的操作都封装到访问者类中，我们只需要调用不同的访问者，而无需改变结构类。平时工作中很少使用到这种模式） 封装一些作用于某种数据结构中的各元素的操作，它可以在不改变这个数据结构的前提下定义作用于这些元素的新的操作。\n访问者模式是一种将数据操作与数据结构分离的设计模式，它是 《设计模式》中较为复杂的一个，但它的使用频率并不高，正如《设计模式》的作者 GOF 对访问者模式的描述：大多数情况下，你并不需要使用访问者模式，但是当你一旦需要使用它时，那你就是真正的需要它了。\n访问者模式的基本思想是，软件系统中拥有一个由许多对象构成的、比较稳定的对象结构，这些对象的类都拥有一个 accept 方法用来接受访问者对象的访问。访问者是一个接口，它拥有多个 visit 方法（参数不同，在golang中表现为该访问者针对不同成员结构体有不同的解决方案），这个方法对访问到的对象结构中不同类型的元素做出不同的处理。在对象结构的一次访问过程中，我们遍历整个对象结构，对每一个元素都实施 accept 方法，在每一个元素的 accept 方法中会调动访问者的 visit 方法，从而使访问者得以处理对象结构的每一个元素，我们可以针对对象结构设计不同的访问者类来完成不同的操作，达到区别对待的效果。\n包含以下角色：\n抽象访问者角色：定义了对每一个元素访问的行为，它的参数就是可以访问的元素，它的方法个数理论上来讲与元素类个数是一样的（抽象元素角色的实现类的个数），从这点不难看出，访问者模式要求元素类的个数不能改变。 具体访问者角色：给出对每一个元素类访问时所产生的具体行为。 抽象元素角色：定义了一个接受访问者的方法，其意义是指，每一个元素都要可以被访问者访问。 具体元素角色：提供接受访问方法的具体实现，而这个具体的实现，通常情况下是使用访问者提供的访问该元素类的方法。 对象结构角色：定义当中所提到的对象结构，对象结构是一个抽象表述，具体点可以理解为一个具有容器性质或者复合对象特性的类，它会含有一组元素，并且可以迭代这些元素，供访问者访问。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 // 抽象元素角色 type animal interface { accept(person) } // 具体元素角色 type dog struct { } func (d dog) accept(p person) { p.doDog(d) fmt.Println(\u0026#34;狗的接受接口\u0026#34;) } type cat struct { } func (c cat) accept(p person) { p.doCat(c) fmt.Println(\u0026#34;猫的接受接口\u0026#34;) } // 抽象访问者角色 type person interface { doDog(d dog) doCat(d cat) } // 具体访问者角色 type owner struct { } func (o owner) doDog(d dog) { fmt.Println(\u0026#34;主人控制狗\u0026#34;) } func (o owner) doCat(d cat) { fmt.Println(\u0026#34;主人控制猫\u0026#34;) } type someOne struct { } func (s someOne) doDog(dog) { fmt.Println(\u0026#34;其他人控制狗\u0026#34;) } func (s someOne) doCat(cat) { fmt.Println(\u0026#34;其他人控制猫\u0026#34;) } // 对象结构角色 type Home struct { animal []animal } func (h *Home) action(p person) { for _, v := range h.animal { v.accept(p) } } func main() { // 创建home对象 home := Home{ animal: make([]animal, 0), } // 添加元素 home.animal = append(home.animal, dog{}, cat{}) home.action(owner{}) } 优点：\n扩展性好。在不修改对象结构中的元素的情况下，为对象结构中的元素添加新的功能（只需要增加新的访问者来专门针对这个功能即可）。 复用性好。通过访问者来定义整个对象的结构通用的功能，从而提高复用程度。 分离无关行为。通过访问者来分离无关的行为，把相关的行为封装在一起，构成一个访问者，这样每一个访问者的功能都比较单一 缺点：\n对象结构变化困难。每增加一个新的元素类，都要在每一个具体访问者类中增加相应的具体操作，这违背了开闭原则 违反了依赖倒置原则。访问者模式依赖了具体类，而没有依赖抽象类。 从本质来说，访问者模式中元素角色只有accept方法来调用访问者中的具体方法，这导致如果一个元素有两个方法的话，必须要新增一个访问者来针对这个新的方法，这会导致访问者的职责混乱，有些表示不同访问者，有些又表示同一访问者访问元素的不同方法。 使用场景：\n对象结构相对稳定，但其操作算法经常变化的程序。（此时就可以将操作包到访问者类中，抽象访问者定义的是大概操作，而可以定义不同的具体访问者来表示不同的操作） 对象结构中的对象需要提供多种不同且不相关的操作，而且要避免让这些操作的变化影响对象结构。 备忘录模式（用于对象的备份，当你需要创建对象状态快照来恢复其之前的状态时， 可以使用备忘录模式） 备忘录模式提供了一种状态恢复的实现机制，使得用户可以方便的回到一个特定的历史步骤，当新的状态无效或存在问题时，可以使用暂时存储起来的备忘录将状态复原，很多软件都提供了撤销操作，如Word、记事本、数据库或操作系统的备份、棋类游戏的悔棋功能都属于这一类。\n又叫快照模式，在不破坏封装性的前提下，捕获一个对象的内部状态，并在该对象之外保存这个当前状态，以便以后当需要时能将该对象恢复到原先保存的状态。\n主要包含以下角色：\n发起人角色：记录该对象当前时刻的内部状态信息，提供创建备忘录和恢复备忘录数据的功能，实现其他业务功能，它可以访问备忘录里的所有信息。 备忘录角色：负责存储发起人的内部状态，在需要的时候提供这些内部状态给发起人。 管理者角色：对备忘录进行管理，提供保存与获取备忘录的功能，但其不能对备忘录的内容进行访问与修改。 备忘录有两个等效的接口：\n窄接口：管理者对象（和除发起人对象之外的任何对象）看到的是备忘录的窄接口，这个窄接口只允许他把备忘录对象传给其他的对象（只能获取备忘录对象，不能对备忘录对象里面的元素访问和修改）。 宽接口：与管理者看到的窄接口相反，发起人对象可以看到一个宽接口，这个宽接口允许他读取所有的数据，以便根据这些数据恢复这个发起人对象的内部状态。 优缺点：\n提供了一种可以恢复状态的机制。当用户需要时能够比较方便地将数据恢复到某个历史状态。 实现了内部状态的封装。除了创建它的发起人之外，其他对象都不能够访问这些状态信息（黑箱备忘录模式）。 简化了发起人的职责。发起人不需要管理和保存其内部状态的各个备份，所有状态信息都保存在备忘录中，并由管理者进行管理，这符合单一职责原则。 可以在不破坏对象封装情况的前提下创建对象状态快照。 缺点：\n资源消耗大。如果要保存的内部状态信息过多或特别的频繁，将会占用较多的内存资源。 负责人必须完整跟踪发起人的生命周期， 这样才能销毁弃用的备忘录。 使用场景：\n需要保存和恢复数据的场景，如玩游戏时的中间结果的存档功能。 需要提供一个可回滚操作的场景。如word撤销，数据库事务操作等。 白箱备忘录模式 备忘录角色对任何对象都提供一个接口，即宽接口，备忘录角色的内部所存储的状态对所有对象公开（违背了备忘录模式的窄接口概念）。\n案例：游戏挑战BOSS，并且可以恢复到决斗前\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 // 发起人对象，即要存进备忘录中的对象;游戏角色类 type gameRole struct { hp int mp int } // 初始化内部状态 func (g *gameRole) initState() { g.hp = 100 g.mp = 100 } // 开战 func (g *gameRole) fight() { g.hp = 40 g.mp = 0 } // 保存角色状态 func (g *gameRole) saveState() *roleStateMemento { return \u0026amp;roleStateMemento{ hp: g.hp, mp: g.hp, } } // 恢复角色状态 func (g *gameRole) recoverState(r *roleStateMemento) { // 将备忘录对象中存储的角色状态赋值给当前角色 g.hp = r.hp g.mp = r.mp } func (g *gameRole) stateDisplay() { // 展示角色信息 fmt.Printf(\u0026#34;hp:%v --mp:%v\\n\u0026#34;, g.hp, g.mp) } // 备忘录对象，保存发起人对象数据 type roleStateMemento struct { hp int mp int } // 备忘录对象管理者角色 type roleStateCaretaker struct { *roleStateMemento } func main() { fmt.Println(\u0026#34;-------开战前----\u0026#34;) // 初始化角色 gameRole := gameRole{} gameRole.initState() gameRole.stateDisplay() // 保存状态 roleStateCaretaker := roleStateCaretaker{ roleStateMemento: gameRole.saveState(), } fmt.Println(\u0026#34;-------开战后----\u0026#34;) gameRole.fight() gameRole.stateDisplay() fmt.Println(\u0026#34;-------回滚----\u0026#34;) gameRole.recoverState(roleStateCaretaker.roleStateMemento) gameRole.stateDisplay() } 白箱备忘录模式是破坏了封装性的，其中任何对象都可以访问备忘录角色 roleStateMemento，任何对象都可以对其进行赋值修改。\n黑箱备忘录模式 备忘录角色对发起人提供一个宽接口（在发起人中定义一个内部的备忘录角色，并在保存备忘录角色时同时赋值），而为其他对象提供一个窄接口（定义一个备忘录角色的接口，外部只能获取接口，无法获取具体结构体）。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 // 发起人对象，即要存进备忘录中的对象;游戏角色类 type gameRole struct { hp int mp int *roleStateMemento } // 初始化内部状态 func (g *gameRole) initState() { g.hp = 100 g.mp = 100 } // 开战 func (g *gameRole) fight() { g.hp = 40 g.mp = 0 } // 保存角色状态 func (g *gameRole) saveState() memento { // 发起人可以直接通过访问内部元素的方法修改和访问备忘录角色，而其他角色无法获取到 g.roleStateMemento = \u0026amp;roleStateMemento{ hp: g.hp, mp: g.hp, } return g.roleStateMemento } // 恢复角色状态 func (g *gameRole) recoverState(r memento) { // 将备忘录对象中存储的角色状态赋值给当前角色 hp, mp := r.getHpAndMp() g.hp = hp g.mp = mp } func (g *gameRole) stateDisplay() { // 展示角色信息 fmt.Printf(\u0026#34;hp:%v --mp:%v\\n\u0026#34;, g.hp, g.mp) } // 黑箱最重要的点就是给备忘录定义一个接口，让备忘录对象实现它，这样管理者角色和其他角色就无法直接获取和修改备忘录中的数据 type memento interface { getHpAndMp() (int, int) } // 备忘录对象，保存发起人对象数据 type roleStateMemento struct { hp int mp int } func (r roleStateMemento) getHpAndMp() (int, int) { return r.hp, r.mp } // 备忘录对象管理者角色 type roleStateCaretaker struct { memento } func main() { fmt.Println(\u0026#34;-------开战前----\u0026#34;) // 初始化角色 gameRole := gameRole{} gameRole.initState() gameRole.stateDisplay() // 保存状态 roleStateCaretaker := roleStateCaretaker{ memento: gameRole.saveState(), } fmt.Println(\u0026#34;-------开战后----\u0026#34;) gameRole.fight() gameRole.stateDisplay() fmt.Println(\u0026#34;-------回滚----\u0026#34;) gameRole.recoverState(roleStateCaretaker.memento) gameRole.stateDisplay() } 解释器模式（使用场景较少，开发一种简单的解释器来处理某种特定的规则，定义的规则可以成为一个语法树，采用递归或循环来实现。场景有规则引擎、自定义语法、sql解析等，就是将一堆字符串解释成golang的代码） 给定一个语言，定义它的文法表示，并定义一个解释器，这个解释器使用该标识来解释语言中的句子。在解释器模式中，我们需要将待解决的问题提取出规则，抽象为一种语言。比如加减法运算的规则就是由数值和符号组成的合法序列\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 // 环境类，存储公用数据 type contexts struct { expr map[variable]int } // 抽象表达式类 type AbstractExpression interface { interpret(contexts) int } // 变量类，终结符表达式 type variable struct { name string } func (v *variable) interpret(c contexts) int { // 直接返回变量的值 return c.expr[*v] } // 加法表达式，非终结符表达式 type plus struct { left AbstractExpression right AbstractExpression } func (p *plus) interpret(c contexts) int { // 将左边表达式和右边表达式相加 return p.left.interpret(c) + p.right.interpret(c) } type minus struct { left AbstractExpression right AbstractExpression } func (m *minus) interpret(c contexts) int { // 将左边表达式和右边表达式相减 return m.left.interpret(c) - m.right.interpret(c) } func main() { // 创建环境对象 contexts := contexts{ expr: make(map[variable]int), } contexts.expr[variable{\u0026#34;a\u0026#34;}] = 1 contexts.expr[variable{\u0026#34;b\u0026#34;}] = 2 contexts.expr[variable{\u0026#34;c\u0026#34;}] = 3 contexts.expr[variable{\u0026#34;d\u0026#34;}] = 4 // 计算a+b-c+d,递归调用会先计算最里面的，因此需要先把b-c计算出来 expr := plus{ left: \u0026amp;variable{\u0026#34;a\u0026#34;}, right: \u0026amp;plus{ left: \u0026amp;variable{\u0026#34;d\u0026#34;}, right: \u0026amp;minus{ left: \u0026amp;variable{\u0026#34;b\u0026#34;}, right: \u0026amp;variable{\u0026#34;c\u0026#34;}, }, }, } fmt.Println(expr.interpret(contexts)) } 优缺点：\n易于改变和扩展文法。由于在解释器模式中使用类来表示语言的文法规则，因此可以通过继承等机制来改变和扩展语法。每一条文法规则都可以表示为一个类，因此可以方便地实现一个简单的语言。 实现文法较为容易。在抽象语法树中每一个表达式节点类的实现方式都是相似的，这些类的代码编写都不会特别复杂。 增加新的解释器表达式较为方便，增加一个终结或非终结式表达式即可，原有表达式类代码无须修改，符合开闭原则。 缺点：\n对于复杂文法难以维护。在解释器模式中，每一条规则至少需要定义一个类，因此如果一个语言包含太多文法规则，类的个数将会急剧增加，导致系统难以管理和维护。 执行效率较低。由于在解释器模式中使用了大量的循环或递归调用，因此在解释较为复杂的句子时其速度很慢，而且代码的调试过程也比较麻烦。 可使用场景少。 使用场景：\n当语言的文法较为简单，且执行效率不是关键问题时。 当问题重复出现，且可以用一种简单的语言来进行表达时。 当一个语言需要解释执行，并且语言中的句子可以表示为一个抽象语法树的时候。 ","date":"2021-03-20T00:00:00Z","permalink":"https://xiaonuoz.github.io/p/%E4%BA%86%E8%A7%A3%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/","title":"了解设计模式"},{"content":"\n时间复杂度 算法的时间复杂度是T(n), T是time，它与问题规模n的大小有关，且可以事先预估时间。因为时间复杂度的函数在n规模大时，其颇为复杂，**因此在问题规模足够大时，可以忽略表达式中的低阶部分，**即只保留时间开销表达式最高阶的那部分，如T(n)=n的三次方+n的二次方+99999，只需要保留n的三次方即可。经典的抓大放小\n甚至可以去掉系数，如3n简化为n，因为3000和9000依然是一个数量级的，但是n和n的平方相差的数量级就极大。\n结论：\n顺序执行的代码只会影响常数列，可以忽略； 只需要挑循环中的一个基本操作，分析它的执行次数与n的关系即可。如循环3000次而代码执行了3000次则为n，执行9000次为3n，再省略常数项依然是n； 如果有多层嵌套循环，只需关注最深层循环循环了几次 ;如循环3000次而代码执行了 3000的平方 的话，则时间复杂度函数是O(n的平方) 即函数是 循环次数与最深层代码循环的关系，如3000对9000则是3n; 3000对9000000则是n的平方\n算法的性能问题只有在n很大的时候才会暴露出来\n空间复杂度 和时间复杂度一样，都是分析所占空间大小和问题规模的关系\n和时间复杂度类似，只需要关注和问题规模相关的变量即可。即当n变为无穷时，对应的变量的内存消耗是否会跟着改变\n数据的逻辑结构 集合 线性结构 树形结构 图状结构 数据的存储结构 顺序存储 链式存储 每个元素在存储自己的数据元素的同时还会额外存储下一个元素的指针，以便指向下一个元素\n单链表 带头节点的头节点不存储数据，但是会存储下一个节点的指针。仅仅是之后实现基本操作时更方便些，可以实现如果要在最开头插入一个节点，修改头节点的next和新增节点的next即可\n下面是带头节点的情况，这种是最常使用的。头节点设为第0个。如果不带头节点则最开始的判断可以让其在0时进行赋值\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 type LNode struct { data string next *LNode } func main() { L := new(LNode) // 在某个节点后插 insertList(L, 1, \u0026#34;abc\u0026#34;) insertList(L, 2, \u0026#34;def\u0026#34;) // 在某个节点前插(两个next则是在2节点之前，从0头节点开始计数) insertListTwo(L.next.next, \u0026#34;obg\u0026#34;) // 删除某个节点 deleteList(L, 3) for { // 先运行一次L=L.next是去除头指针，头指针是没有数据的 L = L.next if L == nil { return } fmt.Println(L.data) } } func insertList(LinkList *LNode, i int, data string) { if i \u0026lt; 0 { return } var p *LNode p = LinkList j := 0 // j表示当前所在节点，最开始是0即头节点，通过if向下循环到需要操作的节点 // 当i为1表示在1节点插入数据，不会走此循环，即将头节点的next给了p 1节点,头节点换存p节点的指针 for k := 0; k \u0026lt; i; k++ { if p != nil \u0026amp;\u0026amp; j \u0026lt; i-1 { p = p.next j++ } } S := new(LNode) S.data = data S.next = p.next p.next = S return } func insertListTwo(LinkList *LNode, data string) { if LinkList == nil { return } S := new(LNode) S.data = data S.data, LinkList.data = LinkList.data, S.data S.next = LinkList.next LinkList.next = S } func deleteList(LinkList *LNode, i int) { if i \u0026lt; 0 { return } var p *LNode p = LinkList j := 0 for k := 0; k \u0026lt; i; k++ { if p != nil \u0026amp;\u0026amp; j \u0026lt; i-1 { p = p.next j++ } } if p == nil { return } if p.next == nil { // 第i-1个节点已经是nil了，则无法删除第i个节点 return } q := p.next // 删除节点则是i-1的q节点的下一个 next p.next = q.next // 将删除节点的下一个节点赋值给p,则为上一个节点 // 释放q q = nil return } 尾插法建立单链表\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 var x string L := new(LNode) r := L fmt.Scanln(\u0026amp;x) for { if x != \u0026#34;9999\u0026#34; { s := new(LNode) s.data = x r.next = s // 让r的指针永远指向最后一个元素。 r = s fmt.Scanln(\u0026amp;x) } else { // 这里将r.next变为nil是避免野指针的情况发生，即不为空有可能会指向内存中未知的指针中的数据[在C中会发生，但在go中用new初始化时会将结构体中的数据都进行零值初始化。但是加上是一个好习惯] r.next = nil break } } for { fmt.Println(L, L.next) if L.next == nil { return } L = L.next } 单链表的逆置\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 p := T.next // 在此处将T进行断链，非常重要的一步。 T.next = nil for { if p == nil { break } // 存储p的下一个节点 k := p.next // T.next最开始是nil，即指向虚无，第二次循环时T的next已经是原来的p1了，即p2.next=p1,再将T头指针的next指向p,现在就是头-p2-p1 // 再次循环，此时k存储的是p3,p是p2，将p2的next存储为T.next即p2-p1 // 下面两句就是非常常规的头插法代码 p.next = T.next T.next = p p = k } 双链表就是每个节点存了前一个和后一个的指针、循环单/双链表则是最后一个节点又指向了头指针\n索引存储 散列存储 数据结构的三要素 逻辑结构 物理结构(存储结构) 数据的运算 在声明定义阶段需要声明数据的逻辑结构和对其的运算方法(创建销毁和curd)。而在初始化时则可根据实际情况选择一种存储结构进行初始化，只有这三个条件都满足。才算实现了数据结构\n栈（Stack） 栈是只允许在一端[表尾]进行插入或删除操作的特殊线性表\n操作分为入栈和出栈，对应线性表的插入和删除\n空栈：栈中没有任何数据\n共享栈指两个栈共享同一片空间，只出现在顺序表实现的栈中，两个栈一个从栈顶往下写，另一个从栈底往上写\n如果使用链表进行初始化栈，它是用头插法在头节点之后进行插入数据和删除数据的，因为链表从头节点向后指，如果想符合后进先出的原则就必须使用头插法，尾插法在每次插入和删除新数据时都需要循环一遍节点，严重影响了性能，头插法的时间复杂度就是O(1)\n不带头链表也如此，在最前头进行插入即可。头即为栈顶\n队列（Queue） 队列是只允许在一端进行插入，在另一端删除的特殊线性表\n操作分为入队和出队，对应线性表的增加和删除\n它符合先进先出原则，只能在队头进行出队，在队尾进行入队\n使用顺序表实现队列 front存储的是队头元素的下标。而rear存储的是队尾可以插入元素的下标，即当前队尾元素的下一个元素。初始化时就让rear和front都指向0\n**队尾指针指向下一个可以插入数据的节点，**因此当数组9下标插完后应指向10，通过取余重新指向了0，如果此时0是有数据的则会符合对头等于队尾的判断，表示为空队列，为了避免判断重合则可以牺牲一个存储单元，在当他为9时则进行判断是否已满，如果不相等则说明0下标的元素已经被取走，可以继续存储，从而形成了一个环状的逻辑。判断是在存储之前进行，因此判断0下标是否相等时是往9下标插入数据[放弃了一个存储单元]。\n因为顺序表静态数组是固定一个大小的，因此就需要通过循环队列取模的方式来将有限变为无线，逻辑上扩大了存储范围\n计算元素个数则可以用 (rear+MaxSize-front) %MaxSize 用队尾加数组总量再减去队头指针 最后取余。要注意的是rear指的是队尾指针的下一个可插入的元素，而不是当前元素\n存满就是MaxSize-1 ，因为舍弃了一个数据单元\n使用单链表实现队列则是定义队尾和队头的指针即可，初始化时头尾都指向头节点，如果是不带头节点的话则头尾都指向null\n中缀表达式、前缀表达式、后缀表达式 因为计算机是顺序读取，因此在日常中常见的中缀表达式对于计算机来说是无法处理的（要判断括号），将其转换成前缀或后缀表达式才可以理解（直接从头到尾挨个计算就可以了）。因此编译程序就会将中缀转成计算机可以理解的表达式，然后通过机器指令传给计算机计算\n后缀表达式可以根据左优先原则，只要左边的运算符能先计算，就优先算左边的\n要注意的是： 先出栈的是“右操作数”，若表达式合法，则最后栈中只会留下一个元素，即最终结果\nKMP算法 next[2]时，S只能为1，因此无论串的长度内容多复杂，它的S的最长相等前后缀长度都是为0，因此next[2]永远确定是为1的\nnext[1]=0时，会让主串和模式串的i,j都往后移动一位，重新开始匹配，因为第一位都对不上了，就只能都后移重新匹配\n如果不会经常出现字串和模式串部分匹配问题，使用KMP算法的意义并不大。\n树 树是从上往下的，因此树的结点路径只能从上往下计算，路径长度则是经过了几条边\n结点的度指的是有几个子节点。树的度则是指每个节点的度的最大值，最大值则等于树的度\n树分有序树和无序树，如果同一层结点的左右位置反映某些逻辑关系则是有序树，反之是无序树\n子节点及子节点向后的节点也可以合并称之为子树，如A节点有三棵子树。B及下面所有，C及下面所有等\n完全二叉树如果某个节点只有一个度，那么它的子节点一定是左子节点\n因此设计平衡排序二叉树尽量的宽，高度尽量低\n二叉树 n个节点只需要n-1个指针就能连接，剩余了n+1个空指针\n如果经常要查询对应节点的父节点就需要使用三叉链表，多存储一个指针来存父节点的指针\n二叉树的先中后序遍历 采用分支节点逐层展开发\n二叉树的层序遍历 如果要通过序列推算出二叉树的形状，一定是 前/后/层级+中级两个才能共同确定出一个树的形状。胡乱搭配是无法推算出的\n线索二叉树 二叉树找前驱和找后继必须要重新遍历一次才行，因此将**空闲的左孩子指针和右孩子指针[空链域]**充当为前驱和后继，并设置一个tag标志就可以实现线索二叉树\n树的存储结构 森林和二叉树的转换 二叉排序树 二叉排序树的查找效率很大程度上取决于树的高度\n平衡二叉树 LL和RR 总共有四种情况，记住左孩子只能右上旋，右孩子左上旋即可。\n一定要注意的是，在A结点的B孩子的子树上插入节点，并不意味着子树原先就是叶子节点，它可能是分支\n在第一次出现大于1的地方，这个节点和它下面的节点组成最小不平衡子树。\n哈夫曼编码 图 最小生成树 在P城和矿场连通后，此时从渔村到P城已经有路线了，因此P到渔村的直线边即使权值最小也不能进行连接\n排序 快速排序 快速排序算法是所有内部排序算法中平均性能最优的排序算法\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 func main() { translate := [10]int{4, 78, 37, 49, 22, 77, 31, 57, 27, 19} QuickSort(\u0026amp;translate, 0, 9) fmt.Println(\u0026#34;arr:=\u0026#34;, translate) } // 用第一个元素作为基准划分为左右两部分,low左指针，high右指针 // 这个方法可以让基准左边的都小于基准，右边的都大于基准，但是不能保证左边和右边是有序的，它们仅仅是都大于或小于基准 // 紧接着通过阶乘的方式缩短范围，直至缩短到low和high不满足条件位置(low\u0026gt;=high) func partition(translate *[10]int, low int, high int) int { pivot := translate[low] // if low \u0026gt;= high 跳出循环 for low \u0026lt; high { // 比较右指针的数据是否比pivot大，大则继续往前移动 for low \u0026lt; high \u0026amp;\u0026amp; translate[high] \u0026gt;= pivot { high = high - 1 } // 原先的基准被pivot暂存，因此可以覆盖 translate[low] = translate[high] // 比较左指针的数据是否比pivot小，小则继续往前移动 for low \u0026lt; high \u0026amp;\u0026amp; translate[low] \u0026lt;= pivot { low = low + 1 } // high的数据放在了原基准的位置，可以覆盖 translate[high] = translate[low] } translate[low] = pivot // 返回存放基准的最终位置 return low } // 快速排序 func QuickSort(translate *[10]int, low int, high int) { if low \u0026lt; high { // 划分 pivotpos := partition(translate, low, high) QuickSort(translate, low, pivotpos-1) QuickSort(translate, pivotpos+1, high) } } 各种排序方法的综合比较 时间性能 空间性能 操作系统小结 进程相关的原语有 创建原语、撤销原语、阻塞原语、唤醒原语、切换原语\n短作业优先 优先级调度算法 多级反馈队列调度算法 PV 因此最好只在临界区里面做最重要的操作\n互斥是指对某一临界资源在某段时刻只能有一个进程使用，PV写在同一进程中，初始值为临界资源的数量。\n同步则是一个进程的发生需要等待另一个进程结束或到某一结点，通常根据实际情况设置初始值，需要在先发生的进程中进行V操作，后发生的进程中进行P操作，这样如果处理机先执行后一个进程也会因为P的值小于0导致进程从运行态转变为阻塞态，直到对应V操作+1以后重新从等待队列唤醒对应进程进入就绪态\n苹果问题（多生产者多消费者） 当缓冲区为1时，由于同一时刻至多只有一个大于0，其他都为0，就可以省略mutex互斥\n吸烟者问题（一生产者多消费者生产不同类型的消息） 读者问题（写者互斥，读者可同时读取） 哲学家问题（进程需要同时持有多个临界资源且每个进程互斥） 管程 管程中有很多个入口（函数），但是每次只会开放其中的一个入口给某一个进程使用。如在生产者消费者问题中，各进程需要互斥的访问共享缓冲区。管程的这种特性即可保证一个时间段内最多只有一个进程在访问缓冲区。要注意的是：这种互斥特性已经由编译器帮忙实现了，程序员并不需要关心，程序员只需要调用特定的管程方法操作管程即可，其中的细节已经封装到底层了。\n死锁 银行家算法 在每次分配资源时先判断这次分配资源是否会发生不安全状态，造成死锁\n死锁的检测 死锁的解除 内存知识 动态重定位：在程序装入内存中时，程序段中的依然还是逻辑地址，只有当程序运行时，cpu会读取重定位寄存器存放的起始位置(程序运行时会先将起始地址从pcb存到重定位寄存器中)，将它与程序段中的逻辑地址相加计算就可以得到物理地址。这是当代计算机使用的装入方式最优解\n基本分页存储管理 cpu在得到逻辑地址后，需要访问两次内存，第一次访问是查页表，第二次则是访问目标单元\n系统仅有一个页表寄存器，而在PCB记录的是这个进程对应的页表。一个系统中可能会有多个页表对应多个进程。当程序运行时就会将PCB的页表数据覆盖给页表寄存器中\n快表 基本分段存储管理方式 段页式管理方式（分页和分段经典折中） 一个进程有一个段表，但是可能会有多个页表（因为每个段表都对应一个页表）。而使用段页存储时，内存是划分为固定长度的多个小区间，因为段页存储最后是由页去操作实际地址\n用户只需要提供段号和段内地址，因为分段是用户可见的，而分页是由操作系统进行分页的，是对用户不可见的，操作系统会帮用户将段内地址拆分为页号和页内偏移量。因此这个存储管理方式是二维的。\n虚拟内存（目前计算机主流设计方案） 前面几个内存存储管理方式有很多缺点，最大的缺点则是只能一次性将数据全部存入内存中，这会导致过大的程序无法运行在小内存中\n请求分页管理方式 要注意的是：只有写指令才需要修改 修改位，并且，一般只修改外存中的修改位即可。这样可以减少访存次数。但是如果是将某个页面换回外存，则需要先将外存最新的数据写回慢表中，然后删除快表对应数据，然后慢表将数据覆盖到外存忠，最后修改对应慢表数据。\n在页面调入内存后，不仅需要修改慢表的数据，页需要将表项复制到快表中。这样查询就可以减少访存次数。\n页面置换算法 通过四轮扫描就可以发现，第一轮先淘汰最近没访问且没被修改过的页面；第二轮淘汰最近没访问但被修改过的页面；第三轮经过第二轮将其访问位都改为0后，淘汰的就是最近访问过但是没有修改过的页面。第四轮则是淘汰最近访问过并且修改过的页面。\n驻留集是指系统给进程所分配的内存块的集合\n文件管理 索引顺序文件 目录结构 文件分配方式-索引分配 但是如果每个索引块中只有一个表项有实际指向数据，就会造成极大的浪费以及不必要的多次读盘操作，\n因此混合索引诞生解决了此问题。\n磁盘调度算法 I/O相关知识 DMA 采用DMA之后数据就不再需要先存到cpu寄存器中再转存内存了\nDMA控制器读写数据的基本单位依然是一个字，但是以读来说，它会先从磁盘中读入一个字放入DR，然后DR再放到内存。然后进行下一个循环，直到读完一个块的数据后(读完DC需要的字节数)，才会给CPU发出中断。\n通道控制方式 通道程序就是一系列针对I/O的通道指令，这种方法CPU干预率进一步降低，通道只有在读取完CPU指定的全部数据后才会发出中断\n","date":"2021-03-15T00:00:00Z","permalink":"https://xiaonuoz.github.io/p/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/","title":"计算机基础"},{"content":"Redis,远程字典服务，是由C语言开发的一个开源的高性能键值对数据库。\nset key value 添加修改数据,get/del key 获取删除数据。操作时返回 (integer) 1 则是操作成功，0则是失败\nredis为每个独立的redis服务提供了16个数据库，编号从0到15。每个数据库之间的数据相互独立，且数据库大小是动态扩容。通过 select [0-15] 来切换到其他数据库中。默认是在0号数据库。\nmove key db 将当前库的某个key移动到其他库中，当前库必须要有这个key且对方库必须不能存在这个key，两者任意一个条件不满足都会移动失败。\ndbsize 可以查看当前库存在多少个key。flushdb 清空当前库的key flushall 清空所有key\n数据类型 string 在setex之后再使用setex会刷新时效性为新设置的秒数值\nhash hsetnx key field value 设置field的值，如果原本存在则不设置，不存在才进行set\nlist stop为-1就表示最后一个数据，lindex表示查看某个索引的值，查最后一个也可以使用-1\n读取只有lrange，这说明读索引依然是从左往右读取，因此如果是lpush那它是从最后一个往前插入数据；rpush则是从第一个开始往后添加数据\nlpop表示移出list中左边的一个值，rpop则是右边出一个。根据索引都是从第一条往后读，lpop是从第一个开始往下进行移出，rpop是从最后一个往前移出\n并且它可以同时等待多个list，某个list有就取，全没有才进行等待。\nlrem key count value 移除指定数据，count表示移除几个，因为值等于value的可能会有多个\nset set类型不允许值数据重复，如果添加的数据在set中已经存在一份，则会添加失败。[因为添加相同的数据是没有意义的]。可以根据这个去重特性做权限控制或者网站访问统计\nsorted_set zremrangebyrank是根据索引删除，且索引来自于zrange显示的顺序。\nsorted_set底层存储依然是基于set结构的，因此数据不能重复，重复添加一个相同的数据，它的值会添加失败，但是如果新增时附带的score值不同，它的score值将被反复覆盖，保留最后一次修改的结果。\nkey通用操作 Linux安装redis 下载安装一条龙\n1 2 3 4 weget https://download.redis.io/releases/redis-6.2.3.tar.gz tar -zxvf ... cd redis make install 服务器的redis通常会启动多个，因此启动根据配置文件进行启动\n1 2 3 4 5 6 7 8 9 10 11 12 13 cp redis.conf redis-6379.conf vim redis-6379.conf //需要修改的字段有 bind 绑定的ip port 启动的端口号 daemonize yes 以守护进程的方式启动，而不是前端启动 logfile \u0026#34;redis-6379.log\u0026#34; 日志的文件名 dir /user/local/src/redis/data 日志的路径名 最后以配置文件方式启动 redis-server redis-6379.conf 连接则是redis-cli -p PORT -h HOST 持久化(优先使用AOF) 官方推荐两个方法同时使用\nRDB 如果出现意外情况导致redis崩溃消失，在下次启动时，redis会自动加载这个dump.rdb文件进行数据恢复，不需要人为操作恢复\n要注意的是 save指令的执行会阻塞当前redis服务器(执行完才能进行后续的set等命令)，直到当前rdb过程完成为止，有可能造成长时间阻塞，线上环境不建议使用。\n可以使用bgsave来替代save，它会生成一个Linux子进程，而不占用redis的进程\nbgsave命令是针对save阻塞问题做的优化。redis内部所有涉及到reb操作都建议采用bgsave的方式。save命令可以放弃使用了\n但是不能说每次程序调用set方法就由程序发送bgsave方法，可以设置自动执行来让服务器自己进行保存\n发生变化包括修改原有key的value，新增key，删除key\nAOF 最后根据策略将AOF缓存的写记录数据存放到 .aof文件中\n**bgrewriteaof 手动后台重写，逻辑和bgsave相似。**要注意的是对一个数据修改删除之后,重写完这个数据的操作会消失，因为它在redis中已经被删除了\n在这两种重写条件中选择一种即可，而size的单位是字节，即linux输入ls -l显示的大小值\n事务 事务的工作流程 锁 watch锁（乐观锁，通过监听一个字段是否改变来确定后面的事务是否失败） 终止执行是直接discard。之前的执行语句都得重写进行\nwatch和mysql的锁有点区别，watch定义在开启事务之前，且watch key和事务中的字段没关系。比如对name进行watch，然后开启事务后set的是 age，假如在没有执行exec前另一个服务器对name字段的值进行了改变，它是可以成功改变name的值的。而此时执行exec就会失败，不存在阻塞的情况。\n只有当watch的对象没有改变时，后面的事务才能成功执行。watch的key改变与否影响着下一个事务的执行，但key的值和事务中的操作值没有关系。watch定义在事务开始之前，和mysql完全不相同\nsetnx 锁（悲观锁，通过设置一个锁来确定之后代码是否执行） 它依然不会阻塞，且value值可以任意，假如在会话1中 setnx lock-age 1 ,此时会话1可以 set age 3 ,但会话2也可以 set age 3 ,**这个锁不会对age字段进行加锁。**它只会对 lock-age 加锁，如果会话2再加一次 set lock-age 1 就会发生失败，因为程序中每次都要使用一次lock-age ，如果失败则意味着有其他地方加锁了。解锁则是删除这个锁，del lock-age 但是经常会出现加锁却忘记解锁的情况，就需要使用 expire lock-key second 来给锁加个生存周期。这个指令只能添加生存周期，因此加生存周期之前要使用setnx进行加锁\n删除策略 过期数据 过期数据一般不会立即删除，它会根据删除策略来延迟删除。过期数据的底层就是开辟了一个空间来存key的内存指针和过期时间，如果到了这个时间点才根据策略进行删除。而开辟的这个空间的结构是hash类型，一个failed一个value,failed存key的内存指针，value存它的过期时间\nredis会在内存占用和CPU占用之间寻找一个平衡，顾此失彼都会造成整体redis性能的下降，甚至引发服务器宕机或内存泄漏\n删除策略 拿时间换空间\n拿空间换时间（图片写错了）\n折中方案，server.hz通过info server 查看hz\n逐出算法 前四个是操作过期数据，后三个操作全部数据，最后一个都不处理\nredis.conf配置部分介绍 高级数据类型 Bitmaps(针对状态统计) Bitmaps本身不是一种数据结构， 实际上它就是字符串，但是它可以对字符串的位进行操作**。Bitmaps并不是实际的数据类型，而是定义在String类型上的一个面向字节操作的集合。** Bitmaps单独提供了一套命令， 所以在Redis中使用Bitmaps和使用字符串的方法不太相同。可以把Bitmaps想象成一个以位为单位的数组， 数组的每个单元只能存储0和1， 数组的下标在Bitmaps中叫做偏移量。\nbitmaps通常作用在统计用户访问量，因为一个用户只有访问和未访问两种状态，为此存储用户的uid会极大的浪费redis的内存。那么以1表示访问，以0表示未访问，而bitmaps的下标索引则代表这个用户的uid，则可以缩减所需内存。\n假设现在有20个用户，userid=0， 5， 11， 15， 19的用户对网站进行了访问， 那么当前Bitmaps初始化结果如下图所示。\n那么设置id为5的bit为1则是 setbit testkey 4 1 设置索引为4的bit为1，取则是 getbit testkey 4，返回的0和1则是代表是否被点播，而4之前的值都会被0填充。而取不存在的位，如 getbit testkey 10 也是能取到的，它被填充为了0\n很多应用的用户id以一个指定数字（例如10000） 开头， 直接将用户id和Bitmaps的偏移量对应势必会造成一定的浪费， 通常的做法是每次做setbit操作时将用户id减去这个指定数字(之前的10000)。 在第一次初始化Bitmaps时， 假如偏移量非常大， 那么整个初始化过程执行会比较慢， 可能会造成Redis的阻塞\nbitpos key targetBit 返回第一个值为targetBit(0/1)的偏移量\nHyperLogLog (基数统计，专用于统计不重复的数据的数量) GEO (地理定位计算,只能计算水平位置的距离) 主从复制(读写分离) 只需要在从机使用slaveof 主机ip 主机port 就可以开始连接了，然后看下主机和从机是否有同步数据成功的日志即可。同步成功后从机只能进行get读的操作，写会失败，且每次主机set一次，从机就可以get到最新的数据\n也可以在启动从机时 在redis-cli后加 \u0026ndash;slaveof 127.0.0.1 6379\n但是最常用的还是修改配置文件的方法：\t在从机的redis.conf中加 slaveof 主机ip 主机port\nslaveof no one 从机发送命令，断开主从连接\n全量复制指的是从发psync2指令那一刻开始主机原来的所有数据，而部分数据则是进行rdb过程中所有的数据，后面发送的指令已经属于命令传播阶段了\n命令传播阶段会因为短时间的网络中断 开启 部分复制\n如slave重连master时，如果运行id和上次不同，就需要判断是否是同一台机器，如果不是就得进行全量复制了。运行id主要用于识别对方身份\n数据同步阶段：\n蓝色的runid和offset是全量复制时主发给从的 主机runid和offset，红色也是主机的runid和offset[当前的偏移量]。offset如果不在缓冲区就说明被挤出去了，就得重新全量复制\n命令传播阶段：\nmaster的复制缓冲区会暂存传播的命令和偏移量。如果从机发来的偏移量和主机记录的偏移量不同则可以知道主机更新了哪些数据[即使两者连接断开]，如果从机发来的偏移量不在主机的复制缓冲区中，则意味着数据过早，已经被挤出缓存区了，只能重新进行全量复制。\nlag项大于1就说明曾经slave掉过\n常见问题：\n哨兵 配置文件 启动顺序为先启动主节点，再从节点，然后再启动哨兵\n工作原理 sentinel会先向master和slave要信息，要完以后建立一个cmd连接。而sentinel之间会组件一个频道，在里面发布订阅信息，如当master记录了第一个sentinel的信息时，第二个sentinel再发info给master就会发现，它会加入那个频道并将自己的信息发布进去。这样新增哨兵就会快速同步给其他原有哨兵。\n当确认主机挂了以后，哨兵会进行选举来确定谁去选新的主机。而每个哨兵都是竞选者也是投票者。具体原理百度即可。\n当未来原master重新连接上了，它也依然是从机。\n集群 而每一个区域，36-41那部分则是一个个的存储槽，每个计算机中都记录了所有机器的槽的位置。当key访问一台机器没找到时，则会让key去对应槽的机器上寻找。\n然而槽的总数是不会发生变化的，每次新增一个节点就只是将当前所有机器中的槽拆出来部分给新的节点\n配置文件 线上的时候，节点服务响应超时时间一般设置为30-60s之间\n而作为集群启动后，redis-cli就需要加 -c 来专门作为集群方式操作，不加c操作key就必须去对应槽的机器中才能操作\n而如果运行过程中如果从机挂了，对应的主机会进行记录（一主对一从时），如果主机挂了，从机会作为新的主机继续运行。而原来的master再上线就只能作为从机运行了。\n监控指标 ","date":"2021-03-10T00:00:00Z","permalink":"https://xiaonuoz.github.io/p/redis%E5%85%A5%E9%97%A8/","title":"Redis入门"},{"content":"常用语句 show tables from databases; 查看某个库中的某个表\nselect database(); 查看当前所在库\ndesc tableName; 查看表的结构\nselect version(); 查看MySQL的版本\nSELECT last_name as “别名” from employees; 为字段起别名，as可以省略，空格区分即可。双引号也可省略\nSELECT DISTINCT id FROM users distinct\u0026ndash;\u0026gt; 对查询出的数据进行去重，当对多个字段去重时 DISTINCT a,b，它会将多个字段视为一个整体，当同时满足的时候就会去重。\nmysql中的加号只能作为运算符，无法拼接字符串，如果是字符型相加，会尝试将其转换成整型，转换失败则会让字符型转为数值 0，只要其中一方为null，其结果肯定为null\nSELECT CONCAT(last_name,\u0026quot; \u0026quot;,first_name) 姓名 FROM employees 拼接字段，并为拼接后的字段起别名为姓名\nIFNULL(expr1,expr2) 判断expr1的值是否为null，为null时把null改为expr2的值\n当有三个条件，可以通过（）来进行细分，如查询编号小于90或大于110或工资大于15000的\nselect * from employees where NOT(id\u0026gt;=90 AND id\u0026lt;=110) OR salary\u0026gt;15000\n模糊查询，like(像)、between and(在什么之间)、in、is null、is not null。like通常和通配符搭配使用，%表示任意多个字符，包含0个字符；_下划线表示任意单个字符；\n如果查询内容中有特殊字符，如下划线百分号等，可以只用反斜杠 \\ 进行转译,可以在语句最后用 ESCAPE定义转译字符\nbetween and表示区间，id between 100 and 120 取id在100到120之间,且包含100和120，临界值不要颠倒，前面是大于后面是小于，颠倒后语意会发生改变。也可以通过 NOT BETWEEN AND 来取反\nin用于判断某字段的值是否属于in列表的某一项，id in(10,20,30)取id是10或20或30的数据，不能使用通配符，因为通配符是和like一起使用，in等价于等号=\nis null 和 is not null用于判断字段值是否为NULL，因为=和\u0026lt;\u0026gt;不能判断null值\n排序语法：select * from table [wherer 条件] order by 排序列表 [asc|desc] 默认为升序asc，可省略。也可以对表达式进行排序且起别名，select *,salary*12*(1+IFNULL(commission,0)) AS 年薪 from table [wherer 条件] order by 年薪 [asc|desc]\nselect * from employees order by salary asc,employees_id desc 先根据工资升序排列，当遇见工资相同时，按id降序排列，order by一般放在查询语句最后面，limit语句除外\n语句的执行顺序一般是先执行form ，[有分组或者连接则先进行]，然后where筛选数据，然后才是select后要显示的字段，最后升序降序有定义则执行，因此如果有分组，select显示的是筛选之后需要显示的字段，并非原始表的字段\n函数 字符函数 upper 将字符串转为大写，lower 将字符串转为小写\nsubstr|substring(str,index,[长度]) 两者都可用，用于切割字符串，要注意的是索引是从1开始。加长度则是截取从指定索引处开始的指定字符长度的字符（即使是中文也依然只占一个字符）\ninstr(str,substr) 用于返回子串substr在str中第一次出现的起始索引，找不到则返回0\ntrim( [i FORM] str) 去掉字符串前后的空格或者指定字符串 i\nLPAD(str,len ,'*') 用指定的字符实现左填充指定长度，将str的头部用星号*填充到 len 个长度，长度包括str本身的字符长度（注意是字符不是字节，字符无论中英文都是只占一个字节），但是如果len\u0026lt;str，它会进行截断（从右边开始截断）, rpad(str,len,'*')则是右填充\nreplace(str,form_str,to_str)，替换，将str中的form_str替换成to_str，如果有多个相同的字符则会全部替换\n数学函数 round(X,[D]) 四舍五入，X为要计算的值，D为保留小数点后几位（以小数点X+1的位数进行四舍五入\nCEIL(X) 向上取整，返回\u0026gt;=该参数的最小整数（如1.00则取1，1.01则取2）\nFLOOR(X) 向下取整，返回\u0026lt;=该参数的最大整数（如1.00则取1，1.99也取1）\nTRUNCATE(X,D) 表示截断，小数点后保留几位，X为原始值，D为保留的位数\nMOD(m,n) 取余，a\u0026gt;=b,可以套用公式 a-a/b*b 进行计算，因为数据库中除法只取整，所以计算结果不一定为0\n时间函数 NOW() 返回当前系统日期+时间；CURDATE() 返回当前系统日期，不包含时间；CURTIME() 返回当前系统时间，不包含日期\n也可以获取指定的部分，年月日时分秒都可以单独获取，YEAR|MONTH|DAY...(时间格式)，在之后加上name,monthname 则可获取英文月份\nstr_to_date('9-3-2019','%m-%d-%Y') 将日期格式的字符转换成真正的日期型数据\ndate_format(date,'%m月/%d日 %Y年') 将日期格式date转换成字符，同时可以自定义字符的输出格式\ndatediff(date1,date2) 返回日期参数的相差天数，由date1减date2\n流程控制函数 select if(expr1,expr2,expr3) expr1表示条件表达式，为true时返回表达式2的值，为false返回表达式3的值\ncase语句即可以搭配select（只能返回值），也可以单独自己使用（返回值或语句）。当显示值的时候不要放分号，一放分号就表示结束了。else可以省略。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 只有then后是语句时才会加分号！！！ 用法1（类似于switch）： case 要判断的字段或表达式 when 常量1 then 要显示的值1或 [语句1;] when 常量2 then 要显示的值2或 [语句2;] ... else 要显示的默认值或语句; end 用法2（类似与多重判断，else if）,case后面不加变量: case when 条件1 then 要显示的值1或 [语句1;] when 条件2 then 要显示的值2或 [语句2;] ... else 要显示的值n或 [语句n;] end 如果是作为独立语句放在BEGIN END中时，then后面只能接语句，且end后面要加case;\n分组函数，将多个值通过计算后获取一个值 sum(列名) 统计该列的和；avg(列名) 统计该列的平均值；min(列名) 统计该列的最低值；max(列名) 统计该列的最大值；count(列名) 统计该列的不为空null的值的个数；\nsum和avg只支持数值型，min、max、count可以处理任何类型。分组函数都忽略了null值，含有null的字段数据是不会参加计算的。null加 任何值其结果都为null\n分组函数可以搭配distinct（去重）使用，sum(distinct salary) 它会先对数据进行去重，然后进行计算\ncount(*)表示统计所有行数，因为只要该行有一个字段不为NULL就会加上那一行，主键永远不会为空\n和分组函数一同查询的字段要求是group by后的字段，如果顺带查询一个普通字段，就对不上（一个对多个普通字段，无论显示什么都是没有意义的）\n分组查询 以基础字段进行分组 1 2 3 4 5 SELECT MAX(salary),job_id FROM `employees` [where =] GROUP BY job_id [order =] //以什么字段进行分组就要最后用group by进行注明以什么分组，分组的含义就是将该字段中相同的数据划分为一个组，通常用于以该字段进行划组来计算其他字段的数值。 //select后job_id字段可以省去，不加则查询出来的数据不会显示job_id一栏，它是进行过分组之后的每个组的job_id，重复的被合并为了一个。 //分组查询是不能使用*号取所有字段的，因为分组是将原始字段筛选，而显示其他字段时，不进行计算则无法知道该显示什么数据 //当描述为每个部门，每个班级等就可以用分组查询 1 2 3 4 SELECT COUNT(*) con,department_id FROM `employees` GROUP BY department_id HAVING con\u0026gt;2 //查询哪个部门员工个数大于2 //对查询出来的分组数据进行筛选时不能使用where，where作用于分组之前，想要对分组之后的数据进行筛选则需要使用Having //原始表就有的数据用where进行筛选，分组后的数据用having进行筛选 分组函数做分组查询的条件时，肯定是放在having之后（因为分组函数已经是对原始表分组后的数据进行计算过了，不属于原始数据了）。出于对性能的考虑，如果是筛选条件是分组的那个字段时，将它放在where中（也属于原始数据）\n一般如果没有分组的情况下，使用where即可。having只在分组情况下使用\n也可以按多个字段分组，当定义的字段全部都一致时才会分为一个组\nselect avg(salary),de_id,job_id from emp group by de_id,job_id\n以表达式或函数分组 1 SELECT COUNT(*) FROM `employees` GROUP BY LENGTH(last_name) HAVING COUNT(*)\u0026gt;5 多表查询 连接是对两表数据进行组装，如果A表的一条数据在B组中满足多个设定的where条件，那么它就会组装多个数据。每一个条件或连接都会形成一个虚拟表，然后一步步的进行筛选最后得出正确结果。无论内连还是外连都会先生成一个笛卡尔乘积的表，然后再通过条件过滤出符合的数据\n内连接（查询两表的交集） 1 2 3 4 SELECT 查询字段 FROM 表1 别名 INNER JOIN 表2 别名 ON 连接条件; //INNER可以省略 等值连接 select boyname,b.girlname form boys as b INNER JOIN girls as g ON b.id=g.boys_id 查询两个表中的数据，当两表某字段相等时显示对应行的值，一个表中有另一个没有则不会被筛选出来。最后结果肯定是 0\u0026lt;=数据\u0026lt;=两表行的乘积(m*n)，即其中的某个表的字段可能会重复显示。两表都有的字段加上前缀声明用哪个表，避免歧义 出现报错\n如果为表起了别名，则查询的字段就不能用原来的表名去限定了\n也可以对连接后的数据进行分组和筛选\nselect count(*),city from de INNER JOIN city ON de.id=city_id group by city 先对选择的表进行连接，然后分组，然后统计数量\n非等值连接 即连接的符号不是等于，select salary,grade_lv from em INNER JOIN job_grade ON salary between g.'min_salary' AND g.'max_salary' ，它表示查询工资和工资等级表，并且筛选出工资在某个区间的数据，比如工资等级表有ABCD四个等级，salary表的每一条数据都会和工资等级表中的每个数据进行判断，当符合条件则将两者合成一条临时数据，然后再进行下一个字段的判断。最后出现的数据就是 每个员工的工资都会分到一个专属的等级，如2000分到D级（1000-2000）\n自连接 对一张表进行连接筛选数据，比如员工表所对应的领导id也在员工表中，则需要自连接\n1 2 3 select e.last_name 员工名,m.last_name 领导名 from employees e INNER JOIN employees m ON e.employee_id=m.manager_id 它将两张相同的表合在一起，然后进行判断，如果判断成功则合为一条，再进行判断，最后只取出员工名和领导名\n这三种连接也可以适用于超过两个表的情况\n1 2 3 select last_name,department_name,city from employees e INNER JOIN departments d ON e.department_id=d.department_id INNER JOIN locations l on d.location_id=l.location_id 表之间必须要有关联，有主外键关系。且不能把两个关联条件用AND放一起，虽然不报错，但是是有问题的。\n三表及以上的连接需要考虑顺序，因为是表1和表2连成一个大表，然后on条件之后，连成的大表再和表3进行连接。且如果表1和表3没主外键关系就连接不起来，只有表1和表2，然后表2和表3\n外连接（查询一个表有，另一个表没有的数据） 外连接是区分主从表的，连接之后两者根据条件匹配，匹配成功时数据会显示出来。当匹配不到时，其值则为NULL。\n所以外连接查询的结果为主表中的所有记录，结果分两部分，一部分从表有数据（类似于内连接），另一部分从表数据为NULL\n**当为左外连接时，left join左边的是主表；当为右外连接时，right join右边的是主表。**即左外交换顺序则可达到右外同样的效果。要查询的信息主要来自哪个表，哪个表就当主表。\n1 2 3 select b.name from beauty b left join boys bo on b.boyfriend_id=bo.id where bo.id IS NULL 对查询结果进行判断是否为NULL来取出不是交集的值时，最好是对从键的主键进行判断，因为其他值可能正常情况下也会为NULL，只有主键永远不会为空，为空就是外连接没匹配到的那部分数据\nmysql不支持全外连接，全外连接就是内连接+表1有但是表2没有的+表2有但是表1没有的 数据\n交叉连接就是笛卡尔乘积现象，表1的数据乘以表2的数据，关键字是CROSS JOIN ，没顺序之分\n子查询 出现在其他语句中（不仅仅是select语句，其他语句也可以使用子查询）的select语句，称为子查询或内查询；内部嵌套其他select语句的查询，称为外查询或主查询\n标量子查询 标量子查询一般搭配着单行操作符使用，\u0026gt; \u0026lt; \u0026gt;= \u0026lt;= = \u0026lt;\u0026gt; ；\n1 2 3 SELECT * FROM `employees` WHERE salary\u0026gt; (SELECT salary FROM employees WHERE last_name=\u0026#39;Abel\u0026#39;) 也可以搭配分组使用\n1 2 3 4 5 SELECT MIN(salary) minsalary FROM employees GROUP BY department_id HAVING minsalary\u0026gt; (SELECT MIN(salary) FROM employees where department_id=50) //在标量子查询中（有大于等于判断），子查询的结果必须只有一行。不可以查出多行，会产生报错。因为大于等于之类的判断无法同时判断多个值（需要in） 子查询的执行是优先于主查询执行的，因为主查询的条件用到了子查询的结果\n多列子查询 列子查询一般搭配着多行操作符使用，in 、any/some 、 all。\n**in /not in 等于查询的条件是子查询结果列表中的任意一个；**ANY/SOME 是和子查询返回的任意某一个值比较（如果是小于任意一个，则是小于最大的那个就行，用max()更合适，可读性更高）；ALL是和子查询返回的所有值进行比较（小于所有值则是小于最小值）。\n行子查询 结果集一行多列或多行多列\n1 2 3 4 SELECT * FROM employees WHERE (employees_id,salary)=( SELECT MIN(employees_id),MAX(salary) FROM employees ) 放select后面的子查询 1 2 3 4 5 6 查询每个部门中的人数 select d.*, ( select count(*) from employees e WHERE d.department_id=e.department_id )\t个数 FROM departments d 这种方法仅仅支持标量子查询，即子查询返回的值只能是一行一列\n放在from后面的子查询 将子查询充当成一张表。子查询必须起别名，不然找不到。也可以搭配内外连接，这相当于一个新的表，里面的字段就是查询出来的字段\n1 2 3 4 5 //查询每个部门的平均工资的工资等级 SELECT ag_dep.*,job_grades.grade_level FROM (select AVG(salary) ag ,department_id from employees GROUP BY department_id) ag_dep INNER JOIN job_grades ON ag_dep.ag BETWEEN lowest_sal AND highest_sal 放在exists后面（相关子查询） exists(完整的查询语句) 用于判断后面的子查询是否有值，返回一个布尔类型，有就是 1。无论结果有多少，它只会返回一个0或者1 [单行单列]，用exists可以实现的用in也能实现\n1 2 3 4 //查询有员工的部门名 select department_name FROM departments d where EXISTS( select * from employees e where d.department_id=e.department_id ) 查询都是一条一条执行，一条一条显示的，因此当主表查出一条数据，然后拿主表的某个字段和从表进行判断。判断失败则会不显示，放select from exists where后面都是如此，甚至其他查询也是如此。\n分页查询 1 2 select 查询列表 from 表 limit offset,size; limit是偏移量，排除从头开始共limit条数据，显示显示size条数据. limit语句放在查询语句的最后\n联合查询 当查询的结果来自多个表，且表之间没有直接连接关系，但查询的结果列数一致时,可以用联合查询将多个独立的查询的结果合并成一个结果（并集）\n1 2 3 4 5 select id,name from employees wherer email like \u0026#39;%a%\u0026#39; UNION select uid,uname from user wherer uid\u0026gt;90 UNION .... 最后表名为最开始的查询字段名，即id,name；uid和uname不会进行显示。\n注意：\n1.要求多条查询语句的查询列数要一致\n2.要求多条查询语句的查询的每一列的类型和顺序最好一致（id对uid，name对uname）\n3.UNION关键字会默认去重（即表之间查询出的语句有相同的行内容），如果不想去重则需要使用 UNION ALL来包含重复项\n新增数据 1 2 3 4 5 insert into 表名（列名） values(值...),(值....) 列和值要一一对应，省略列名时值要和表中列一一对应。此方法支持插入多行， 此方法也支持子查询插入(将查询出来的结果集插入到表中，不需要加values关键字) insert into 表名（列名） select id,name,age from user 1 2 3 4 也可以通过 insert into 表名 set id=1,name=\u0026#34;xx\u0026#34; 来实现新增时只给指定值进行赋值，其他值为默认值，不支持插入多行和子查询 修改数据 1 2 update 表名 set 列=新值,列=新值... where 条件 一定要加条件，否则是将这个表中所有数据对应的列数据进行修改 update也可以进行连接来修改多个表的记录（了解即可）\n1 2 3 4 5 6 7 8 9 update 表名 别名 inner|left|right join 表2 别名 on 条件 set 表1.列名=值，表2.列名=值 where 表1.列名=条件 and 表2.列名=条件 update user u inner join admin a on u.id=a.uid set u.name=\u0026#39;admin\u0026#39;,a.type=\u0026#39;success\u0026#39; where u.id=1 这样就可以实现对user表中id为1的name列和admin表中uid为1的type列进行修改 删除数据 删除单行数据\n1 2 delete from 表名 where 条件 不加条件则把表中数据全部删除 删除多行数据（了解即可）\n1 2 3 delete 要删除的表的别名 from 表1 别名 join 表2 别名 on 连接条件 where 筛选条件 TRUNCATE TABLE 表名 清空数据，将表中数据全部删除，这个删除不允许加where，只能删除全部。\nDELETE删除后自增长列的值从断点开始，而TRUNCATE自增长列重新从1开始\n当涉及事务时，DELETE删除可以回滚，而TRUNCATE不能回滚，所以尽量少使用TRUNCATE\nDDL对库和表的操作 CREATE DATABASE [IF NOT EXISTS] 库名 ，IF NOT EXISTS 表示如果库不存在则创建库，存在则警告，不会报错。IF EXISTS则相反意义\nALTER DATABASE 库名 SET CHARACTER SET gbk 更改库的编码\nDROP DATABASE 库名 删除库\n表的操作 DESC 表名 查看表结构\n创建表\n1 2 3 4 5 create table 表名( 列名 列的类型[(长度) 约束], 列名 列的类型[(长度) 约束], 列名 列的类型[(长度) 约束] ) 修改表\n1 2 3 4 5 6 7 8 9 ALTER TABLE 表名 CHANGE COLUMN 旧列名 新列名 新列名的类型 //修改列名，COLUMN可省略，其他操作不能省略，建议都加上 ALTER TABLE 表名 MODIFY COLUMN 列名 新类型 新约束 //修改列的类型或约束,不加约束则是将原有约束全删除 ALTER TABLE 表名 ADD COLUMN 列名 列类型 [first|after 字段名] //添加新的列（可以指定在第一行或某行之后，默认是添加到最后） ALTER TABLE 表名 DROP COLUMN 列名 //删除列 ALTER TABLE 表名 RENAME TO 新表名 //修改表名 DROP TABLE [IF EXISTS] 表名 删除表,IF EXITS仅仅适用于库和表\n复制表\n1 2 3 4 5 CREATE TABLE copy_table LIKE table //仅仅复制表的结构 CREATE TABLE copy SELECT * FROM table //复制table表的结构+数据 CREATE TABLE copy SELECT id,name FROM table where nation=\u0026#39;中国\u0026#39; //复制部分结构和部分数据，可以设置where 1=2来复制部分结构不复制数据 数据库数据类型 浮点型如果小数位超出，则会四舍五入显示最大长度的值，如类型 (2,1) 的1.26显示为1.3。而定点型则是直接截断\n字符型：\nenum类型：\n枚举类型，要求插入的值必须属于列表中的指定的值。忽略大小写。c1 ENUM('a','b','c')\nset类型：\n集合类型，和ENUM类型类似，最大的区别是集合可以对于一个插入选取多个成员，而ENUM只能选一个。如 s1 SET('a','b','c')，它可以插入VALUES(\u0026lsquo;a,b\u0026rsquo;)\n对于TIMESTAMP，它把客户端插入的时间从当前时区转化为UTC（世界标准时间）进行存储。查询时，将其又转化为客户端当前时区进行返回。\n而对于DATETIME，不做任何改变，基本上是原样输入和输出。\nTIMESTAMP和DATETIME除了存储范围和存储方式不一样，没有太大区别。当然，对于跨时区的业务，TIMESTAMP更为合适。\n约束 UNIQUE：唯一可为空，用于保证该字段的值具有唯一性，但可以为空（只能有一个值为null,因为两个null就重复了，违反了唯一的性质【版本不同可以会可以存在多个NULL】），primary key是唯一且不能为空\n主键有且只有一个，唯一约束可以有多个\nFOREIGN KEY：外键约束 ，用于限制两表的关系，保证该字段的值必须来自于主表的关联列的值。在从表添加外键约束，用于引用主表中某列的值。在创建表时使用外键约束是无效的，只有在修改时有用\nCHECK检查约束在mysql中无效\nSHOW INDEX FROM 表名 查看stuinfo表中的所有索引，包括主键、外键、唯一和自己创建的索引\n列级约束在字段后添加即可，表级约束则是 [CONSTRAINT 约束名] 约束类型(字段名) , 外键约束有点特殊，是 [CONSTRAINT 约束名] FOREIGN KEY(此表中的字段名) REFERENCES 主表(主表的主键)，外键表名通常是fk_当前表名_主表名。CONSTRAINT是别名的关键字，但是对主键起别名在mysql中是没有效果的\n外键的主表关联列必须是一个key(即必须是一个主键或唯一键【数据唯一性】)，插入数据时先插入主表再插入从表，删除数据时则先删除从表数据再删除主表数据。\n如果想只删除主表而不动从表，则需要使用级联删除或者级联置空。\n级联删除只需要在创建或修改约束时在后面加 ON DELETE CASCADE,设置级联删除后，当删除主表的数据时会同时删除从表中外键值为主表key字段值的数据\n级联置空则是在创建或修改约束时在后面加 ON DELETE SET NULL,意思是当主表数据被删除时，从表中所对应的外键值变为NULL\n1 2 3 4 ALTER TABLE 表名 ADD 约束(字段名) //为某字段添加表级约束，列级约束只能使用MODIFY,表级约束两种方法都可以使用 ALTER TABLE 表名 MODIFY COLUMN 列名 原字段类型 新约束 //修改列的约束,不加约束则是将原有约束全删除 ALTER TABLE 表名 DROP 约束类型 约束的字段名 //删除约束，仅限于表级约束。列级约束使用MODIFY即可 列级约束语法都支持，但外键约束没有效果；表级约束默认约束(DEFAULT)和非空约束不支持，其他都支持。\nAUTO_INCREMENT:自增长列约束，默认从1开始；如果想更改起始值，可以通过插入一行数据设置自增长列为你想要的起始值，如设置为10，后插入的则都会从10开始自增1。一个表最多只能有一个自增长列，且标识列的类型只能是数值型（int、float等）\n事务 一个或一组sql语句组成一个执行单元，这个执行单元要么全部执行，要么全部不执行\n事务由一个或多个SQL语句组成一个单元，在这个单元中，每个语句是相互依赖的。整个单元作为一个不可分割的整体，如果单元中的某条SQL语句执行失败或发生错误，整个单元将会进行回滚。所有受到影响的数据将返回到事务开始以前的状态；如果单元中的所有SQL语句均执行成功，则事务被顺利执行。\n**而在mysql中使用最多的存储引擎是innodb,myisam,memory。其中只有innodb支持事务。**通过 SHOW ENGINES 可以查看当前数据库支持的存储引擎\n在mysql中想开启事务必须先设置自动提交功能为禁用，set autocommit=0;，这个值是控制隐式事务的自动提交(insert、update、delete都属于隐式事务)。这个命令只针对当前会话有效。关闭后又会恢复开启状态\n创建事务后，更改的数据会先暂存在内存中，直到提交或回滚才会确定是否落盘；\n脏读发生的情况就是读到了另一个事务在内存中的数据。不可重复读则是再次读取时读到了另一个事务提交后的字段（两次查询结果不同）。幻读则是一个事务提交后，另一个事务进行更新列操作会包含前一个事务（如更新多行符合条件的列会把另一个事务新增的符合条件的列也进行更新）；通过更改事务级别避免这些问题。\nmysql的默认级别为REPEATABLE READ 可重复读。可以避免脏读和不可重复读，幻读无法避免\nselect @@tx_isolation; 查看隔离级别；设置事务级别可以设置为只针对当前会话有效[set后加session]，设置全局[加global]，详细命令百度即可。\ndelete删除的数据可以通过回滚，但是truncated删除的数据是无法回滚的，一定要注意\n视图 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 创建视图： create view 视图名 AS 复杂查询语句; 使用 select * from 视图名 where 条件语句 修改： create or replace view 视图名 AS 查询语句; 如果视图存在则修改，不存在则创建 alter view 视图名 AS 查询语句; 修改视图 查看视图: DESC 视图名; SHOW CREATE VIEW 视图名; 查看视图的详细信息 删除视图: DROP VIEW 视图1,视图2,...; 视图就类似于字符串拼接，将一个复杂的查询语句包装成一个视图进行重复使用，后期使用时再加各种条件筛选出数据。\n视图可以实现\n重用sql语句 简化复杂的sql操作，不必知道它的查询细节 保护数据，提高安全性（对原始数据进行封装，与原始表相分离，查询视图返回的就只有创建视图时提供的字段，而不是所有字段，避免被人拖库等安全性问题） 视图也可以进行增删改查，语句和操作表相同，当用sql语句对视图进行更改时，会转换成对原表操作的sql语句。但如果mysql不能正确的确定被更新的基数据，则不允许更新(即视图中的某个字段在原表中并不存在，则不能进行更新)\n视图的可更新性和视图中查询的定义有关系，以下类型的视图是不能更新的\n包含某些关键字的sql语句: 分组函数、distinct、group by、having、union、union all 常量视图 select中包含子查询 join 创建一个视图，其中包含查询一个不能更新的视图 where的子查询引用了from关键字 只要是对原表的数据进行过处理的视图都是不可更新的\n变量 变量分为系统变量和自定义变量；系统变量中有全局变量和会话变量；全局变量更改后针对所有会话有效，但是重启mysql后就会失效，会恢复为配置文件中的默认值\n系统变量 1 2 3 4 5 SHOW GLOBAL|[SESSION] VARIABLES [LIKE \u0026#39;%ab%\u0026#39;]; 查看系统 全局/会话变量,session不写默认也是查看会话变量，可以加like查看某些变量 SELECT @@[global.|session]系统变量名 查看指定的系统变量的值，加global则是查看全局的，不加或加session则为会话的 SET @@[global.|session]系统变量名 = 值\t设置变量值，加不加global意义与上面一致 自定义变量 用户自定义的用户变量，只针对当前会话有效\n1 2 声明并初始化(也可以为原有值进行赋值)：\tSET @用户变量名=值 查看自定义变量：SELECT @用户变量名 局部变量仅仅在定义它的begin end中有效（类似于代码块），且必须放在begin end中的最前面。\n1 2 3 声明： DECLARE 变量名 数据类型 [DEFAULT 值]; //局部变量必须声明且赋值，用default可以在声明时赋值 赋值： SET 变量名=值\t//注意局部变量没有@ 使用： SELECT 变量名 存储过程 存储过程和函数都相当于golang中的函数，用于封装一组sql语句集合。它可以提高代码的重用性，简化操作\n并且第一次使用后会进行编译，而编译过后执行速度会比普通的sql语句快。且他封装后只需要连接一次数据库，平常却是有多少sql语句连接多少次；这些优点提高了存储过程的效率\n创建存储过程 1 2 3 4 5 DELIMITER $ CREATE PROCEDURE 存储过程名(参数列表) BEGIN 存储过程体(一组合法的SQL语句); END $ 要注意的是：\n参数列表包含三部分，参数模式 参数名 参数类型，如 IN stuName varchar(20)\n参数模式有IN、OUT、INOUT，IN表示该参数可以作为输入，也就是该参数需要调用方传入值；OUT则是该参数可以作为返回值[sql中没有return]，INOUT就是该参数需要传入值，同时也可以返回值；参数之间用逗号隔开\n如果存储过程体仅仅只有一句话，BEGIN EDN可以省略\n存储过程体中的每条SQL语句的结尾要求必须加分号，但是分号又会和普通语句的分号冲突，因此需要使用 DELIMITER 结束标记 来给存储过程体的结尾分号起别名\n调用存储过程 CALL 存储过程名(实参列表)创建时定义的结束标记 ,例如 CALL myp1()$\n案例：创建存储过程，实现用户是否登录成功\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 //创建 DELIMITER $; CREATE PROCEDURE login(IN username VARCHAR(20),IN password VARCHAR(20)) BEGIN DECLARE result INT DEFAULT 0; SELECT count(*) INTO result FROM admin WHERE admin.username=username AND admin.password=password; SELECT IF(result\u0026gt;0,\u0026#39;成功\u0026#39;,\u0026#39;失败\u0026#39;); END $ //调用 CALL login(\u0026#39;admin\u0026#39;,\u0026#39;123456\u0026#39;)$ 删除存储过程 drop procedure 存储过程名 ，一次只能删除一个\n查看存储过程则是 SHOW CREATE PROCEDURE 存储过程名;\n函数 它和存储过程很像，区别是存储过程有0个或多个返回值，而函数有且只能有1个返回值\n调用语法：SELECT 函数名(参数列表) ,执行函数中的所有语句，并且显示返回值\n流程控制 if语句(case在之前的流程控制有) 1 2 3 4 5 6 7 功能：实现多重分支，只能应用在BEGIN END中 IF 条件1 THEN 语句1; ELSEIF 条件2 THEN 语句2; ... [ELSE 语句n;] END IF 循环 索引 索引分为单值索引和复合索引；索引原理就是排序哈希表，就是排好序的快速查找数据结构。加速where和order by的查询\n1 create index 索引名 on 表名(字段名,字段名...) //为某字段建立索引，索引名通常取名为idx_表名_字段名;有多个字段时采用驼峰命名 一般来说索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储在磁盘上。经常性的修改和物理删除数据会导致数据不连贯，索引出现不准确的问题，因此建议使用软删除(delete_at)。且变化频繁的数据不太适合建索引，它会导致频繁更新索引\n虽然索引大大提高了查询速度，同时却会降低更新表的速度，如对表INSERT、UPDATE和DELETE。因为更新表时，MySQL不仅要保存数据，还要保存索引文件每次更新添加的索引列字段，都会调整因为更新所带来的键值变化后的索引信息\n一张表最好不要建立超过五个索引，且通常复合索引用的多。\n在创建表时，主键已经默认成为了一个唯一索引\n哪些情况需要创建索引 哪种情况不需要创建索引 对差异率和重复率不高的数据(重复且平均分配的值)进行创建索引没有太多意义。索引的选择性是指索引列中不同值的数目与表中记录数的比。比如有2000条数据，1980个不同的值，它选择性就是1980/2000=0.99。一个索引的选择性越接近于1，这个索引的效率就越高。\nEXPLAIN(查询执行计划) 使用explain关键字可以模拟优化器执行SQL查询语句，从而知道Mysql是如何处理SQL语句的。分析查询语句或表结构的性能瓶颈。语句就是explain+SQL语句\n它可以获取到：\n表的读取顺序 数据读取操作的操作类型 哪些索引可以使用 哪些索引被实际使用 表之间的引用 每张表有多少行被优化器查询 id 它是select查询的序列号，包含一组数字，表示查询中执行select语句或操作表的顺序\n它有三种情况，id相同，id不同，id相同和不同都存在的情况\nselect_type table 表示这一行的数据是关于哪张表的\ntype 访问类型排列\n从最好到最差依次是：system \u0026gt; const \u0026gt; eq_ref \u0026gt; ref \u0026gt; range \u0026gt; index \u0026gt; ALL，一般来说，当数据量很大时，得保证查询至少达到range级别，最好能达到ref级别。\npossible_keys 显示可能应用在这张表中的索引，一个或多个。\n查询涉及到的字段上若存在索引，则该索引将被列出，但不一定被查询实际应用\nkey 实际使用的索引。如果为NULL，则没有使用索引\n查询中若使用了覆盖索引(查询的字段和创建的索引字段相吻合)，则该索引仅出现在key列表中。即理论上不需要使用索引，但实际运行时发现有复合索引和查询的字段相吻合，则实际使用了这个索引\nkey_len 表示索引中使用的字节数，可通过该列计算查询中使用的索引的长度。在不损失精确性的情况下，长度越短越好。也变相的表示为条件字段的个数，如where后的条件字段类型是char(4),那它的长度则是utf-8 3字节*4 定长 +一个允许为NULL的字节，即为13\nkey_len显示的值为索引字段的最大可能长度，并非实际使用长度，即key_len是根据表定义计算而得，不是通过表内检索出的。\n即同样的查询结果下，len越小越好\nref 显示索引的哪一列被使用了，如果可能的话，最好是一个常数。说明哪些列或常量被用于查找索引列上的值(where后的判断条件值的类型，如t1=\u0026lsquo;ac\u0026rsquo;，那ref显示的就是const,因为\u0026rsquo;ac\u0026rsquo;是一个常量)。\nrows 根据表统计信息及索引选用情况，大致估算出找到所需的记录所需要读取的行数。即读了原表多少条数据查到目标数据(值越小越好)，不建索引时读取肯定会读全表，建立之后通过索引查询，这个rows就会减少许多\nExtra 包含不适合在其他列中显示但十分重要的额外信息\nUsing filesort:文件排序，说明MySQL会对数据使用一个外部的索引排序，而不是按照表内的索引顺序进行读取。Mysql中无法利用索引完成的排序称为 文件排序。如本身的索引是b1_b2_b3，select b1字段使用部分索引，而order by b3，跳过了b2，就会出现order无法使用索引，只能自己根据表进行排序，影响了性能\nUsing temporary:使用了临时表保存中间结果，MySQL在对查询结果排序时使用了临时表。常见于排序order by和分组查询group by。\nUsing index:表示相应的select 操作使用了覆盖索引（select 后的数据列只用从索引中就能够取得[部分满足也可以]，不必读取数据行，查询列被所建的索引覆盖），避免了访问表的数据行，提高了效率；如果同时出现了using where，表明索引被用来执行索引键值查找；如果没出现using where，表明索引用来读取数据而非执行查找动作。\nUsing where:表明使用了where过滤\nUsing join buffer:使用了连接缓存\nImpossible where:where子句的值总是false ，不能用来获取任何元组\n要注意的是如果在where后索引字段使用 in 或者 \u0026gt; \u0026lt;等范围会导致索引失效，如果要解决这个就尽量用等号，功能无法用等号实现，就绕过这个范围判断的字段来创建索引\n*尽量减少使用select ，这样只会使用到全表扫描，无法使用索引，降低性能\n当两表链接时，从表一定要加索引[根据条件创建索引]，因为关联起来时主表肯定数据全部都需要，而从表是根据条件[字段和主表某字段相等]来获取交集或者差集，因此给从表加索引可以加速从表的查询速度，最大化的优化查询\n永远是小表驱动大表，如书籍类目表和书籍表，以类目表为主表。因为类目表数据更少，主表一般都是所有数据都会保存，如果把书籍表当成主表，数据量更大，io操作更多，性能就无法提升上来。\n索引失效的原因 最佳左前缀法则：如果创建了复合索引，要遵守最佳左前缀法则。指的是where查询从索引的最左前列开始并且不跳过索引中的列。因为一跳过或者只取之后字段就会和复合索引不匹配，部分使用如果只使用第一个之后的字段也会导致索引失效。[带头大哥不能死，中间兄弟不能断] 在索引列上做任何操作（计算、函数、[自动或手动的类型转换]）都会导致索引失效而转向全表扫描 存储引擎不能使用索引中范围条件后右边的列**[范围之后全失效]。**即条件判断不是常量而是范围(如age\u0026gt;15)就会导致范围字段之后的索引失效(范围判断本身的索引字段age不会失效) 尽量使用覆盖索引(只访问索引的查询[索引列和查询列一致])，减少select * MySQL在使用不等于(!= or \u0026lt;\u0026gt;)时会导致无法使用索引，转而变成全表扫描 is null 和 is not null 也会导致索引失效 like以通配符开头(\u0026rsquo;%abc\u0026hellip;\u0026rsquo;)时，MySQL的索引会失效，从而变成全表扫描ALL **[百分like加右边]，如果非要开头加通配符的话，select之后可以通过使用覆盖索引或者部分覆盖来解决索引失效的问题。**k%kk%不会导致失效，从另一个角度来看它都在右边 字符串不加单引号会导致索引失效(发生了自动的类型转换，符合第2点) 少用or，用它来连接时会导致索引失效 where条件后如果顺序不对，但字段对的上索引时，MySQL的优化器会自动排序。但是最好一致，避免MySQL再次翻译转换\n查询优化 1.永远小表驱动大表 即小的数据集驱动大的数据集。如书籍类目表和书籍表，以类目表为主表。因为类目表数据更少，主表一般都是所有数据都会保存，如果把书籍表当成主表，数据量更大，io操作更多，性能就无法提升上来。\n原理是exists会先查询外表数据，然后将其放到子查询中做条件验证 [遍历]，根据验证结果是TRUE还是FALSE还决定主查询的数据结果是否得以保留。因此B表多于A表数据时使用exists。\n而in是先查询子查询的数据，再循环多次查询外表数据，因此B小于A时使用in\norder by 关键字优化 order by 尽量使用index方式排序，避免出现using filesort 问题，且不要使用select * 尽可能在索引列上完成排序操作，遵照索引的最佳左前缀原则。不用索引排序则会出现using filesort问题 如果真的有using filesort问题出现，且无法进行SQL语句优化，修改配置文件，尝试提高 sort_buffer_size或 max_length_for_sort_data 索引用于查找和排序，所以如果where后有order时，它也会使用索引，如果条件和order是顺序的，如创建了索引c1_c2_c3，条件是 where c1=x and c2=y order by c3 ，虽然explain显示只有两个索引被使用，但c3也被使用到了，只是用在排序上没显示而已。c1=x and c2\u0026gt;y order by c2,c3也可以使用索引，虽然c2断了，但是后续order by的c2接上了。\n但如果 c1=x order by c3则会出现 using filesort 问题，因为索引断了；且 c1=x order by c3,c2也会出现相同的问题，因为order by调换位置显示的结果是不同的，无法使用优化器自动调换。而 c1=x order by c2,c1 不会出现这个问题，因为c1通过条件判断已经是个常量，优化器直接省略了它。order by c1 asc,c2 desc 由于两个排序方式不同也会导致using filesort问题。\n因此order by 语句最好使用索引最左前列原则，或者where子句与order by 子句条件列组合起来满足索引最左前列也可以。\n出现using filesort的情况分别有：\nASC DESC不一致 丢失第一个索引字段 丢失中间索引字段 order by中出现了不是索引字段中的字段 第一个字段在where子句那里并且使用了大于小于等或者in，且order by排序没有重新从第一个索引字段开始 group by也同理，分组之前必排序，且无法解决using filesort时也需要更改配置文件。where高于having，能写在where限定的条件就不要去having限定了。\nmysql有两种排序方式，文件排序(using filesort)或扫描有序索引排列(using index) mysql能为排序和查询使用相同的索引，即如果排序和查询都用上索引则是最优解 慢查询日志 show variables like '%slow_query_log'; 查看慢查询日志是否开启。set global slow_query_log=1;开启\nshow global variables like 'long_query_time%';查看运行时间，只有在大于这个值时才会记录。set global long_query_time=3 设置运行时间阈值，设置完后马上生效，但是要重启会话才能查到新值\nmysqldumpslow 日志分析工具 show profile 它是MySQL提供可以用来分析当前会话中语句执行的资源消耗情况和生命周期各部分消耗情况。可以用于SQL的调优的测量。\nshow variables like 'profiling' 查看是否开启\nset profiling=on; 开启此功能\nshow profiles; 查看最近执行的sql， show profile cpu,block io for query Query_id 根据查出来的Query_id查看某条sql的完整生命周期每一步耗费的时间和资源\n常见情况：只要有其中一个就必须得优化\n排查流程： 收到故障消息 开启慢查询日志，抓出执行慢的SQL EXPLAIN分析 分析没找到问题，进一步分析，使用show profile 全局查询日志 功能类似于show profile，建议使用show profile\n数据库的锁 表锁 偏向MyISAM存储引擎，开销小，加锁快；无死锁；锁定粒度大，发生锁冲突的概率高，并发度最低\n创建表时在创建结尾添加engine=myisam; 即使用这个引擎。\nshow open tables; 查看数据库有哪些表锁\n因此表锁有必要设置为读锁，而写锁尽量少使用\n读锁 在一个会话中对某个表加读锁后 lock table mytabs read\n当前会话可以读这个表但不能读其他表，无法写，即只有解锁后才能进行其他操作。 其他会话可以读这个表和其他表，写会阻塞，直到会话解锁。 写锁 在一个会话中对某个表加读锁后 lock table mytabs write\n当前会话可以读这个表和写这个表，但不能读写其他表。即只有解锁后才能进行其他表的操作。\n其他会话可以读其他表，读和写这个表都会阻塞，直到会话解锁。\n简而言之，针对其他对话，读锁会阻塞写，但不会阻塞读。而写锁会把读和写都阻塞。\n而加了锁的那个会话，为读锁时只能读当前表，不能写当前表和读写其他表;为写锁时能读和写当前表，不能读写其他表。都只有解锁后才能操作其他表。\n行锁 偏向InnoDB存储引擎，开销大，加锁慢；会出现死锁；锁定粒度小，发生锁冲突的概率低，并发度最高\n创建表时在创建结尾添加engine=innodb; 即使用这个引擎。可以不用，因为默认就是innodb引擎\ninnodb会对使用索引的列添加行锁，即sql语句带索引的时默认为行锁。如果没索引或者 索引失效[严重] 则会变成表锁\n行锁时，当前表是处在事务中的情况。其他表可以读但不能写同一行数据(读的以前数据，避免脏读)，会阻塞直到事务commit才能写\n分析用show profile即可看见lock的情况\n优化建议：\n尽可能让所有数据检索都通过索引来完成，避免无索引或索引失效让行锁升级为表锁。 合理设计索引，尽量缩小锁的范围 尽可能较少检索条件，避免间隙锁 尽量控制事务大小，减少锁定资源量和时间长度 间隙锁 因此很多框架删除功能在底层也是update delete_at(软删除)，原因就是避免间隙锁的发生\n","date":"2021-02-20T00:00:00Z","permalink":"https://xiaonuoz.github.io/p/mysql%E5%9F%BA%E7%A1%80/","title":"MySQL基础"},{"content":"来源于知乎: kaipython\n使用token进行用户认证，用中间件来保护路由，它可以通过上下文获取用户id,gin.GetHander(\u0026quot;user\u0026quot;)，然后再用id去数据库中查询是否有该用户，token保存在http请求的头部，注意方法是 GetHander。\n参考视频：https://www.bilibili.com/video/BV1CE411H7bQ?p=5\n1. JSON Web Token是什么 JSON Web Token (JWT)是一个开放标准(RFC 7519)，它定义了一种紧凑的、自包含的方式，用于作为JSON对象在各方之间安全地传输信息。该信息可以被验证和信任，因为它是数字签名的。\n2. 什么时候你应该用JSON Web Token 下列场景中使用JSON Web Token是很有用的：\nAuthorization (授权) : 这是使用JWT的最常见场景。一旦用户登录，后续每个请求都将包含JWT，允许用户访问该令牌允许的路由、服务和资源。单点登录是现在广泛使用的JWT的一个特性，因为它的开销很小，并且可以轻松地跨域使用。 Information Exchange (信息交换) : 对于安全的在各方之间传输信息而言，JSON Web Tokens无疑是一种很好的方式。因为JWT可以被签名，例如，用公钥/私钥对，你可以确定发送人就是它们所说的那个人。另外，由于签名是使用头和有效负载计算的，您还可以验证内容没有被篡改。 3. JSON Web Token的结构是什么样的 JSON Web Token由三部分组成，它们之间用圆点(.)连接。这三部分分别是：\nHeader Payload Signature 因此，一个典型的JWT看起来是这个样子的：\nxxxxx.yyyyy.zzzzz\n接下来，具体看一下每一部分：\nHeader header典型的由两部分组成：token的类型（“JWT”）和算法名称（比如：HMAC SHA256或者RSA等等）。 例如：\n1 2 3 4 { \u0026#39;alg\u0026#39;: \u0026#34;HS256\u0026#34;, \u0026#39;typ\u0026#39;: \u0026#34;JWT\u0026#34; } 然后，用Base64对这个JSON编码就得到JWT的第一部分\nPayload JWT的第二部分是payload，它包含声明（要求）。声明是关于实体(通常是用户)和其他数据的声明。声明有三种类型: registered, public 和 private。 Registered claims : 这里有一组预定义的声明，它们不是强制的，但是推荐。比如：iss (issuer), exp (expiration time), sub (subject), aud (audience)等。 Public claims : 可以随意定义。 Private claims : 用于在同意使用它们的各方之间共享信息，并且不是注册的或公开的声明。 下面是一个例子： 1 2 3 4 5 { \u0026#34;sub\u0026#34;: \u0026#39;1234567890\u0026#39;, \u0026#34;name\u0026#34;: \u0026#39;john\u0026#39;, \u0026#34;admin\u0026#34;:true } 对payload进行Base64编码就得到JWT的第二部分\n注意，不要在JWT的payload或header中放置敏感信息，除非它们是加密的。\nSignature 为了得到签名部分，你必须有编码过的header、编码过的payload、一个秘钥，签名算法是header中指定的那个，然对它们签名即可。\n例如：\nHMACSHA256(base64UrlEncode(header) + \u0026ldquo;.\u0026rdquo; + base64UrlEncode(payload), secret)\n签名是用于验证消息在传递过程中有没有被更改，并且，对于使用私钥签名的token，它还可以验证JWT的发送方是否为它所称的发送方。\n看一张官网的图就明白了：\n4. JSON Web Tokens是如何工作的 在认证的时候，当用户用他们的凭证成功登录以后，一个JSON Web Token将会被返回。此后，token就是用户凭证了，你必须非常小心以防止出现安全问题。一般而言，你保存令牌的时候不应该超过你所需要它的时间。\n无论何时用户想要访问受保护的路由或者资源的时候，用户代理（通常是浏览器）都应该带上JWT，典型的，通常放在Authorization header中，用Bearer schema。\nheader应该看起来是这样的：\nAuthorization: Bearer\n服务器上的受保护的路由将会检查Authorization header中的JWT是否有效，如果有效，则用户可以访问受保护的资源。如果JWT包含足够多的必需的数据，那么就可以减少对某些操作的数据库查询的需要，尽管可能并不总是如此。\n如果token是在授权头（Authorization header）中发送的，那么跨源资源共享(CORS)将不会成为问题，因为它不使用cookie。\n5. 基于Token的身份认证 与 基于服务器的身份认证 5.1 基于服务器的身份认证\n在讨论基于Token的身份认证是如何工作的以及它的好处之前，我们先来看一下以前我们是怎么做的：\nHTTP协议是无状态的，也就是说，如果我们已经认证了一个用户，那么他下一次请求的时候，服务器不知道我是谁，我们必须再次认证\n传统的做法是将已经认证过的用户信息存储在服务器上，比如Session。用户下次请求的时候带着Session ID，然后服务器以此检查用户是否认证过。\n这种基于服务器的身份认证方式存在一些问题：\nSessions : 每次用户认证通过以后，服务器需要创建一条记录保存用户信息，通常是在内存中，随着认证通过的用户越来越多，服务器的在这里的开销就会越来越大。 Scalability : 由于Session是在内存中的，这就带来一些扩展性的问题。 CORS : 当我们想要扩展我们的应用，让我们的数据被多个移动设备使用时，我们必须考虑跨资源共享问题。当使用AJAX调用从另一个域名下获取资源时，我们可能会遇到禁止请求的问题。 CSRF : 用户很容易受到CSRF攻击。 5.2. JWT与Session的差异 相同点是，它们都是存储用户信息；然而，Session是在服务器端的，而JWT是在客户端的。\nSession方式存储用户信息的最大问题在于要占用大量服务器内存，增加服务器的开销。\n而JWT方式将用户状态分散到了客户端中，可以明显减轻服务端的内存压力。\nSession的状态是存储在服务器端，客户端只有session id；而Token的状态是存储在客户端。\n5.3. 基于Token的身份认证是如何工作的 基于Token的身份认证是无状态的，服务器或者Session中不会存储任何用户信息。\n没有会话信息意味着应用程序可以根据需要扩展和添加更多的机器，而不必担心用户登录的位置。\n虽然这一实现可能会有所不同，但其主要流程如下：\n-用户携带用户名和密码请求访问 -服务器校验用户凭据 -应用提供一个token给客户端 -客户端存储token，并且在随后的每一次请求中都带着它 -服务器校验token并返回数据\n注意：\n-每一次请求都需要token -Token应该放在请求header中 -我们还需要将服务器设置为接受来自所有域的请求，用Access-Control-Allow-Origin: *\n5.4. 用Token的好处 - 无状态和可扩展性：Tokens存储在客户端。完全无状态，可扩展。我们的负载均衡器可以将用户传递到任意服务器，因为在任何地方都没有状态或会话信息。 - 安全：Token不是Cookie。（The token, not a cookie.）每次请求的时候Token都会被发送。而且，由于没有Cookie被发送，还有助于防止CSRF攻击。即使在你的实现中将token存储到客户端的Cookie中，这个Cookie也只是一种存储机制，而非身份认证机制。没有基于会话的信息可以操作，因为我们没有会话!\n还有一点，token在一段时间以后会过期，这个时候用户需要重新登录。这有助于我们保持安全。还有一个概念叫token撤销，它允许我们根据相同的授权许可使特定的token甚至一组token无效。\n5.5. JWT与OAuth的区别 -OAuth2是一种授权框架 ，JWT是一种认证协议 -无论使用哪种方式切记用HTTPS来保证数据的安全性 -OAuth2用在使用第三方账号登录的情况(比如使用weibo, qq, github登录某个app)，而JWT是用在前后端分离, 需要简单的对后台API进行保护时使用。\n","date":"2021-02-15T00:00:00Z","permalink":"https://xiaonuoz.github.io/p/jwtjsonwebtoken/","title":"jwtJSONWEBTOKEN"},{"content":"gorm介绍 安装 1 go get -u github.com/jinzhu/gorm 连接数据库 连接不同的数据库都需要导入对应数据的驱动程序，GORM已经贴心的包装了一些驱动程序，只需要按如下方式导入需要的数据库驱动即可：\n1 2 3 4 import _ \u0026#34;github.com/jinzhu/gorm/dialects/mysql\u0026#34; // import _ \u0026#34;github.com/jinzhu/gorm/dialects/postgres\u0026#34; // import _ \u0026#34;github.com/jinzhu/gorm/dialects/sqlite\u0026#34; // import _ \u0026#34;github.com/jinzhu/gorm/dialects/mssql\u0026#34; 连接MySQL（db最好定义为全局变量） db定义为全局变量的原因首先是避免重复生成新的连接，并且db本身就代表orm的一个连接池句柄了，其他地方可以直接通过db. 来操作数据库 [redigo中通过初始化连接池然后Get()获取池里面的连接，这里db就是一个连接池，调用db的方法默认会从池中抽取一个连接来使用，使用完默认自动放回去] ，所以在定义全局的db后最好不要再close()掉db了，让它随程序的存在而存在，程序停止则池子停止。\ndb连接池默认空闲数好像是5，可以通过DB().SetMaxIdleConns()来设置最大空闲数，通过SetMaxOpenConns()设置最大连接数。\n1 2 3 4 5 6 7 8 9 10 import ( \u0026#34;github.com/jinzhu/gorm\u0026#34; _ \u0026#34;github.com/jinzhu/gorm/dialects/mysql\u0026#34; ) func main() { //parseTime 将数据中包含时间类型的解析成数据库时间类型 db, err := gorm.Open(\u0026#34;mysql\u0026#34;, \u0026#34;user:password@(localhost:port)/dbname?charset=utf8mb4\u0026amp;parseTime=True\u0026#34;) //defer db.Close() } 连接PostgreSQL 基本代码同上，注意引入对应 postgres驱动并正确指定 gorm.Open()参数。\n1 2 3 4 5 6 7 8 9 import ( \u0026#34;github.com/jinzhu/gorm\u0026#34; _ \u0026#34;github.com/jinzhu/gorm/dialects/postgres\u0026#34; ) func main() { db, err := gorm.Open(\u0026#34;postgres\u0026#34;, \u0026#34;host=myhost port=myport user=gorm dbname=gorm password=mypassword\u0026#34;) defer db.Close() } 连接Sqlite3 基本代码同上，注意引入对应 sqlite驱动并正确指定 gorm.Open()参数。\n1 2 3 4 5 6 7 8 9 import ( \u0026#34;github.com/jinzhu/gorm\u0026#34; _ \u0026#34;github.com/jinzhu/gorm/dialects/sqlite\u0026#34; ) func main() { db, err := gorm.Open(\u0026#34;sqlite3\u0026#34;, \u0026#34;/tmp/gorm.db\u0026#34;) defer db.Close() } 连接SQL Server 基本代码同上，注意引入对应 mssql驱动并正确指定 gorm.Open()参数。\n1 2 3 4 5 6 7 8 9 import ( \u0026#34;github.com/jinzhu/gorm\u0026#34; _ \u0026#34;github.com/jinzhu/gorm/dialects/mssql\u0026#34; ) func main() { db, err := gorm.Open(\u0026#34;mssql\u0026#34;, \u0026#34;sqlserver://username:password@localhost:1433?database=dbname\u0026#34;) defer db.Close() } GORM基本示例 GORM操作MySQL 使用GORM连接上面的 db1进行创建、查询、更新、删除操作。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/jinzhu/gorm\u0026#34; _ \u0026#34;github.com/jinzhu/gorm/dialects/mysql\u0026#34; ) // UserInfo 用户信息 type UserInfo struct { ID uint Name string Gender string Hobby string } var db *gorm.DB func initMysql()(err error){ db, err = gorm.Open(\u0026#34;mysql\u0026#34;, \u0026#34;user:password@(localhost:port)/dbname?charset=utf8mb4\u0026amp;parseTime=True\u0026#34;) if err != nil { return err } return err } func main() { err := initMysql() if err != nil { panic(err) } defer db.Close() //结构体名和成员名必须大写，否则在AutoMigrate函数中无法对其成员值进行修改 //自动创建数据库表（它会将结构体和数据库表进行对应，对名称进行解析，两个单词之间用下划线分割，且加上复数，即user_infos） //此方法对给定的模型进行自动迁移，只会添加缺失的字段，不会删除/更改当前数据，当表存在也添加此方法，将结构体与数据库中的表进行关联,结构体与表不对应则会添加字段 db.AutoMigrate(\u0026amp;UserInfo{}) //添加数据, mysql无法添加切片类型数据 //结构体对象比较大，建议传指针 u:=UserInfo{Id:1,Name: \u0026#34;笑傩\u0026#34;,Gender: \u0026#34;男\u0026#34;,Hobby: \u0026#34;篮球\u0026#34;} db.Create(\u0026amp;u) //查询数据 u:=UserInfo{Id: 1} db.Find(\u0026amp;u) var uu UserInfo //根据条件查询 db.Find(\u0026amp;uu, \u0026#34;hobby=?\u0026#34;, \u0026#34;足球\u0026#34;) fmt.Printf(\u0026#34;%#v\\n\u0026#34;, uu) //更新数据 db.Model(\u0026amp;u).Update(\u0026#34;hobby=?\u0026#34;,\u0026#34;足球\u0026#34;) fmt.Println(u) ////删除数据 db.Delete(\u0026amp;u) } 要注意的是db.AutoMigrate()用于与数据库表进行同步，如果不需要同步，仅仅是绑定的话，可以使用 db.Model(\u0026amp;User{})，Model 指定你想运行db操作的模型，它不会因为结构体的新增改变数据库结构，仅绑定。\nGORM Model定义 在使用ORM工具时，通常我们需要在代码中定义模型（Models）与数据库中的数据表进行映射，在GORM中模型（Models）通常是正常定义的结构体、基本的go类型或它们的指针。 同时也支持 sql.Scanner及 driver.Valuer接口（interfaces）。\ngorm.Model 为了方便模型定义，GORM内置了一个 gorm.Model结构体。gorm.Model是一个包含了 ID, CreatedAt, UpdatedAt, DeletedAt四个字段的Golang结构体。\n1 2 3 4 5 6 7 // gorm.Model 定义 type Model struct { ID uint `gorm:\u0026#34;primary_key\u0026#34;` CreatedAt time.Time UpdatedAt time.Time DeletedAt *time.Time } 可以将它嵌入到自己设置的模型中：\n1 2 3 4 5 // 将 `ID`, `CreatedAt`, `UpdatedAt`, `DeletedAt`字段注入到`User`模型中 type User struct { gorm.Model Name string } 当然也可以完全自己定义模型：\n1 2 3 4 5 // 不使用gorm.Model，自行定义模型 type User struct { ID int Name string } 模型定义示例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 type User struct { gorm.Model Name string Age sql.NullInt64\t//零值类型 Birthday *time.Time Email string `gorm:\u0026#34;type:varchar(100);unique_index\u0026#34;`\t//type设置字段类型。unique_index 建立一个唯一的索引 Role string\t`gorm:\u0026#34;size:255\u0026#34;`\t//设置字段大小为255 //默认情况下，所有字段的零值, 比如 0, \u0026#39;\u0026#39;, false 或者其它 零值，都不会保存到数据库内，使用指针可以避免这种情况。 //例如我给某字段赋予一个默认的零值，此时这个值并不会保存到数据库，而是使用数据库设定的默认值。当使用指针时，指针是一个十六位数，可以顺利存进去 MemberNumber *string `gorm:\u0026#34;unique;not null\u0026#34;`\t//设置会员号唯一并且不能为空 Num int `gorm:\u0026#34;AUTO_INCREMENT\u0026#34;`\t//设置num为自增类型 Address string\t`gorm:\u0026#34;index:addr\u0026#34;` //给address字段创建名为addr的索引 IgnoreMe int `gorm:\u0026#34;-\u0026#34;`\t//忽略本字段，即该字段不会体现在数据库中 } 结构体标记（tags） 使用结构体声明模型时，标记（tags）是可选项。gorm支持以下标记:\n支持的结构体标记（Struct tags） 关联相关标记（tags） 主键、表名、列名的约定 主键（Primary Key） GORM 默认会使用名为ID的字段作为表的主键。\n1 2 3 4 5 6 7 8 9 10 11 type User struct { ID string // 名为`ID`的字段会默认作为表的主键 Name string } // 使用`AnimalID`作为主键 type Animal struct { AnimalID int64 `gorm:\u0026#34;primary_key\u0026#34;` Name string Age int64 } 表名（Table Name） 表名默认就是结构体名称的复数，例如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 type User struct {} // 默认表名是 `users` // 将 User 的表名设置为 `profiles` func (User) TableName() string { return \u0026#34;profiles\u0026#34; } // 也可以在设置表名时进行判断 func (u User) TableName() string { if u.Role == \u0026#34;admin\u0026#34; { return \u0026#34;admin_users\u0026#34; } else { return \u0026#34;users\u0026#34; } } // 禁用默认表名的复数形式，如果置为 true，则 `User` 的默认表名是 `user` db.SingularTable(true) 也可以通过 Table()指定表名：\n1 2 3 4 5 6 7 8 9 // 使用User结构体创建名为`deleted_users`的表 db.Table(\u0026#34;deleted_users\u0026#34;).CreateTable(\u0026amp;User{}) var deleted_users []User db.Table(\u0026#34;deleted_users\u0026#34;).Find(\u0026amp;deleted_users) //// SELECT * FROM deleted_users; db.Table(\u0026#34;deleted_users\u0026#34;).Where(\u0026#34;name = ?\u0026#34;, \u0026#34;jinzhu\u0026#34;).Delete() //// DELETE FROM deleted_users WHERE name = \u0026#39;jinzhu\u0026#39;; GORM还支持更改默认表名称规则：\n1 2 3 4 //每次执行创建语句之前会执行此函数，在每个表之前增加前缀 gorm.DefaultTableNameHandler = func (db *gorm.DB, defaultTableName string) string { return \u0026#34;prefix_\u0026#34; + defaultTableName; } 列名（Column Name） 列名由字段名称进行下划线分割来生成\n1 2 3 4 5 6 type User struct { ID uint // column name is `id` Name string // column name is `name` Birthday time.Time // column name is `birthday` CreatedAt time.Time // column name is `created_at` } 可以使用结构体tag指定列名：\n1 2 3 4 5 type Animal struct { AnimalId int64 `gorm:\u0026#34;column:beast_id\u0026#34;` // set column name to `beast_id` Birthday time.Time `gorm:\u0026#34;column:day_of_the_beast\u0026#34;` // set column name to `day_of_the_beast` Age int64 `gorm:\u0026#34;column:age_of_the_beast\u0026#34;` // set column name to `age_of_the_beast` } 时间戳跟踪 CreatedAt 如果模型有 CreatedAt字段，该字段的值将会是初次创建记录的时间。\n1 2 3 4 db.Create(\u0026amp;user) // `CreatedAt`将会是当前时间 // 可以使用`Update`方法来改变`CreateAt`的值 db.Model(\u0026amp;user).Update(\u0026#34;CreatedAt\u0026#34;, time.Now()) UpdatedAt 如果模型有 UpdatedAt字段，该字段的值将会是每次更新记录的时间。\n1 2 3 db.Save(\u0026amp;user) // `UpdatedAt`将会是当前时间 db.Model(\u0026amp;user).Update(\u0026#34;name\u0026#34;, \u0026#34;jinzhu\u0026#34;) // `UpdatedAt`将会是当前时间 DeletedAt 如果模型有 DeletedAt字段，调用 Delete删除该记录时，将会设置 DeletedAt字段为当前时间，而不是直接将记录从数据库中删除（软删除）。删除后，无论查询还是修改都无法查询到该行数据了\nCRUD 创建 创建记录 首先定义模型：\n1 2 3 4 5 type User struct { ID int64 Name string Age int64 } 使用使用 NewRecord()查询主键是否存在，主键为空使用 Create()创建记录：\n1 2 3 4 u:=User{Name: \u0026#34;笑傩\u0026#34;,Age: 18} fmt.Println(db.NewRecord(\u0026amp;u))\t//如果自己设置主键的值，此方法是判断是否存在相同的主键值,不存在则返回true db.Create(\u0026amp;u) fmt.Println(db.NewRecord(\u0026amp;u))\t//这个fmt返回的是false，因为当创建后，u实例获取到了数据库中ID的值，此值在数据库中存在，所以返回false 成员首字母一定要大写，不然无法创建对应的字段！！\n默认值 可以通过 tag 定义字段的默认值，比如：\n1 2 3 4 5 type User struct { ID int64 Name string `gorm:\u0026#34;default:\u0026#39;小王子\u0026#39;\u0026#34;` Age int64 } 注意：通过tag定义字段的默认值，在创建记录时候生成的 SQL 语句会排除没有值或值为 零值 的字段(即insert时不设置字段为对应的零值，取决于数据库设置的默认值)。 在将记录插入到数据库后，Gorm会从数据库加载那些字段的默认值。（tag一定要在第一次建表时就设置好，因为当表建好后再设置默认值是无效的，数据库此时对应字段已经设置为了默认的NULL，它无法进行自动迁移）\n举个例子：\n1 2 3 var user = User{Name:\u0026#34;\u0026#34;,Age: 18} db.Debug().Create(\u0026amp;user)\t//Debug()：在进行增删改查操作时会在终端打印出操作语句 //INSERT INTO `users` (`age`) VALUES (18) 上面代码实际执行的SQL语句是 INSERT INTO users(\u0026quot;age\u0026quot;) values('99');，排除了零值字段 Name，而在数据库中这一条数据会使用tag设置的默认值作为Name字段的值。\n**注意：**所有字段的零值, 比如 0, \u0026quot;\u0026quot;,false或者其它 零值，都不会保存到数据库内，但会使用他们数据库中的默认值。 如果你想避免这种情况，可以考虑使用指针或实现 Scanner/Valuer接口，比如：\n使用指针方式实现零值存入数据库 1 2 3 4 5 6 7 8 // 使用指针 type User struct { ID int64 Name *string `gorm:\u0026#34;default:\u0026#39;笑傩\u0026#39;\u0026#34;` Age int64 } user := User{Name: new(string), Age: 18))} db.Create(\u0026amp;user) // 此时数据库中该条记录name字段的值就是\u0026#39;\u0026#39; 使用Scanner/Valuer接口方式实现零值存入数据库 1 2 3 4 5 6 7 8 // 使用 Scanner/Valuer type User struct { ID int64 Name sql.NullString `gorm:\u0026#34;default:\u0026#39;笑傩\u0026#39;\u0026#34;` // sql.NullString 实现了Scanner/Valuer接口 Age int64 } user := User{Name: sql.NullString{\u0026#34;\u0026#34;, true}, Age:18} db.Create(\u0026amp;user) // 此时数据库中该条记录name字段的值就是\u0026#39;\u0026#39; 扩展创建选项 例如 PostgreSQL数据库中可以使用下面的方式实现合并插入, 有则更新, 无则插入。只针对PostgreSQL\n1 2 3 // 为Instert语句添加扩展SQL选项 db.Set(\u0026#34;gorm:insert_option\u0026#34;, \u0026#34;ON CONFLICT\u0026#34;).Create(\u0026amp;product) // INSERT INTO products (name, code) VALUES (\u0026#34;name\u0026#34;, \u0026#34;code\u0026#34;) ON CONFLICT; 查询 一般查询 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // 根据主键查询第一条记录，仅当主键为整型时可用 db.First(\u0026amp;user) //// SELECT * FROM users ORDER BY userId LIMIT 1; // 随机获取一条记录 db.Take(\u0026amp;user) //// SELECT * FROM users LIMIT 1; // 根据主键查询最后一条记录 db.Last(\u0026amp;user) //// SELECT * FROM users ORDER BY id DESC LIMIT 1; // 查询所有的记录，users是一个切片，用切片承载查询出来的结果 db.Find(\u0026amp;users) //// SELECT * FROM users; Where 条件 普通SQL查询 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 // Get first matched record db.Where(\u0026#34;name = ?\u0026#34;, \u0026#34;jinzhu\u0026#34;).First(\u0026amp;user) //// SELECT * FROM users WHERE name = \u0026#39;jinzhu\u0026#39; limit 1; // Get all matched records db.Where(\u0026#34;name = ?\u0026#34;, \u0026#34;jinzhu\u0026#34;).Find(\u0026amp;users) //// SELECT * FROM users WHERE name = \u0026#39;jinzhu\u0026#39;; // \u0026lt;\u0026gt; db.Where(\u0026#34;name \u0026lt;\u0026gt; ?\u0026#34;, \u0026#34;jinzhu\u0026#34;).Find(\u0026amp;users) //// SELECT * FROM users WHERE name \u0026lt;\u0026gt; \u0026#39;jinzhu\u0026#39;; // IN db.Where(\u0026#34;name IN (?)\u0026#34;, []string{\u0026#34;jinzhu\u0026#34;, \u0026#34;jinzhu 2\u0026#34;}).Find(\u0026amp;users) //// SELECT * FROM users WHERE name in (\u0026#39;jinzhu\u0026#39;,\u0026#39;jinzhu 2\u0026#39;); // LIKE db.Where(\u0026#34;name LIKE ?\u0026#34;, \u0026#34;%jin%\u0026#34;).Find(\u0026amp;users) //// SELECT * FROM users WHERE name LIKE \u0026#39;%jin%\u0026#39;; // AND db.Where(\u0026#34;name = ? AND age \u0026gt;= ?\u0026#34;, \u0026#34;jinzhu\u0026#34;, \u0026#34;22\u0026#34;).Find(\u0026amp;users) //// SELECT * FROM users WHERE name = \u0026#39;jinzhu\u0026#39; AND age \u0026gt;= 22; // Time db.Where(\u0026#34;updated_at \u0026gt; ?\u0026#34;, lastWeek).Find(\u0026amp;users) //// SELECT * FROM users WHERE updated_at \u0026gt; \u0026#39;2000-01-01 00:00:00\u0026#39;; // BETWEEN db.Where(\u0026#34;created_at BETWEEN ? AND ?\u0026#34;, lastWeek, today).Find(\u0026amp;users) //// SELECT * FROM users WHERE created_at BETWEEN \u0026#39;2000-01-01 00:00:00\u0026#39; AND \u0026#39;2000-01-08 00:00:00\u0026#39;; Struct \u0026amp; Map查询 1 2 3 4 5 6 7 8 9 10 11 // Struct db.Where(\u0026amp;User{Name: \u0026#34;jinzhu\u0026#34;, Age: 20}).First(\u0026amp;user) //// SELECT * FROM users WHERE name = \u0026#34;jinzhu\u0026#34; AND age = 20 LIMIT 1; // Map db.Where(map[string]interface{}{\u0026#34;name\u0026#34;: \u0026#34;jinzhu\u0026#34;, \u0026#34;age\u0026#34;: 20}).Find(\u0026amp;users) //// SELECT * FROM users WHERE name = \u0026#34;jinzhu\u0026#34; AND age = 20; // 主键的切片 db.Where([]int64{20, 21, 22}).Find(\u0026amp;users) //// SELECT * FROM users WHERE id IN (20, 21, 22); **提示：**当通过结构体进行查询时，GORM将会只通过非零值字段查询，这意味着如果你的字段值为 0，''，false或者其他 零值时，将不会被用于构建查询条件，例如：\n1 2 db.Where(\u0026amp;User{Name: \u0026#34;jinzhu\u0026#34;, Age: 0}).Find(\u0026amp;users) //// SELECT * FROM users WHERE name = \u0026#34;jinzhu\u0026#34;; 你可以使用指针或实现 Scanner/Valuer 接口来避免这个问题.\n1 2 3 4 5 6 7 8 9 10 11 12 13 // 使用指针 type User struct { gorm.Model Name string Age *int } // 使用 Scanner/Valuer type User struct { gorm.Model Name string Age sql.NullInt64 // sql.NullInt64 实现了 Scanner/Valuer 接口 } Not 条件（不等于） 作用与 Where 类似的情形如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 db.Not(\u0026#34;name\u0026#34;, \u0026#34;jinzhu\u0026#34;).First(\u0026amp;user) //// SELECT * FROM users WHERE name \u0026lt;\u0026gt; \u0026#34;jinzhu\u0026#34; LIMIT 1; // Not In db.Not(\u0026#34;name\u0026#34;, []string{\u0026#34;jinzhu\u0026#34;, \u0026#34;jinzhu 2\u0026#34;}).Find(\u0026amp;users) //// SELECT * FROM users WHERE name NOT IN (\u0026#34;jinzhu\u0026#34;, \u0026#34;jinzhu 2\u0026#34;); // Not In slice of primary keys db.Not([]int64{1,2,3}).First(\u0026amp;user) //// SELECT * FROM users WHERE id NOT IN (1,2,3); db.Not([]int64{}).First(\u0026amp;user) //// SELECT * FROM users; // Plain SQL db.Not(\u0026#34;name = ?\u0026#34;, \u0026#34;jinzhu\u0026#34;).First(\u0026amp;user) //// SELECT * FROM users WHERE NOT(name = \u0026#34;jinzhu\u0026#34;); // Struct db.Not(User{Name: \u0026#34;jinzhu\u0026#34;}).First(\u0026amp;user) //// SELECT * FROM users WHERE name \u0026lt;\u0026gt; \u0026#34;jinzhu\u0026#34;; Or条件 1 2 3 4 5 6 7 8 9 10 db.Where(\u0026#34;role = ?\u0026#34;, \u0026#34;admin\u0026#34;).Or(\u0026#34;role = ?\u0026#34;, \u0026#34;super_admin\u0026#34;).Find(\u0026amp;users) //// SELECT * FROM users WHERE role = \u0026#39;admin\u0026#39; OR role = \u0026#39;super_admin\u0026#39;; // Struct db.Where(\u0026#34;name = \u0026#39;jinzhu\u0026#39;\u0026#34;).Or(User{Name: \u0026#34;jinzhu 2\u0026#34;}).Find(\u0026amp;users) //// SELECT * FROM users WHERE name = \u0026#39;jinzhu\u0026#39; OR name = \u0026#39;jinzhu 2\u0026#39;; // Map db.Where(\u0026#34;name = \u0026#39;jinzhu\u0026#39;\u0026#34;).Or(map[string]interface{}{\u0026#34;name\u0026#34;: \u0026#34;jinzhu 2\u0026#34;}).Find(\u0026amp;users) //// SELECT * FROM users WHERE name = \u0026#39;jinzhu\u0026#39; OR name = \u0026#39;jinzhu 2\u0026#39;; 内联条件(在立即执行方法中添加条件，而不是使用where) 作用与 Where查询类似，当内联条件与多个立即执行方法一起使用时, 内联条件不会传递给后面的立即执行方法（即只有当前立即执行方法可以使用，后面的立即执行方法是无法复用当前方法的内联条件的）。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // 根据主键获取记录 (只适用于整形主键) db.First(\u0026amp;user, 23) //// SELECT * FROM users WHERE id = 23 LIMIT 1; // 根据主键获取记录, 如果它是一个非整形主键 db.First(\u0026amp;user, \u0026#34;id = ?\u0026#34;, \u0026#34;string_primary_key\u0026#34;) //// SELECT * FROM users WHERE id = \u0026#39;string_primary_key\u0026#39; LIMIT 1; // Plain SQL db.Find(\u0026amp;user, \u0026#34;name = ?\u0026#34;, \u0026#34;jinzhu\u0026#34;) //// SELECT * FROM users WHERE name = \u0026#34;jinzhu\u0026#34;; db.Find(\u0026amp;users, \u0026#34;name \u0026lt;\u0026gt; ? AND age \u0026gt; ?\u0026#34;, \u0026#34;jinzhu\u0026#34;, 20) //// SELECT * FROM users WHERE name \u0026lt;\u0026gt; \u0026#34;jinzhu\u0026#34; AND age \u0026gt; 20; // Struct db.Find(\u0026amp;users, User{Age: 20}) //// SELECT * FROM users WHERE age = 20; // Map db.Find(\u0026amp;users, map[string]interface{}{\u0026#34;age\u0026#34;: 20}) //// SELECT * FROM users WHERE age = 20; 额外查询选项 1 2 3 // 为查询 SQL 添加额外的 SQL 操作 db.Set(\u0026#34;gorm:query_option\u0026#34;, \u0026#34;FOR UPDATE\u0026#34;).First(\u0026amp;user, 10) //// SELECT * FROM users WHERE id = 10 FOR UPDATE; FirstOrInit 获取匹配的第一条记录，否则根据给定的条件初始化一个新的对象 (仅支持 struct 和 map 条件)（注意：这个仅仅是初始化一个新的实例，并不会入库到数据库，不会入库到数据库！！！）\n1 2 3 4 5 6 7 8 9 // 未找到 db.FirstOrInit(\u0026amp;user, User{Name: \u0026#34;non_existing\u0026#34;}) //// user -\u0026gt; User{Name: \u0026#34;non_existing\u0026#34;} // 找到 db.Where(User{Name: \u0026#34;Jinzhu\u0026#34;}).FirstOrInit(\u0026amp;user) //// user -\u0026gt; User{Id: 111, Name: \u0026#34;Jinzhu\u0026#34;, Age: 20} db.FirstOrInit(\u0026amp;user, map[string]interface{}{\u0026#34;name\u0026#34;: \u0026#34;jinzhu\u0026#34;}) //// user -\u0026gt; User{Id: 111, Name: \u0026#34;Jinzhu\u0026#34;, Age: 20} Attrs 如果记录未找到，将使用参数初始化 struct.\n1 2 3 4 5 6 7 8 9 10 11 12 13 // 未找到 db.Where(User{Name: \u0026#34;non_existing\u0026#34;}).Attrs(User{Age: 20}).FirstOrInit(\u0026amp;user) //// SELECT * FROM USERS WHERE name = \u0026#39;non_existing\u0026#39;; //// user -\u0026gt; User{Name: \u0026#34;non_existing\u0026#34;, Age: 20} db.Where(User{Name: \u0026#34;non_existing\u0026#34;}).Attrs(\u0026#34;age\u0026#34;, 20).FirstOrInit(\u0026amp;user) //// SELECT * FROM USERS WHERE name = \u0026#39;non_existing\u0026#39;; //// user -\u0026gt; User{Name: \u0026#34;non_existing\u0026#34;, Age: 20} // 找到 db.Where(User{Name: \u0026#34;Jinzhu\u0026#34;}).Attrs(User{Age: 30}).FirstOrInit(\u0026amp;user) //// SELECT * FROM USERS WHERE name = jinzhu\u0026#39;; //// user -\u0026gt; User{Id: 111, Name: \u0026#34;Jinzhu\u0026#34;, Age: 20} Assign 不管记录是否找到，都将参数赋值给 struct.\n1 2 3 4 5 6 7 8 // 未找到 db.Where(User{Name: \u0026#34;non_existing\u0026#34;}).Assign(User{Age: 20}).FirstOrInit(\u0026amp;user) //// user -\u0026gt; User{Name: \u0026#34;non_existing\u0026#34;, Age: 20} // 找到 db.Where(User{Name: \u0026#34;Jinzhu\u0026#34;}).Assign(User{Age: 30}).FirstOrInit(\u0026amp;user) //// SELECT * FROM USERS WHERE name = jinzhu\u0026#39;; //// user -\u0026gt; User{Id: 111, Name: \u0026#34;Jinzhu\u0026#34;, Age: 30} FirstOrCreate 获取匹配的第一条记录, 否则根据给定的条件创建一个新的记录 (仅支持 struct 和 map 条件)（要入库到数据库中）\n1 2 3 4 5 6 7 8 // 未找到 db.FirstOrCreate(\u0026amp;user, User{Name: \u0026#34;non_existing\u0026#34;}) //// INSERT INTO \u0026#34;users\u0026#34; (name) VALUES (\u0026#34;non_existing\u0026#34;); //// user -\u0026gt; User{Id: 112, Name: \u0026#34;non_existing\u0026#34;} // 找到 db.Where(User{Name: \u0026#34;Jinzhu\u0026#34;}).FirstOrCreate(\u0026amp;user) //// user -\u0026gt; User{Id: 111, Name: \u0026#34;Jinzhu\u0026#34;} Attrs 如果记录未找到，将使用参数创建 struct 和记录.\n1 2 3 4 5 6 7 8 9 10 // 未找到 db.Where(User{Name: \u0026#34;non_existing\u0026#34;}).Attrs(User{Age: 20}).FirstOrCreate(\u0026amp;user) //// SELECT * FROM users WHERE name = \u0026#39;non_existing\u0026#39;; //// INSERT INTO \u0026#34;users\u0026#34; (name, age) VALUES (\u0026#34;non_existing\u0026#34;, 20); //// user -\u0026gt; User{Id: 112, Name: \u0026#34;non_existing\u0026#34;, Age: 20} // 找到 db.Where(User{Name: \u0026#34;jinzhu\u0026#34;}).Attrs(User{Age: 30}).FirstOrCreate(\u0026amp;user) //// SELECT * FROM users WHERE name = \u0026#39;jinzhu\u0026#39;; //// user -\u0026gt; User{Id: 111, Name: \u0026#34;jinzhu\u0026#34;, Age: 20} Assign 不管记录是否找到，都将参数赋值给 struct 并保存至数据库.\n1 2 3 4 5 6 7 8 9 10 11 // 未找到 db.Where(User{Name: \u0026#34;non_existing\u0026#34;}).Assign(User{Age: 20}).FirstOrCreate(\u0026amp;user) //// SELECT * FROM users WHERE name = \u0026#39;non_existing\u0026#39;; //// INSERT INTO \u0026#34;users\u0026#34; (name, age) VALUES (\u0026#34;non_existing\u0026#34;, 20); //// user -\u0026gt; User{Id: 112, Name: \u0026#34;non_existing\u0026#34;, Age: 20} // 找到 db.Where(User{Name: \u0026#34;jinzhu\u0026#34;}).Assign(User{Age: 30}).FirstOrCreate(\u0026amp;user) //// SELECT * FROM users WHERE name = \u0026#39;jinzhu\u0026#39;; //// UPDATE users SET age=30 WHERE id = 111; //// user -\u0026gt; User{Id: 111, Name: \u0026#34;jinzhu\u0026#34;, Age: 30} 高级查询 子查询 基于 *gorm.expr 的子查询\n1 2 db.Where(\u0026#34;amount \u0026gt; ?\u0026#34;, db.Table(\u0026#34;orders\u0026#34;).Select(\u0026#34;AVG(amount)\u0026#34;).Where(\u0026#34;state = ?\u0026#34;, \u0026#34;paid\u0026#34;).SubQuery()).Find(\u0026amp;orders) // SELECT * FROM \u0026#34;orders\u0026#34; WHERE \u0026#34;orders\u0026#34;.\u0026#34;deleted_at\u0026#34; IS NULL AND (amount \u0026gt; (SELECT AVG(amount) FROM \u0026#34;orders\u0026#34; WHERE (state = \u0026#39;paid\u0026#39;))); 选择字段 Select，指定你想从数据库中检索出的字段，默认会选择全部字段。\n1 2 3 4 5 6 7 8 db.Select(\u0026#34;name, age\u0026#34;).Find(\u0026amp;users) //// SELECT name, age FROM users; db.Select([]string{\u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;}).Find(\u0026amp;users) //// SELECT name, age FROM users; db.Table(\u0026#34;users\u0026#34;).Select(\u0026#34;COALESCE(age,?)\u0026#34;, 42).Rows() //// SELECT COALESCE(age,\u0026#39;42\u0026#39;) FROM users; 排序 Order，指定从数据库中检索出记录的顺序。设置第二个参数 reorder 为 true ，可以覆盖前面定义的排序条件（不加则是在原来的基础上继续排序）。\n1 2 3 4 5 6 7 8 9 10 11 db.Order(\u0026#34;age desc, name\u0026#34;).Find(\u0026amp;users) //// SELECT * FROM users ORDER BY age desc, name; // 多字段排序 db.Order(\u0026#34;age desc\u0026#34;).Order(\u0026#34;name\u0026#34;).Find(\u0026amp;users) //// SELECT * FROM users ORDER BY age desc, name; // 覆盖排序 db.Order(\u0026#34;age desc\u0026#34;).Find(\u0026amp;users1).Order(\u0026#34;age\u0026#34;, true).Find(\u0026amp;users2) //// SELECT * FROM users ORDER BY age desc; (users1) //// SELECT * FROM users ORDER BY age; (users2) 数量 Limit，指定从数据库检索出的最大记录数。\n1 2 3 4 5 6 7 db.Limit(3).Find(\u0026amp;users) //// SELECT * FROM users LIMIT 3; // -1 取消 Limit 条件 db.Limit(10).Find(\u0026amp;users1).Limit(-1).Find(\u0026amp;users2) //// SELECT * FROM users LIMIT 10; (users1) //// SELECT * FROM users; (users2) 偏移 Offset，指定开始返回记录前要跳过的记录数。\n1 2 3 4 5 6 7 db.Offset(3).Find(\u0026amp;users) //// SELECT * FROM users OFFSET 3; // -1 取消 Offset 条件 db.Offset(10).Find(\u0026amp;users1).Offset(-1).Find(\u0026amp;users2) //// SELECT * FROM users OFFSET 10; (users1) //// SELECT * FROM users; (users2) 总数 Count，该 model 能获取的记录总数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 db.Where(\u0026#34;name = ?\u0026#34;, \u0026#34;jinzhu\u0026#34;).Or(\u0026#34;name = ?\u0026#34;, \u0026#34;jinzhu 2\u0026#34;).Find(\u0026amp;users).Count(\u0026amp;count) //// SELECT * from USERS WHERE name = \u0026#39;jinzhu\u0026#39; OR name = \u0026#39;jinzhu 2\u0026#39;; (users) //// SELECT count(*) FROM users WHERE name = \u0026#39;jinzhu\u0026#39; OR name = \u0026#39;jinzhu 2\u0026#39;; (count) db.Model(\u0026amp;User{}).Where(\u0026#34;name = ?\u0026#34;, \u0026#34;jinzhu\u0026#34;).Count(\u0026amp;count) //// SELECT count(*) FROM users WHERE name = \u0026#39;jinzhu\u0026#39;; (count) // Table() 指定你要运行db操作的表 db.Table(\u0026#34;deleted_users\u0026#34;).Count(\u0026amp;count) //// SELECT count(*) FROM deleted_users; db.Table(\u0026#34;deleted_users\u0026#34;).Select(\u0026#34;count(distinct(name))\u0026#34;).Count(\u0026amp;count) //// SELECT count( distinct(name) ) FROM deleted_users; (count) 注意 Count 必须是链式查询的最后一个操作 ，因为它会覆盖前面的 SELECT，但如果里面使用了 count 时（在查询语句中）不会覆盖\n后端分页 post是文章的结构体，Limit(pageSize)表示每次查询的最大数量\n一对多查询-预加载Preload 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 //结构体定义 type Admin struct { User []User ID int Username string AdID int } type User struct { UserID int Price float64 AdminID int Admin *Admin `gorm:\u0026#34;ForeignKey:UserID;AssociationForeignKey:AdID\u0026#34;` } //多表查询 admin := Admin{ID: 1} db.Debug().Preload(\u0026#34;User\u0026#34;).Find(\u0026amp;admin) user := User{UserID: 1} db.Debug().Preload(\u0026#34;Admin\u0026#34;).Find(\u0026amp;user) //[2021-03-11 22:35:40] [83.77ms] SELECT * FROM `admins` WHERE `admins`.`id` = 1 //[2021-03-11 22:35:40] [84.80ms] SELECT * FROM `users` WHERE (`admin_id` IN (1)) //Preload用于一对多查询，当查询admin表时又需要users表的数据，则可使用此方法，它不需要额外设置tag进行外键关联，但是对字段有要求，Preload方法的参数为另一个结构体的名称[要打双引号]（在admin结构体中所取的名称）。它会先查询admin表，再查询User表进行填充。 //主表的查询字段（主键）必须取名为 ID ，它会根据id查询到admin的数据，然后再查询User结构体所对应表中的数据，查询user表的查询字段是主表 \u0026lt;结构体名_主表查询字段成员名\u0026gt; ，因此user结构体中要设定AdminID成员进行关联 //如果主表的查询字段不为ID时则需要用tag额外定义，来指定外键进行查询 [ `gorm:\u0026#34;ForeignKey:UserID;AssociationForeignKey:AdID\u0026#34;` ] ForeignKey指向当前结构体的成员名，AssociationForeignKey指向外键结构体的相对应的成员名 嵌套预加载Preload GORM 支持嵌套预加载，例如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 type Admin struct { ID int Username string User []User } type User struct { ID int Price float64 AdminID int Student Student } type Student struct { Sid int Name string UserID int } //一定要注意的是嵌套是student有user的键(userid)，user有admin的键(adminid)，层层嵌套出去的 //[2021-03-11 23:45:55] [83.77ms] SELECT * FROM `admins` WHERE `admins`.`id` = 1 //[2021-03-11 23:45:56] [78.89ms] SELECT * FROM `users` WHERE (`admin_id` IN (1)) //[2021-03-11 23:45:56] [82.81ms] SELECT * FROM `students` WHERE (`user_id` IN (1)) Preload在使用更新updates之类时也会同时更新结构体中子结构体的数据\n1 INSERT INTO `experts` (`created_at`,`updated_at`,`deleted_at`,`nickname`,`account`,`password`,`id`) VALUES (\u0026#39;2020-03-21 08:10:01\u0026#39;,\u0026#39;2020-03-20 00:00:00\u0026#39;,NULL,\u0026#39;专家谁\u0026#39;,\u0026#39;admin1\u0026#39;,\u0026#39;123456\u0026#39;,1) ON DUPLICATE KEY UPDATE `id`=`id` 它会使用上述insert代码，且后面会有 ON DUPLICATE KEY UPDATE ,**这部分语句的作用会在新增时主键或外键冲突时更新，不冲突则会新增数据。**因为在value中加入了id，当id冲突时，本来会进行更新，又因为update id=id，即id依然等于1，依然冲突。表明当冲突时不进行更新。\n如果想更新主表的同时新增子结构体对应的表，则需要将子结构体的id变为不冲突的id。但是这样子就会出现两条除id外其他数据相同的数据，而如果要进行更新操作，则必须在 ON DUPLICATE KEY UPDATE 后添加需要修改的字段才行，但是gorm并没有这种操作。如果要实现这种操作，可以在修改id的同时也修改子结构体的其他字段值，gorm会新增一条不同数据\n1 2 3 [SQL] ON DUPLICATE KEY UPDATE id = 5, password = \u0026#39;upPassword\u0026#39;; 如果想更新主表的同时不更新子结构体的表，则可以将子表进行初始化 obj.Experts = pkg.Experts{}\nGroup \u0026amp; Having 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 rows, err := db.Table(\u0026#34;orders\u0026#34;).Select(\u0026#34;date(created_at) as date, sum(amount) as total\u0026#34;).Group(\u0026#34;date(created_at)\u0026#34;).Rows() for rows.Next() { ... } // 使用Scan将多条结果扫描进事先准备好的结构体切片中 type Result struct { Date time.Time Total int } var rets []Result db.Table(\u0026#34;users\u0026#34;).Select(\u0026#34;date(created_at) as date, sum(age) as total\u0026#34;).Group(\u0026#34;date(created_at)\u0026#34;).Scan(\u0026amp;rets) rows, err := db.Table(\u0026#34;orders\u0026#34;).Select(\u0026#34;date(created_at) as date, sum(amount) as total\u0026#34;).Group(\u0026#34;date(created_at)\u0026#34;).Having(\u0026#34;sum(amount) \u0026gt; ?\u0026#34;, 100).Rows() for rows.Next() { ... } type Result struct { Date time.Time Total int64 } //Having和where类似，但是where是对原始数据进行判断，having是判断经过计算后的数据 db.Table(\u0026#34;orders\u0026#34;).Select(\u0026#34;date(created_at) as date, sum(amount) as total\u0026#34;).Group(\u0026#34;date(created_at)\u0026#34;).Having(\u0026#34;sum(amount) \u0026gt; ?\u0026#34;, 100).Scan(\u0026amp;results) 连接 Joins，指定连接条件\n1 2 3 4 5 6 7 8 9 rows, err := db.Table(\u0026#34;users\u0026#34;).Select(\u0026#34;users.name, emails.email\u0026#34;).Joins(\u0026#34;left join emails on emails.user_id = users.id\u0026#34;).Rows() for rows.Next() { ... } db.Table(\u0026#34;users\u0026#34;).Select(\u0026#34;users.name, emails.email\u0026#34;).Joins(\u0026#34;left join emails on emails.user_id = users.id\u0026#34;).Scan(\u0026amp;results) // 多连接及参数 db.Joins(\u0026#34;JOIN emails ON emails.user_id = users.id AND emails.email = ?\u0026#34;, \u0026#34;jinzhu@example.org\u0026#34;).Joins(\u0026#34;JOIN credit_cards ON credit_cards.user_id = users.id\u0026#34;).Where(\u0026#34;credit_cards.number = ?\u0026#34;, \u0026#34;411111111111\u0026#34;).Find(\u0026amp;user) Pluck Pluck，查询 model 中的一个列作为切片，如果您想要查询多个列，您应该使用 Scan\n1 2 3 4 5 6 7 8 9 10 var ages []int64 db.Find(\u0026amp;users).Pluck(\u0026#34;age\u0026#34;, \u0026amp;ages) var names []string db.Model(\u0026amp;User{}).Pluck(\u0026#34;name\u0026#34;, \u0026amp;names) db.Table(\u0026#34;deleted_users\u0026#34;).Pluck(\u0026#34;name\u0026#34;, \u0026amp;names) // 想查询多个字段？ 这样做： db.Select(\u0026#34;name, age\u0026#34;).Find(\u0026amp;users) 扫描 Scan，扫描结果至一个 struct.\n1 2 3 4 5 6 7 8 9 10 11 12 13 type Result struct { Name string Age int } var result Result db.Table(\u0026#34;users\u0026#34;).Select(\u0026#34;name, age\u0026#34;).Where(\u0026#34;name = ?\u0026#34;, \u0026#34;Antonio\u0026#34;).Scan(\u0026amp;result) var results []Result db.Table(\u0026#34;users\u0026#34;).Select(\u0026#34;name, age\u0026#34;).Where(\u0026#34;id \u0026gt; ?\u0026#34;, 0).Scan(\u0026amp;results) // 原生 SQL db.Raw(\u0026#34;SELECT name, age FROM users WHERE name = ?\u0026#34;, \u0026#34;Antonio\u0026#34;).Scan(\u0026amp;result) 链式操作相关 链式操作 Method Chaining，Gorm 实现了链式操作接口，所以你可以把代码写成这样：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // 创建一个查询 tx := db.Where(\u0026#34;name = ?\u0026#34;, \u0026#34;jinzhu\u0026#34;) // 添加更多条件 if someCondition { tx = tx.Where(\u0026#34;age = ?\u0026#34;, 20) } else { tx = tx.Where(\u0026#34;age = ?\u0026#34;, 30) } if yetAnotherCondition { tx = tx.Where(\u0026#34;active = ?\u0026#34;, 1) } 在调用立即执行方法前不会生成 Query语句，借助这个特性你可以创建一个函数来处理一些通用逻辑。\n立即执行方法 Immediate methods ，立即执行方法是指那些会立即生成 SQL语句并发送到数据库的方法, 他们一般是 CRUD方法，比如：\nCreate, First, Find, Take, Save, UpdateXXX, Delete, Scan, Row, Rows…\n这有一个基于上面链式方法代码的立即执行方法的例子：\n1 tx.Find(\u0026amp;user) 生成的SQL语句如下：\n1 SELECT * FROM users where name = \u0026#39;jinzhu\u0026#39; AND age = 30 AND active = 1; 范围 Scopes，Scope是建立在链式操作的基础之上的。\n基于它，你可以抽取一些通用逻辑，写出更多可重用的函数库。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 func AmountGreaterThan1000(db *gorm.DB) *gorm.DB { return db.Where(\u0026#34;amount \u0026gt; ?\u0026#34;, 1000) } func PaidWithCreditCard(db *gorm.DB) *gorm.DB { return db.Where(\u0026#34;pay_mode_sign = ?\u0026#34;, \u0026#34;C\u0026#34;) } func PaidWithCod(db *gorm.DB) *gorm.DB { return db.Where(\u0026#34;pay_mode_sign = ?\u0026#34;, \u0026#34;C\u0026#34;) } func OrderStatus(status []string) func (db *gorm.DB) *gorm.DB { return func (db *gorm.DB) *gorm.DB { return db.Scopes(AmountGreaterThan1000).Where(\u0026#34;status IN (?)\u0026#34;, status) } } db.Scopes(AmountGreaterThan1000, PaidWithCreditCard).Find(\u0026amp;orders) // 查找所有金额大于 1000 的信用卡订单 db.Scopes(AmountGreaterThan1000, PaidWithCod).Find(\u0026amp;orders) // 查找所有金额大于 1000 的 COD 订单 db.Scopes(AmountGreaterThan1000, OrderStatus([]string{\u0026#34;paid\u0026#34;, \u0026#34;shipped\u0026#34;})).Find(\u0026amp;orders) // 查找所有金额大于 1000 且已付款或者已发货的订单 多个立即执行方法 Multiple Immediate Methods，在 GORM 中使用多个立即执行方法时，后一个立即执行方法会复用前一个立即执行方法的条件 (不包括内联条件) 。\n1 db.Where(\u0026#34;name LIKE ?\u0026#34;, \u0026#34;jinzhu%\u0026#34;).Find(\u0026amp;users, \u0026#34;id IN (?)\u0026#34;, []int{1, 2, 3}).Count(\u0026amp;count) 生成的 Sql\n1 2 3 SELECT * FROM users WHERE name LIKE \u0026#39;jinzhu%\u0026#39; AND id IN (1, 2, 3) SELECT count(*) FROM users WHERE name LIKE \u0026#39;jinzhu%\u0026#39; 更新 更新所有字段 Save()默认会根据指定的主键值查询并更新该对象的所有字段，即使你没有赋值。\n如果save时没有指定主键值，或者根据主键值没有查询到数据，那么更新会变成插入新数据，这个点一定要注意\n1 2 3 4 5 6 7 8 db.First(\u0026amp;user) user.ID = 1 user.Name = \u0026#34;七米\u0026#34; user.Age = 99 db.Save(\u0026amp;user) //// UPDATE `users` SET `created_at` = \u0026#39;2020-02-16 12:52:20\u0026#39;, `updated_at` = \u0026#39;2020-02-16 12:54:55\u0026#39;, `deleted_at` = NULL, `name` = \u0026#39;七米\u0026#39;, `age` = 99, `active` = true WHERE `users`.`deleted_at` IS NULL AND `users`.`id` = 1 更新修改字段 如果你只希望更新指定字段，可以使用 Update(更新单个字段)或者 Updates(更新多个字段)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // 更新单个属性，如果它有变化 db.Model(\u0026amp;user).Update(\u0026#34;name\u0026#34;, \u0026#34;hello\u0026#34;) //// UPDATE users SET name=\u0026#39;hello\u0026#39;, updated_at=\u0026#39;2013-11-17 21:34:10\u0026#39; WHERE id=111; // 根据给定的条件更新单个属性 db.Model(\u0026amp;user).Where(\u0026#34;active = ?\u0026#34;, true).Update(\u0026#34;name\u0026#34;, \u0026#34;hello\u0026#34;) //// UPDATE users SET name=\u0026#39;hello\u0026#39;, updated_at=\u0026#39;2013-11-17 21:34:10\u0026#39; WHERE id=111 AND active=true; // 使用 map 更新多个属性，只会更新其中有变化的属性 db.Model(\u0026amp;user).Updates(map[string]interface{}{\u0026#34;name\u0026#34;: \u0026#34;hello\u0026#34;, \u0026#34;age\u0026#34;: 18, \u0026#34;active\u0026#34;: false}) //// UPDATE users SET name=\u0026#39;hello\u0026#39;, age=18, active=false, updated_at=\u0026#39;2013-11-17 21:34:10\u0026#39; WHERE id=111; // 使用 struct 更新多个属性，只会更新其中有变化且为非零值的字段 db.Model(\u0026amp;user).Updates(User{Name: \u0026#34;hello\u0026#34;, Age: 18}) //// UPDATE users SET name=\u0026#39;hello\u0026#39;, age=18, updated_at = \u0026#39;2013-11-17 21:34:10\u0026#39; WHERE id = 111; // 警告：当使用 struct 更新时，GORM只会更新那些非零值的字段 // 对于下面的操作，不会发生任何更新，\u0026#34;\u0026#34;, 0, false 都是其类型的零值 db.Model(\u0026amp;user).Updates(User{Name: \u0026#34;\u0026#34;, Age: 0, Active: false}) 更新选定字段 如果你想更新或忽略某些字段，你可以使用 Select，Omit\n1 2 3 4 5 db.Model(\u0026amp;user).Select(\u0026#34;name\u0026#34;).Updates(map[string]interface{}{\u0026#34;name\u0026#34;: \u0026#34;hello\u0026#34;, \u0026#34;age\u0026#34;: 18, \u0026#34;active\u0026#34;: false}) //// UPDATE users SET name=\u0026#39;hello\u0026#39;, updated_at=\u0026#39;2013-11-17 21:34:10\u0026#39; WHERE id=111; db.Model(\u0026amp;user).Omit(\u0026#34;name\u0026#34;).Updates(map[string]interface{}{\u0026#34;name\u0026#34;: \u0026#34;hello\u0026#34;, \u0026#34;age\u0026#34;: 18, \u0026#34;active\u0026#34;: false}) //// UPDATE users SET age=18, active=false, updated_at=\u0026#39;2013-11-17 21:34:10\u0026#39; WHERE id=111; 无Hooks更新(指定单一或多个字段更新，不更新钩子函数) 上面的更新操作会自动运行 model 的 BeforeUpdate, AfterUpdate 方法，更新 UpdatedAt 时间戳, 在更新时保存其 Associations, 如果你不想调用这些方法，你可以使用 UpdateColumn， UpdateColumns\n1 2 3 4 5 6 7 // 更新单个属性，类似于 `Update` db.Model(\u0026amp;user).UpdateColumn(\u0026#34;name\u0026#34;, \u0026#34;hello\u0026#34;) //// UPDATE users SET name=\u0026#39;hello\u0026#39; WHERE id = 111; // 更新多个属性，类似于 `Updates` db.Model(\u0026amp;user).UpdateColumns(User{Name: \u0026#34;hello\u0026#34;, Age: 18}) //// UPDATE users SET name=\u0026#39;hello\u0026#39;, age=18 WHERE id = 111; 批量更新 批量更新时 Hooks（钩子函数）不会运行。\n1 2 3 4 5 6 7 8 9 db.Table(\u0026#34;users\u0026#34;).Where(\u0026#34;id IN (?)\u0026#34;, []int{10, 11}).Updates(map[string]interface{}{\u0026#34;name\u0026#34;: \u0026#34;hello\u0026#34;, \u0026#34;age\u0026#34;: 18}) //// UPDATE users SET name=\u0026#39;hello\u0026#39;, age=18 WHERE id IN (10, 11); // 使用 struct 更新时，只会更新非零值字段，若想更新所有字段，请使用map[string]interface{}，map可以将零值携带进数据库 db.Model(\u0026amp;User{}).Updates(User{Name: \u0026#34;hello\u0026#34;, Age: 18}) //// UPDATE users SET name=\u0026#39;hello\u0026#39;, age=18; // 使用 `RowsAffected` 获取当前操作更新影响的行数 db.Model(\u0026amp;User{}).Updates(User{Name: \u0026#34;hello\u0026#34;, Age: 18}).RowsAffected 使用SQL表达式更新 先查询表中的第一条数据保存至user变量。\n1 2 var user User db.First(\u0026amp;user) 1 2 3 4 5 6 7 8 9 10 11 db.Model(\u0026amp;user).Update(\u0026#34;age\u0026#34;, gorm.Expr(\u0026#34;age * ? + ?\u0026#34;, 2, 100)) //// UPDATE `users` SET `age` = age * 2 + 100, `updated_at` = \u0026#39;2020-02-16 13:10:20\u0026#39; WHERE `users`.`id` = 1; db.Model(\u0026amp;user).Updates(map[string]interface{}{\u0026#34;age\u0026#34;: gorm.Expr(\u0026#34;age * ? + ?\u0026#34;, 2, 100)}) //// UPDATE \u0026#34;users\u0026#34; SET \u0026#34;age\u0026#34; = age * \u0026#39;2\u0026#39; + \u0026#39;100\u0026#39;, \u0026#34;updated_at\u0026#34; = \u0026#39;2020-02-16 13:05:51\u0026#39; WHERE `users`.`id` = 1; db.Model(\u0026amp;user).UpdateColumn(\u0026#34;age\u0026#34;, gorm.Expr(\u0026#34;age - ?\u0026#34;, 1)) //// UPDATE \u0026#34;users\u0026#34; SET \u0026#34;age\u0026#34; = age - 1 WHERE \u0026#34;id\u0026#34; = \u0026#39;1\u0026#39;; db.Model(\u0026amp;user).Where(\u0026#34;age \u0026gt; 10\u0026#34;).UpdateColumn(\u0026#34;age\u0026#34;, gorm.Expr(\u0026#34;age - ?\u0026#34;, 1)) //// UPDATE \u0026#34;users\u0026#34; SET \u0026#34;age\u0026#34; = age - 1 WHERE \u0026#34;id\u0026#34; = \u0026#39;1\u0026#39; AND age \u0026gt; 10; 修改Hooks中的值 如果你想修改 BeforeUpdate, BeforeSave 等 Hooks 中更新的值，你可以使用 scope.SetColumn, 例如：\n1 2 3 4 5 6 7 //加密明文，用于用户密码设置 func (user *User) BeforeSave(scope *gorm.Scope) (err error) { //GenerateFromPassword: 将明文密码加密，然后通过SetColumn存入数据库 if pw, err := bcrypt.GenerateFromPassword(user.Password, 0); err == nil { scope.SetColumn(\u0026#34;EncryptedPassword\u0026#34;, pw) } } 其它更新选项(针对SQL Server) 1 2 3 // 为 update SQL 添加其它的 SQL db.Model(\u0026amp;user).Set(\u0026#34;gorm:update_option\u0026#34;, \u0026#34;OPTION (OPTIMIZE FOR UNKNOWN)\u0026#34;).Update(\u0026#34;name\u0026#34;, \u0026#34;hello\u0026#34;) //// UPDATE users SET name=\u0026#39;hello\u0026#39;, updated_at = \u0026#39;2013-11-17 21:34:10\u0026#39; WHERE id=111 OPTION (OPTIMIZE FOR UNKNOWN); 删除 删除记录（只针对通过ID主键进行删除） 警告 ：删除记录时，请确保主键字段有值，GORM 会通过主键去删除记录，如果主键为空，GORM 会删除该 model 的所有记录（表中所有数据都会被删除！！！）。\n1 2 3 4 5 6 7 // 删除现有记录 db.Delete(\u0026amp;email) //// DELETE from emails where id=10; // 为删除 SQL 添加额外的 SQL 操作 db.Set(\u0026#34;gorm:delete_option\u0026#34;, \u0026#34;OPTION (OPTIMIZE FOR UNKNOWN)\u0026#34;).Delete(\u0026amp;email) //// DELETE from emails where id=10 OPTION (OPTIMIZE FOR UNKNOWN); 批量删除 删除全部匹配的记录\n1 2 3 4 5 db.Where(\u0026#34;email LIKE ?\u0026#34;, \u0026#34;%jinzhu%\u0026#34;).Delete(Email{}) //// DELETE from emails where email LIKE \u0026#34;%jinzhu%\u0026#34;; db.Delete(Email{}, \u0026#34;email LIKE ?\u0026#34;, \u0026#34;%jinzhu%\u0026#34;) //// DELETE from emails where email LIKE \u0026#34;%jinzhu%\u0026#34;; 软删除（当没有使用gorm.Model构建结构体时，删除直接就是物理删除） 如果一个 model 有 DeletedAt 字段，他将自动获得软删除的功能！ 当调用 Delete 方法时， 记录不会真正的从数据库中被删除， 只会将 DeletedAt 字段的值会被设置为当前时间\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 db.Delete(\u0026amp;user) //// UPDATE users SET deleted_at=\u0026#34;2013-10-29 10:23\u0026#34; WHERE id = 111; // 批量删除 db.Where(\u0026#34;age = ?\u0026#34;, 20).Delete(\u0026amp;User{}) //// UPDATE users SET deleted_at=\u0026#34;2013-10-29 10:23\u0026#34; WHERE age = 20; // 查询记录时会忽略被软删除的记录 db.Where(\u0026#34;age = 20\u0026#34;).Find(\u0026amp;user) //// SELECT * FROM users WHERE age = 20 AND deleted_at IS NULL; // Unscoped 方法可以查询被软删除的记录 db.Unscoped().Where(\u0026#34;age = 20\u0026#34;).Find(\u0026amp;users) //// SELECT * FROM users WHERE age = 20; 物理删除（用gorm.Model构建结构体时） 1 2 3 // Unscoped 方法可以物理删除记录 db.Unscoped().Delete(\u0026amp;order) //// DELETE FROM orders WHERE id=10; DB.Where().Find().RecordNotFound()判断查询结果是否存在\n","date":"2021-01-15T00:00:00Z","permalink":"https://xiaonuoz.github.io/p/gorm%E6%A1%86%E6%9E%B6%E5%88%9D%E8%AF%86/","title":"GORM框架初识"},{"content":"Git基础 安装调配 安装好git后先配置user信息\n1 2 git config --global user.name \u0026#39;your_name\u0026#39; git config --global user.email \u0026#39;your_email\u0026#39; config有三个作用域，local（缺省，只对某个仓库有效）、global（对当前用户所有仓库有效）、system（对系统所有登录的用户有效），在作用域之后加 \u0026ndash;list可以显示config相关作用域的配置。local优先级大于global，global优先级大于system\n建立git仓库提交代码 如果是将原有代码纳入git管理就只需要进入项目所在文件夹，然后 git init即可\n对于新的项目则是进入某个文件夹，然后 git init your_project(此命令会在当前路径下创建和项目名称同名的文件夹)，然后进入 your_project\ngit提交流程：\n先通过 git add files将文件从工作目录添加到暂存区（可以将暂存区的东西重新回退覆盖到工作目录），多次add后觉得没问题了再通过 git commit -m '提交信息'提交到git中。\ngit status 查看暂存区和工作目录的状态。\ngit log查看git的版本历史，可以使用 git log --oneline来简洁查看版本历史。历史过多使用空格可以向下翻页，b 向上翻页\n对于git已经跟踪过的文件（存入暂存区的文件），可以通过 git add -u 将工作目录中所有被git管理的文件一起添加到暂存区。got rm file删除暂存区中的文件\n1 2 3 4 5 6 7 8 9 10 如果要将本地git和GitHub连接起来，首先要在GitHub建一个仓库，然后关联本地和GitHub,此处是用orgin来代替后面一长串的地址，类似别名。现在可以直接clone拉取远端代码，它会自动帮我们起origin的别名，并且初始化本地库 git remote add orgin git@github.com..../..git // git remote -v 查看连接上的仓库 如果GitHub上的项目中有readme或者license文件，则需要先pull,pull会将本地的master和远端的master相关联，并且拉取远端的文件 git pull orgin //git clone git@github.com:.... 将远端的git项目拷贝到本地当前文件夹中 然后将本地项目push到远端，可以接分支名只push一个分支，如果Git连接名是orgin是可以缺省的 git push \u0026lt;远程主机名\u0026gt; \u0026lt;本地分支名\u0026gt;:\u0026lt;远程分支名\u0026gt; 常用命令基础 git reset --hard xxxhash 将git的指针指向某次commit后，即回退到之前某个版本，这个版本之后的commit都会消失，此时可以以当前版本创建新的分支或者tag，但千万不要进行git push ，会导致远端之后的版本消失。此时本地的commit对比远端commit应该会落后几个版本，创建完分支或tag后可以通过git pull重新同步本地commit 变回去。\ngit reset --soft xxx 仅仅在本地库中移动HEAD指针，不会改变本地真实文件的内容，暂存区依然是之前最新版本的内容，可以理解为撤销了这个commit之后的所有提交，本地库和真实代码的差异放在了暂存区中。\u0026ndash;mixed则是移动了指针和暂存区，真实内容不会更改，即差异化出现在了工作区中。\ngit mv old_name new_name对git跟踪后的文件进行重命名，同时会更改本地文件名\ngir rm file 删除暂存区和工作区中对应的文件\ngit branch -av 查看有多少个分支，信息前的*符号表示目前工作在哪个分支\ngit branch 分支名 创建一个以当前分支内容相同的新分支\ngit checkout master 切换分支到master\ngit checkout -b newBranchName branchOrCommit 创建一个基于某分支或commit的新分支并切换到分支上\ngit branch -d 分支名 删除分支\ngit merge 分支名a 合并分支，将分支a的内容合并到当前指针指向的分支，但a分支还是存在的，可以继续向前迭代，因此在合并前一定要checkout到a分支外那个合并的目的分支\ngit diff --cached 比较暂存区和当前所在分支最新commit（HEAD）的差异，只输入 git diff是比较工作区和暂存区的差异\ngit reset HEAD -- \u0026lt;fileName\u0026gt; 将暂存区恢复成当前所在分支的最新commit（HEAD），通常用在暂存区有东西后，工作区有更优的方案了，也可以在HEAD后加fileName来恢复部分文件\ngit checkout -- \u0026lt;file\u0026gt;将工作区的某个文件内容回退恢复成暂存区中该文件的内容\ngit commit --amend 对最近一次提交的commit的message作变更\ngit rebase -i 父commit，如果要修改某个commit的属性，可以通过此命令更改，它的参数是对应修改commit的父commit，输入后会进入一个交互页面，里面有此父commit的所有子commit，如要修改子commit的message，则将pick改成reword(或者r)，改完后保存退出则会跳到对应的修改页面，修改message然后保存退出即可，修改后对应commit的哈希码会因此发生改变。改变的原因是git会在分离头指针状态下剪切子commit，并根据命令（pick则是只剪切，reword是剪切但修改message）进行操作，最后子commit的哈希码都会发生改变。\ngit rebase其中还有一个命令是 [s,squash]，它可以将多个分支合并。即找出需要合并的多个commit的最前面的一个的它的父commit，在一条分支上都是父传子一直穿下去，找出父commit后它会显示出后面所有的子commit，使用rebase命令进入交互页面，在需要合并的多个commit中，最后一个子commit使用pick命令只剪切，前面的commit使用squash合并到pick的那个子commit中。如果合并的commit在历史版本中不连续，可以通过rebase的交互页面，通过调换位置的方法放在一起（剪切粘贴），依然是第一行pick，后面squash，如果是需要合并最古老的commit（没有父级），在rebase的交互页面添加对应的commit即可\ngit stash 假如在开发中偶遇临时任务，如正在修改index文件，修改到一半临时加塞了另一个任务，可以通过此命令先将工作区和暂存区状态存到一个堆栈中，且工作区和暂存区会回退到最新一次commit，改完临时任务之后，再通过 git stash pop(取出堆栈内容且清除堆栈中的对应内容)或 git stash appluy(取出堆栈内容但保存堆栈对应内容)\ngti中有三大objects：commit、tree及blob，它们的关系是，commit是每一次提交，它里面含有tree的信息，tree表示了commit在当时的时间点的项目中文件的结构，它可以包含其他tree（就像文件夹下还有文件夹），tree保存了blob的哈希码，blob存放了文件的内容。这里要注意的是，在git中，即使文件名不同，只要文件内容相同，那么它们就是同一个文件，并不会因为名字不同而产生两个文件，这样大大节约了存储空间\n分支的本质就是一个指针，它指向的是该分支中最新的一次提交commit\ngit cat-file -p 7737016481f通过此命令可以查看commit、tree、blob的内容，-t查看对应的类型\n分离头指针 如果不小心通过git checkout命令切换到某个commit中（即HEAD指向某个commit），git会提示我们正处于分离头指针的状态下（工作在没有分支的情况下），如果我们做了大量的修改，但是某天我们突然又切换到另一个commit时，我们的修改就有可能被git当做垃圾清除掉，因此这个动作十分危险。\n**分离头指针的用处：进行尝试性的变更时。**当我们试着修改某些文件时，如果觉得修改的效果不满意，可以直接切换到别的分支，丢弃当前修改。如果我们切换出去时，觉得当前的修改十分重要，就按照git的提示创建一个新分支与此commit进行绑定 git branch 新分支名 commit哈希码，如果某个变更（提交）是非常重要的，那么一定要跟某个分支绑定在一起\n多人开发 千万不要在master分支上进行代码的迭代，master是作为服务器运行分支\nGit Flow工作流\n主干分支 master：主要负责管理正在运行的生产环境代码。永远保持与正在运行的生产环境完全一致。 开发分支 develop ： 主要负责管理正在开发过程中的代码。一般情况下应该是最新的代码。程序员从这个分支中迁出新分支进行功能开发 功能分支 feature：为了不影响短周期的开发工作，一般把中长期开发模块从开发分支中独立出来。开发完成后再合并到最新的开发分支中。 bug 修复 分支 hotfix [热修复] ：主要负责管理生产环境下出现的紧急修复的代码。从主干分支分出，修复完毕并测试上线后，合并回主干分支。并会后可视情况删除该分支。 预发布分支 release ： 较大的版本上线前，会从开发分支中分出预发布分支，用于进行最后阶段的集成测试。该版本上线后会合并到主干分支上。 在多人开发的情况下，经常会出现同时有多人操作同一个项目的不同文件，当其他人push后，本地的git就不是远端最新的commit了，此时如果不在意线性分支，可以使用 git merge github/commitName将远端最新的和本地进行关联，commitName可以通过 git branch -av查看。此时本地就拉取了最新的commit，再进行push就不会出错了。\n当出现两人修改相同文件的不同位置时，某一方提交后，另一方使用 git pull会拉取并和远端关联，此时这个文件有对方修改的数据，且自己修改的数据不会消失\n当两人同时修改相同文件的相同位置，一方提交后，另一方pull会发生冲突，提示某文件内容有问题，此时打开对应文件就能发现远端和本地的差异\n当两人同时修改同一文件时，一方修改了该文件的文件名并提交，另一方在旧文件名上修改内容并提交就会出错，此时只需要使用 git pull 拉取远端最新的，git会将改名后的commit拉取过来，并且本地修改过的旧文件名中的内容会出现在新文件名内容中。\n当两人同时对同一个文件修改文件名，一方提交后另一方pull就会报错，此时本地会出现两个不同名字的同一文件，需要经过协商删除某个或保留，然后通过 add--commit---push的流程重新提交一个commit\n千万不要在多人开发时使用 git push -f ，这会导致历史commit部分丢失，也不要对公共分支进行变基（Rebase）操作，例如修改某个commit的message，这会导致commit的id改变，原commit分叉成新的分支，如果有其他人基于原commit进行开发，两个分支就会分叉，重新合成需要多人merge关联新的commit\n普通的搜索是匹配仓库名和描述，可以在搜索栏中加入 in:readme即表示在readme中搜索关键字，加入 stars:\u0026gt;1000表示筛选出星星数超过一千。根据内容搜索则是 '文件内容' filename:文件名，如 'struct' filename:server.go\n在GitHub中对分支集成有三种策略\nCreate a merge commit 是将某分支的最后一个提交合并到目标分支中，生成一个新的commit。就是把提交历史原封不动的拷贝过来，包含完整的提交历史记录。\nSquash and merge 压缩提交，有一些很小的提交，或者是纠正前面的错误的提交，对于这类提交对整个工程来说不需要单独显示出来一次提交，不然导致项目的提交历史过于复杂；所以基于这种原因，我们可以把Beijing上的所有提交都合并成一个提交；然后提交到主干。\nRebase and merge ,由于squash merge会变更提交者作者信息，这是一个很大的问题，后期问题追溯不好处理(当然也可以由分支的所有者来执行squash merge操作，以解决部分问题)，rebase merge可以保留提交的作者信息，同时可以合并commit历史。\n如果想对github上开源的项目进行修改并提给作者，可以通过fork将此项目复制一份到自己仓库中(直接拉取的是没有写的权限的)，拉取下来修改好提交上去，然后创建一个pull request发送给原作者。原作者看见同意后merge到自己的仓库中。\n找github资源的小技巧： 常用前缀后缀：\n找百科大全: awesome golang 针对某个技术的所有东西，很杂很丰富 找例子： gin sample 找相应项目的例子，商城或者其他项目样例 找空架子项目：go-gin starter/go boilerplate 找相应技术的空脚架 找技术教程：go tutorial 找相应教程 ","date":"2020-12-03T00:00:00Z","permalink":"https://xiaonuoz.github.io/p/git%E4%B8%89%E5%89%91%E5%AE%A2/","title":"Git三剑客"},{"content":"Elasticsearch 介绍 Elasticsearch（ES）是一个基于Lucene构建的开源、分布式、RESTful接口的全文搜索引擎。Elasticsearch还是一个分布式文档数据库，其中每个字段均可被索引，而且每个字段的数据均可被搜索，ES能够横向扩展至数以百计的服务器存储以及处理PB级的数据。可以在极短的时间内存储、搜索和分析大量的数据。通常作为具有复杂搜索场景情况下的核心发动机。\nElasticsearch能做什么 当你经营一家网上商店，你可以让你的客户搜索你卖的商品。在这种情况下，你可以使用ElasticSearch来存储你的整个产品目录和库存信息，为客户提供精准搜索，可以为客户推荐相关商品。 当你想收集日志或者交易数据的时候，需要分析和挖掘这些数据，寻找趋势，进行统计，总结，或发现异常。在这种情况下，你可以使用Logstash或者其他工具来进行收集数据，当这引起数据存储到ElasticsSearch中。你可以搜索和汇总这些数据，找到任何你感兴趣的信息。 对于程序员来说，比较有名的案例是GitHub，GitHub的搜索是基于ElasticSearch构建的，在github.com/search页面，你可以搜索项目、用户、issue、pull request，还有代码。共有40~50个索引库，分别用于索引网站需要跟踪的各种数据。虽然只索引项目的主分支（master），但这个数据量依然巨大，包括20亿个索引文档，30TB的索引文件。 Elasticsearch基本概念 Near Realtime(NRT) 几乎实时 Elasticsearch是一个几乎实时的搜索平台。意思是，从索引一个文档到这个文档可被搜索只需要一点点的延迟，这个时间一般为毫秒级。\nCluster 集群 群集是一个或多个节点（服务器）的集合， 这些节点共同保存整个数据，并在所有节点上提供联合索引和搜索功能。一个集群由一个唯一集群ID确定，并指定一个集群名（默认为“elasticsearch”）。该集群名非常重要，因为节点可以通过这个集群名加入群集，一个节点只能是群集的一部分。\n确保在不同的环境中不要使用相同的群集名称，否则可能会导致连接错误的群集节点。例如，你可以使用logging-dev、logging-stage、logging-prod分别为开发、阶段产品、生产集群做记录。\nNode节点 节点是单个服务器实例，它是群集的一部分，可以存储数据，并参与群集的索引和搜索功能。就像一个集群，节点的名称默认为一个随机的通用唯一标识符（UUID），确定在启动时分配给该节点。如果不希望默认，可以定义任何节点名。这个名字对管理很重要，目的是要确定你的网络服务器对应于你的ElasticSearch群集节点。\n我们可以通过群集名配置节点以连接特定的群集。默认情况下，每个节点设置加入名为“elasticSearch”的集群。这意味着如果你启动多个节点在网络上，假设他们能发现彼此都会自动形成和加入一个名为“elasticsearch”的集群。\n在单个群集中，你可以拥有尽可能多的节点。此外，如果“elasticsearch”在同一个网络中，没有其他节点正在运行，从单个节点的默认情况下会形成一个新的单节点名为”elasticsearch”的集群。\nIndex索引 索引是具有相似特性的文档集合。例如，可以为客户数据提供索引，为产品目录建立另一个索引，以及为订单数据建立另一个索引。索引由名称（必须全部为小写）标识，该名称用于在对其中的文档执行索引、搜索、更新和删除操作时引用索引。在单个群集中，你可以定义尽可能多的索引。\nType类型 在索引中，可以定义一个或多个类型。类型是索引的逻辑类别/分区，其语义完全取决于你。一般来说，类型定义为具有公共字段集的文档。例如，假设你运行一个博客平台，并将所有数据存储在一个索引中。在这个索引中，你可以为用户数据定义一种类型，为博客数据定义另一种类型，以及为注释数据定义另一类型。\nDocument文档 文档是可以被索引的信息的基本单位。例如，你可以为单个客户提供一个文档，单个产品提供另一个文档，以及单个订单提供另一个文档。本文件的表示形式为JSON（JavaScript Object Notation）格式，这是一种非常普遍的互联网数据交换格式。\n在索引/类型中，你可以存储尽可能多的文档。请注意，尽管文档物理驻留在索引中，文档实际上必须索引或分配到索引中的类型。\nShards \u0026amp; Replicas分片与副本 索引可以存储大量的数据，这些数据可能超过单个节点的硬件限制。例如，十亿个文件占用磁盘空间1TB的单指标可能不适合对单个节点的磁盘或可能太慢服务仅从单个节点的搜索请求。\n为了解决这一问题，Elasticsearch提供细分你的指标分成多个块称为分片的能力。当你创建一个索引，你可以简单地定义你想要的分片数量。每个分片本身是一个全功能的、独立的“指数”，可以托管在集群中的任何节点。\nShards分片的重要性主要体现在以下两个特征：\n分片允许你水平拆分或缩放内容的大小 分片允许你分配和并行操作的碎片（可能在多个节点上）从而提高性能/吞吐量 这个机制中的碎片是分布式的以及其文件汇总到搜索请求是完全由ElasticSearch管理，对用户来说是透明的。 在同一个集群网络或云环境上，故障是任何时候都会出现的，拥有一个故障转移机制以防分片和节点因为某些原因离线或消失是非常有用的，并且被强烈推荐。为此，Elasticsearch允许你创建一个或多个拷贝，你的索引分片进入所谓的副本或称作复制品的分片，简称Replicas。\nReplicas的重要性主要体现在以下两个特征：\n副本为分片或节点失败提供了高可用性。为此，需要注意的是，一个副本的分片不会分配在同一个节点作为原始的或主分片，副本是从主分片那里复制过来的。 副本允许用户扩展你的搜索量或吞吐量，因为搜索可以在所有副本上并行执行。 ES基本概念与关系型数据库的比较 ES API 以下示例使用 curl演示。（也可以使用Postman来进行创建删除等，更加智能）\n查看健康状态 1 curl -X GET 127.0.0.1:9200/_cat/health?v 输出：\n1 2 epoch timestamp cluster status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent 1564726309 06:11:49 elasticsearch yellow 1 1 3 3 0 0 1 0 - 75.0% 查询当前es集群中所有的indices 1 curl -X GET 127.0.0.1:9200/_cat/indices?v 输出：\n1 2 3 4 health status index uuid pri rep docs.count docs.deleted store.size pri.store.size green open .kibana_task_manager LUo-IxjDQdWeAbR-SYuYvQ 1 0 2 0 45.5kb 45.5kb green open .kibana_1 PLvyZV1bRDWex05xkOrNNg 1 0 4 1 23.9kb 23.9kb yellow open user o42mIpDeSgSWZ6eARWUfKw 1 1 0 0 283b 283b 创建索引 1 curl -X PUT 127.0.0.1:9200/www 输出：\n1 {\u0026#34;acknowledged\u0026#34;:true,\u0026#34;shards_acknowledged\u0026#34;:true,\u0026#34;index\u0026#34;:\u0026#34;www\u0026#34;} 删除索引 1 curl -X DELETE 127.0.0.1:9200/www 输出：\n1 {\u0026#34;acknowledged\u0026#34;:true} 插入记录 1 2 3 4 5 6 curl -H \u0026#34;ContentType:application/json\u0026#34; -X POST 127.0.0.1:9200/user/person -d \u0026#39; { \u0026#34;name\u0026#34;: \u0026#34;dsb\u0026#34;, \u0026#34;age\u0026#34;: 9000, \u0026#34;married\u0026#34;: true }\u0026#39; 输出：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \u0026#34;_index\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;person\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;MLcwUWwBvEa8j5UrLZj4\u0026#34;, \u0026#34;_version\u0026#34;: 1, \u0026#34;result\u0026#34;: \u0026#34;created\u0026#34;, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 2, \u0026#34;successful\u0026#34;: 1, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;_seq_no\u0026#34;: 3, \u0026#34;_primary_term\u0026#34;: 1 } 也可以使用PUT方法，但是需要传入id\n1 2 3 4 5 6 curl -H \u0026#34;ContentType:application/json\u0026#34; -X PUT 127.0.0.1:9200/user/person/4 -d \u0026#39; { \u0026#34;name\u0026#34;: \u0026#34;sb\u0026#34;, \u0026#34;age\u0026#34;: 9, \u0026#34;married\u0026#34;: false }\u0026#39; 检索 Elasticsearch的检索语法比较特别，使用GET方法携带JSON格式的查询条件。\n全检索：\n1 curl -X GET 127.0.0.1:9200/user/person/_search 按条件检索：\n1 2 3 4 5 6 curl -H \u0026#34;ContentType:application/json\u0026#34; -X PUT 127.0.0.1:9200/user/person/4 -d \u0026#39; { \u0026#34;query\u0026#34;:{ \u0026#34;match\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;sb\u0026#34;} } }\u0026#39; ElasticSearch默认一次最多返回10条结果，可以像下面的示例通过size字段来设置返回结果的数目。\n1 2 3 4 5 6 7 curl -H \u0026#34;ContentType:application/json\u0026#34; -X PUT 127.0.0.1:9200/user/person/4 -d \u0026#39; { \u0026#34;query\u0026#34;:{ \u0026#34;match\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;xx\u0026#34;}, \u0026#34;size\u0026#34;: 2 } }\u0026#39; 也可以通过Postman:\nGo操作Elasticsearch elastic client 我们使用第三方库https://github.com/olivere/elastic来连接ES并进行操作。\n注意下载与你的ES相同版本的client，例如我们这里使用的ES是7.2.1的版本，那么我们下载的client也要与之对应为 github.com/olivere/elastic/v7。\n使用 go.mod来管理依赖：\n1 2 3 require ( github.com/olivere/elastic/v7 v7.0.4 ) 简单示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;github.com/olivere/elastic/v7\u0026#34; ) // Elasticsearch demo type Person struct { Name string `json:\u0026#34;name\u0026#34;` Age int `json:\u0026#34;age\u0026#34;` Married bool `json:\u0026#34;married\u0026#34;` } func main() { client, err := elastic.NewClient(elastic.SetURL(\u0026#34;http://192.168.1.7:9200\u0026#34;)) if err != nil { // Handle error panic(err) } fmt.Println(\u0026#34;connect to es success\u0026#34;) p1 := Person{Name: \u0026#34;xx\u0026#34;, Age: 22, Married: false} //这里第一个Index默认存在，第二个是索引，第三个是类型，第四个是将结构体反序列化成json格式然后do进es,但是我们是传结构体进去，它会自动转换成json //v7版本后已弃用.Type(\u0026#34;xxx\u0026#34;)，现在默认的type为_doc put1, err := client.Index(). Index(\u0026#34;user\u0026#34;). BodyJson(p1). Do(context.Background()) if err != nil { // Handle error panic(err) } fmt.Printf(\u0026#34;Indexed user %s to index %s, type %s\\n\u0026#34;, put1.Id, put1.Index, put1.Type) } ","date":"2020-11-20T00:00:00Z","permalink":"https://xiaonuoz.github.io/p/go%E6%93%8D%E4%BD%9Celasticsearch/","title":"Go操作Elasticsearch"},{"content":"gin的路由 gin框架中采用的路由库是基于httprouter做的，httprouter会将所有路由规则构造成一颗前缀树\n例如有root and as at cn com，这样子查询效率会比传统的KV模式要快\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 // 1. 创建路由，默认用了Logger(), Recovery()两个中间件 r:=gin.Default() // 2. 绑定路由规则GET r.GET(\u0026#34;/user/:name/*action\u0026#34;, func(c *gin.Context) { //取API参数 name:=c.Param(\u0026#34;name\u0026#34;) ac:=c.Param(\u0026#34;action\u0026#34;) c.String(http.StatusOK,name+\u0026#34; is \u0026#34;+ac) }) // 2. 取URL参数 r.GET(\u0026#34;/test\u0026#34;, func(c *gin.Context) { //URL参数可以通过DefaultQuery()或Query()方法获取 //DefaultQuery()若参数不存在，返回一个设定的默认值，Query()若不存在是返回空字符串 name:=c.DefaultQuery(\u0026#34;name\u0026#34;,\u0026#34;没有此变量\u0026#34;) c.String(http.StatusOK,name) }) // 3. 监听相应ip和端口，ip为空则为本地 r.Run(\u0026#34;:8000\u0026#34;) 上传文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 //限制表单上传大小为8MB，gin框架默认限制为32MB r.MaxMultipartMemory = 8 \u0026lt;\u0026lt; 20 // 可以获取客户端传来的表单参数 r.POST(\u0026#34;/form\u0026#34;, func(c *gin.Context) { //表单参数可以设置当参数不存在时的默认值 c.DefaultPostForm(\u0026#34;type\u0026#34;,\u0026#34;alert\u0026#34;) //接受有此变量的参数,若变量不存在则返回空字符串 name:=c.PostForm(\u0026#34;username\u0026#34;) password:=c.PostForm(\u0026#34;password\u0026#34;) //多选框用PostFormArray()可以以切片的格式接受数据 hobbys:=c.PostFormArray(\u0026#34;hobby\u0026#34;) c.String(http.StatusOK,fmt.Sprintf(\u0026#34;name is %s,\\nusername is %s,\\nhobby is %v\u0026#34;,name,password,hobbys)) //在前端表单中，enctype为multipart/form-data则可以发送文件，同时也能发送文字 //存单个图片 //file, _ := c.FormFile(\u0026#34;file\u0026#34;) //// 存到项目根目录下，名字不变 //c.SaveUploadedFile(file,file.Filename) //c.String(http.StatusOK,file.Filename+\u0026#34;upload success\u0026#34;) //存多个图片,上传多个文件的话前端的input标签要加 multiple form, err := c.MultipartForm() if err != nil { c.String(http.StatusBadRequest,\u0026#34;upload file faild,err:\u0026#34;+err.Error()) } files:=form.File[\u0026#34;files\u0026#34;] for _,file:=range files{ err := c.SaveUploadedFile(file, file.Filename) if err != nil { c.String(http.StatusBadRequest,\u0026#34;upload file faild,err:\u0026#34;+err.Error()) return } } c.String(200,fmt.Sprintf(\u0026#34;upload file success,%v file\u0026#34;,len(files))) }) routes group（路由组） 路由组是为了管理一些相同的URL，进行规范\n1 2 3 4 5 6 7 8 9 10 11 12 // 2. 集中处理请求,{}花括号去掉也不会报错，这是一个代码书写规范 // 使用组的时候访问地址就是 address:port/v1/login v1:=r.Group(\u0026#34;/v1\u0026#34;) { v1.GET(\u0026#34;/login\u0026#34;,login) v1.GET(\u0026#34;/submit\u0026#34;,submit) } v2:=r.Group(\u0026#34;/v2\u0026#34;) { v2.GET(\u0026#34;/add\u0026#34;,add) v2.GET(\u0026#34;/deltel\u0026#34;,deltel) } gin数据解析和绑定 json数据解析和绑定 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 type Login struct { //binding:\u0026#34;required\u0026#34;为前端的必选字段，如果接受到的值是空值则会报错 User string\t`json:\u0026#34;user\u0026#34; binding:\u0026#34;required\u0026#34;` Password string\t`json:\u0026#34;password\u0026#34; binding:\u0026#34;required\u0026#34;` } func main() { // 1. 创建路由 r:=gin.Default() // 2. JSON绑定 r.POST(\u0026#34;/loginJson\u0026#34;, func(c *gin.Context) { var login Login //将request的body中的数据自动按照json格式解析并绑定到结构体, //当客户端传输的数据为json格式的内容时，结构体的tag要定义json格式,且绑定的方法为ShouldBindJSON(\u0026amp;login) err := c.ShouldBindJSON(\u0026amp;login) if err != nil { //返回错误信息，且返回的内容是一个json格式 //gin.H封装了生成json数据的工具 c.JSON(http.StatusBadRequest,gin.H{\u0026#34;error\u0026#34;:err.Error()}) return } c.JSON(http.StatusOK,login.User) }) // 3. 监听相应ip和端口，ip为空则为本地 r.Run(\u0026#34;:8000\u0026#34;) } 表单数据解析和绑定 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 type Login struct { User string\t`form:\u0026#34;username\u0026#34;` Password string\t`form:\u0026#34;password\u0026#34;` } func main() { // 1. 创建路由 r:=gin.Default() // 2. JSON绑定 r.POST(\u0026#34;/loginForm\u0026#34;, func(c *gin.Context) { var login Login //Bind()默认解析并绑定form格式 //根据请求头中的content-type自动推断 err := c.Bind(\u0026amp;login) if err != nil { //返回错误信息，且返回的内容是一个json格式 //gin.H封装了生成json数据的工具 c.JSON(http.StatusBadRequest,gin.H{\u0026#34;error\u0026#34;:err.Error(),\u0026#34;status\u0026#34;:\u0026#34;400\u0026#34;}) return } c.JSON(http.StatusOK,login.User) }) // 3. 监听相应ip和端口，ip为空则为本地 r.Run(\u0026#34;:8000\u0026#34;) } 1 2 3 4 5 \u0026lt;form action=\u0026#34;http://127.0.0.1:8000/loginForm\u0026#34; method=\u0026#34;post\u0026#34; enctype=\u0026#34;multipart/form-data\u0026#34;\u0026gt; 用户名：\u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;username\u0026#34; \u0026gt;\u0026lt;br\u0026gt; 密\u0026amp;nbsp\u0026amp;nbsp码：\u0026lt;input type=\u0026#34;password\u0026#34; name=\u0026#34;password\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;submit\u0026#34; value=\u0026#34;登录\u0026#34;\u0026gt; \u0026lt;/form\u0026gt; URI数据解析和绑定 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 type Login struct { User string\t`uri:\u0026#34;username\u0026#34;` Password string\t`uri:\u0026#34;password\u0026#34;` } func main() { // 1. 创建路由 r:=gin.Default() // 2. JSON绑定 r.GET(\u0026#34;/loginUri/:username/:password\u0026#34;, func(c *gin.Context) { var login Login err := c.ShouldBindUri(\u0026amp;login) if err != nil { //返回错误信息，且返回的内容是一个json格式 //gin.H封装了生成json数据的工具 c.JSON(http.StatusBadRequest,gin.H{\u0026#34;error\u0026#34;:err.Error(),\u0026#34;status\u0026#34;:\u0026#34;400\u0026#34;}) return } c.JSON(http.StatusOK,login.User+\u0026#34;----\u0026#34;+login.Password) }) // 3. 监听相应ip和端口，ip为空则为本地 r.Run(\u0026#34;:8000\u0026#34;) } 使用 curl http://127.0.0.1:8000/loginUri/root/admin 可以测试是否成功，root就是用户名，admin是密码\ngin渲染 各种数据格式的响应 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 //1. json r.GET(\u0026#34;/someJSON\u0026#34;, func(c *gin.Context) { c.JSON(http.StatusOK,gin.H{\u0026#34;message\u0026#34;:\u0026#34;someJSON\u0026#34;,\u0026#34;status\u0026#34;:200}) }) //2. 结构体 r.GET(\u0026#34;/someStruct\u0026#34;, func(c *gin.Context) { var msg =struct{ Name string Message string Number int }{\u0026#34;root\u0026#34;,\u0026#34;message\u0026#34;,123} c.JSON(http.StatusOK,msg) }) //3. XML r.GET(\u0026#34;/someXML\u0026#34;, func(c *gin.Context) { c.XML(200,gin.H{\u0026#34;message\u0026#34;:\u0026#34;abc\u0026#34;}) }) //4. YAML响应 r.GET(\u0026#34;/someYAML\u0026#34;, func(c *gin.Context) { c.YAML(http.StatusOK,gin.H{\u0026#34;name\u0026#34;:\u0026#34;zhangsan\u0026#34;}) }) //5. protobuf格式,谷歌开发的高效存储读取的工具 r.GET(\u0026#34;/someProtoBuf\u0026#34;, func(c *gin.Context) { resp:=[]int64{int64(1),int64(9)} //定义数据 label:=\u0026#34;lable\u0026#34; //构建protobuf格式数据 data:=\u0026amp;protoexample.Test{ Label: \u0026amp;label, Reps: resp, } c.ProtoBuf(http.StatusOK,data) }) HTML模板渲染 gin支持加载HTML模板，然后根据模板参数进行配置并返回响应的数据，本质上就是字符串替换 LoadHTMLGlob()方法可以加载模板文件(加载路径下的所有或部分文件) 1 2 3 4 5 6 7 //加载模板文件,r.LoadHTMLFiles()是匹配文件，r.LoadHTMLGlob()匹配路径 r.LoadHTMLGlob(\u0026#34;templates/*\u0026#34;) r.GET(\u0026#34;/index\u0026#34;, func(c *gin.Context) { //根据文件名渲染 //加载模板是加载的路径，替换的是文件中的某个变量 c.HTML(200,\u0026#34;index.tmpl\u0026#34;,gin.H{\u0026#34;title\u0026#34;:\u0026#34;标题\u0026#34;}) }) index.tmpl:\n1 2 3 4 5 \u0026lt;html\u0026gt; \u0026lt;h1\u0026gt; {{.title}} \u0026lt;/h1\u0026gt; \u0026lt;/html\u0026gt; 重定向 1 2 3 4 r.GET(\u0026#34;/redirect\u0026#34;, func(c *gin.Context) { //支持内部和外部重定向 c.Redirect(http.StatusMovedPermanently,\u0026#34;http://www.baidu.com/\u0026#34;) }) 同步异步 goroutine机制可以方便的实现异步处理 在启动新的goroutine时，不能使用原始上下文，必须使用它的只读副本（gin框架要求的） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 //异步，瞬间结束，goroutine在后台继续执行 r.GET(\u0026#34;/async\u0026#34;, func(c *gin.Context) { //必须先复制一个副本 copyContext:=c.Copy() //需要注意的是这里开了一个goroutine在后台执行后，即使GET结束了依然在执行，因为GET不是主函数，即使结束了也不会影响到goroutine go func() { time.Sleep(3*time.Second) log.Println(\u0026#34;异步执行:\u0026#34;+copyContext.Request.URL.Path) }() //可以开多个goroutine且共用一个context副本 go func() { time.Sleep(3*time.Second) log.Println(\u0026#34;异步执行:\u0026#34;+copyContext.Request.URL.Path) }() }) //同步，需等待3S才会执行结束 r.GET(\u0026#34;/sync\u0026#34;, func(c *gin.Context) { time.Sleep(3 * time.Second) log.Println(\u0026#34;同步执行:\u0026#34; + c.Request.URL.Path) }) gin中间件 gin可以构建中间件，但它只对注册过的路由函数起作用 对于分组路由，嵌套使用中间件可以限定中间件的作用范围 中间件分为全局中间件，单个路由中间件和群组中间件 gin中间件必须是一个gin.HandlerFunc类型，中间件就类似于拦截器，例如作用于某些页面需要用户登录，先在中间件进行判断，判断成功后再进行真正的业务处理 中间件在处理用户请求之前会执行，之后也会执行 全局中间件 所有请求都经过此中间件，可以定义多个中间件，执行顺序是依次顺序执行。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 func MiddleWare() gin.HandlerFunc{ return func(c *gin.Context) { t:=time.Now() //在GET之前执行 fmt.Println(\u0026#34;中间件执行了\u0026#34;) //设置变量到context（本质上是把一个值设置到request里面去），通过get()取 c.Set(\u0026#34;name\u0026#34;,\u0026#34;zhangsan\u0026#34;) //执行函数（发送数据到request），Next()就是遍历一遍c.handlers，然后把所有的c.handlers（GET/POST的第二个函数参数）都执行 c.Next() //下面是在response响应之前GET执行之后进行的操作 //当执行完c.Next()后，request之后GET之前的中间件代码就已经结束了，下面的就都是response之前要执行的代码 status:=c.Writer.Status() fmt.Println(status) //统计从请求过来到发送response之间的耗时 t1:=time.Since(t) fmt.Println(\u0026#34;耗时：\u0026#34;,t1) } } func main() { // 1. 创建路由，默认用了Logger(), Recovery()两个中间件 r:=gin.Default() //注册中间件 r.Use(MiddleWare()) //{}是为了代码规范 { r.GET(\u0026#34;/middleware\u0026#34;, func(c *gin.Context) { //取值,GET返回值有两个，一个获取到的值和一个是否值存在的布尔值 value,_:=c.Get(\u0026#34;name\u0026#34;) c.JSON(200,gin.H{\u0026#34;name\u0026#34;:value}) }) } // 3. 监听相应ip和端口，ip为空则为本地 r.Run(\u0026#34;:8000\u0026#34;) 当执行完c.Next()后，request之后GET之前的中间件代码就已经结束了，下面的就都是response之前要执行的代码\n在中间件中可以设定三种代码： Next()、return、Abort()\nNext(): 表示跳过当前中间件的剩余内容，去执行下一个中间件。当所有操作执行完之后，再以出栈的顺序执行剩余代码。类似于一个划分，Next()之前的代码是在客户端发起请求后服务器顺序执行的，之后的代码是在服务器response之后 客户端收到数据之前倒叙执行的。就像函数调用栈的形式，中间件1的Next()指向了中间件2，中间件2的Next()部分指向了具体的handlerfunc,当具体func执行完后才会执行中间件2的剩余代码，以此类推。 return:终止执行当前中间件的剩余内容，转而执行下一个中间件。且所有函数执行完后不会执行return之后的代码。如上图中把中间件2的next换成return后的打印顺序是1-2-3-5。 4在return之后，不会发生执行。如函数调用栈中某个子函数在执行一半时return，这个子函数会立即返回结束。 Abort(): 不再执行Abort()之后的中间件，在Abort()之前的代码执行完后开始向客户端响应，先执行Abort之下的代码，然后倒叙向客户端响应。依然如函数调用栈，Abort()的调用表示不会再调用任何子函数，执行完有Abort的当前函数后层层返回，执行上级中间件next()后的剩余代码。 局部中间件 1 2 3 4 5 6 7 8 9 10 11 //注册中间件 r.Use(MiddleWare()) //{}是为了代码规范 { //根路由后面是定义的局部中间件 r.GET(\u0026#34;/middleware\u0026#34;,MiddleWare(), func(c *gin.Context) { //取值,GET返回值有两个，一个获取到的值和一个是否值存在的布尔值 value,_:=c.Get(\u0026#34;name\u0026#34;) c.JSON(200,gin.H{\u0026#34;name\u0026#34;:value}) }) } 会话控制 Cookie是什么 HTTP是无状态协议，服务器不能记录浏览器的访问状态，也就是说服务器不能区分两次请求是否由同一个客户端发出 Cookie就是解决HTTP协议无状态的方案之一，它实际上就是服务器保存在浏览器上的一段信息，浏览器有了Cookie之后，每次向服务器发送请求都会同时将该信息发送给服务器，服务器收到请求后就可以根据该信息处理请求 Cookie由服务器创建，并发送给浏览器，最终由浏览器保存 Cookie的用途 保持用户登录状态 电商网站的购物车（京东），商品添加到cookie，无需登录，关闭浏览器重新打开依然存在 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // 服务端给客户端cookie r.GET(\u0026#34;cookie\u0026#34;, func(c *gin.Context) { //获取客户端是否携带了cookie cookie, err := c.Cookie(\u0026#34;key_cookie\u0026#34;) if err != nil { cookie =\u0026#34;Not Set\u0026#34; //给客户端设置cookie，随着response一起返回给浏览器 //maxAge int,cookie的存在时间，单位为秒，超过时间cookie会过期,为0时会跟着会话结束而结束 //path,cookie在浏览器中的所在目录，默认为“”，让浏览器自己分配 //domain ,域名，通常设置为“”即可 //secure,是否只能通过https访问 //httpOnly,是否允许别人通过js获取自己的cookie c.SetCookie(\u0026#34;key_cookie\u0026#34;,\u0026#34;value_cookie\u0026#34;,60,\u0026#34;\u0026#34;,\u0026#34;\u0026#34;,false,true) } //第一次是NotSet，刷新页面重新发送请求后则是value_cookie fmt.Println(cookie) }) 示例：用中间件判断用户登录\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 func cookieGin() gin.HandlerFunc{ return func(c *gin.Context) { cookie, err := c.Cookie(\u0026#34;name\u0026#34;) if err == nil { if cookie == \u0026#34;zhangsan\u0026#34;{ c.Next() return } } //返回错误 c.JSON(http.StatusBadRequest,gin.H{\u0026#34;error\u0026#34;:\u0026#34;StatusUnauthorized\u0026#34;}) //c.Abort()的作用是不再调用后续的函数处理,直接response c.Abort() return } } func main() { r:=gin.Default() r.GET(\u0026#34;/login\u0026#34;, func(c *gin.Context) { c.SetCookie(\u0026#34;name\u0026#34;,\u0026#34;zhangsan\u0026#34;,60,\u0026#34;/\u0026#34;,\u0026#34;localhost\u0026#34;,false,true) }) //不进行login直接home就会被中间件拦截 r.GET(\u0026#34;/home\u0026#34;, cookieGin(), func(c *gin.Context) { cookie, _ := c.Cookie(\u0026#34;name\u0026#34;) c.JSON(200,gin.H{\u0026#34;name\u0026#34;:cookie}) }) r.Run(\u0026#34;:8000\u0026#34;) } Cookie的缺点 不安全，明文的（加密了也不太安全） 增加了带宽消耗 可以被禁用（禁用之后功能就不再完善） cookie有上限 session session它可以弥补cookie的不足。session必须依赖于cookie才能使用。它在服务端生成一个SessionId，然后放在cookie里传给客户端，cookie本身改存到了服务端。\nsession常用的是临时session，用来记录用户的状态，当浏览器关闭时session也会消失。下次打开浏览器就需要重新存入。\n在三次握手阶段，客户端第一次握手时请求服务器，服务器产生cookie。\n然后在第二次握手之前服务器将这个cookie加密作为key，生成session为value，然后存在服务器设置的容器中。第二次握手服务器将cookie明文发给客户端，客户端决定是否存储。\n第三次握手客户端发送cookie过去，服务器加密cookie并以这个cookie去查找是否有对应的session。\ngin-contrib : gin框架的各种插件中间件，如session在gin中是默认不支持的，可以通过这个包中的session来给gin安装，使用中间件的方式。\nhttps://github.com/gin-contrib/sessions session在gin中的使用\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 package main import ( \u0026#34;github.com/gin-contrib/sessions\u0026#34; \u0026#34;github.com/gin-contrib/sessions/redis\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; ) func main() { r := gin.Default() store, _ := redis.NewStore(10, \u0026#34;tcp\u0026#34;, \u0026#34;localhost:6379\u0026#34;, \u0026#34;\u0026#34;, []byte(\u0026#34;secret\u0026#34;)) // \u0026#34;mysession\u0026#34;是cookie的名称，即key,在setCookie中的name等于的内容 r.Use(sessions.Sessions(\u0026#34;mysession\u0026#34;, store)) r.GET(\u0026#34;/incr\u0026#34;, func(c *gin.Context) { session := sessions.Default(c) var count int v := session.Get(\u0026#34;count\u0026#34;) if v == nil { count = 0 } else { count = v.(int) count++ } session.Set(\u0026#34;count\u0026#34;, count) session.Save() c.JSON(200, gin.H{\u0026#34;count\u0026#34;: count}) }) r.Run(\u0026#34;:8000\u0026#34;) } 比如存储到redis中，session会在redis中以session_开头加cookie的value的加密值来拼接一个redis的key: session_UHNOMHSNWKEJWSJIOPY2AQI7VCE3P2SMDV4LGSFNHJWPCJFM72ZQ，且该用户的所有的session.set操作都会放到这一个key中[无论有多少个session，都不会创建新的redis-key，仅在这一个key中进行拼接存储]，获取时会先获取这里面的所有值，因为里面存的是key-value的格式，它获取所有值后再通过key来获取具体的session-value\n总结下来session的流程是客户端请求，然后服务器返回cookie，同时将这个cookie的value值加密组装存到redis或其他容器中（不取name的原因是name在不同主机中值是相同的），然后返回cookie的name和value等给客户端，下次客户端访问就可以将这个cookie的name和value传给服务器，服务器通过将value加密然后查询是否有对应的session值\n一定要注意的是每个浏览器(开无痕也会出现新的cookie)都会有一个唯一的cookie来存储它自己的session，因此在这个cookie使用session.Del(\u0026ldquo;key\u0026rdquo;) 只会删除当前cookie中存的session值，不会影响到其他的key-session。所以在实际应用中同一主机不同浏览器存储的session是不同的，因为两者是不同的cookie，redis中指向的是不同的key。如在Chrome中登录了淘宝，Edge中并不会直接显示你已经登录了，你需要重新登录，他们属于两个不同的session。\n一定要记得，并不是每一个会话都有一个单独的session,session取决于这个cookie是否更改过，关闭此浏览器再重新打开这个网页后，虽然会话是新的，但是其cookie还是原本的值，在redis中依然存在。只有换一个浏览器(或开启无痕模式)或者 删除cookie让其重新生成新的cookie 才能在redis中生成新的session记录。\nset和delete操作后一定要进行save操作。\noptions可以设置这个session存在的时间，当其为0时表示在会话结束时就会消失。重新打开浏览器session就会失效。\n1 2 3 store.Options(sessions.Options{ MaxAge: 10, }) Session中间件开发 设计一个通用的session服务，支持内存存储和redis存储\nsession模块设计\n本质上k-v系统，通过key进行增删改查 session可以存储在内存或者redis中 https://github.com/XiaoNuoZ/session_cookie_Middleware\n模板引擎的使用 在一些前后端不分离的Web架构中，我们通常需要在后端将一些数据渲染到HTML文档中，从而实现动态的网页（网页的布局和样式大致一样，但展示的内容并不一样）效果。\n我们这里说的模板可以理解为事先定义好的HTML文档文件，模板渲染的作用机制可以简单理解为文本替换操作–使用相应的数据去替换HTML文档中事先准备好的标记。\nGo语言内置了文本模板引擎 text/template和用于HTML文档的 html/template。它们的作用机制可以简单归纳如下：\n模板文件通常定义为 .tmpl和 .tpl为后缀（也可以使用其他的后缀），必须使用 UTF8编码。 模板文件中使用 {{和 }}包裹和标识需要传入的数据。 传给模板这样的数据就可以通过点号（.）来访问，如果数据是复杂类型的数据，可以通过{ { .FieldName }}来访问它的字段。 除 {{和 }}包裹的内容外，其他内容均不做修改原样输出。 Go语言模板引擎的使用可以分为三部分：定义模板文件、解析模板文件和模板渲染.\n其中，定义模板文件时需要我们按照相关语法规则去编写，就是在html内容中加入{{ . }}来进行接收数据\n上面定义好了模板文件之后，可以使用下面的常用方法去解析模板文件，得到模板对象：\n1 2 3 func (t *Template) Parse(src string) (*Template, error) func ParseFiles(filenames ...string) (*Template, error) func ParseGlob(pattern string) (*Template, error) 渲染模板简单来说就是使用数据去填充模板。\n1 2 func (t *Template) Execute(wr io.Writer, data interface{}) error func (t *Template) ExecuteTemplate(wr io.Writer, name string, data interface{}) error 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 func sayHello(w http.ResponseWriter,r *http.Request){ //解析模板 t,_:=template.ParseFiles(\u0026#34;./hello.html\u0026#34;) //也可以使用t,_:=template.New(\u0026#34;hello.html\u0026#34;).ParseFiles(\u0026#34;./hello.html\u0026#34;) //一定要注意的就是New中的参数一定要和模板文件名字对应上，不然解析会失败 //如果ParseFiles传了很多个文件，那么至少要和其中一个对应上 //需要注意的是，结构体首字母必须大写才能在html中接收到，但是map不在乎大小写 u1:=User{ Name: \u0026#34;笑傩\u0026#34;, Age: 18, Address: \u0026#34;四川\u0026#34;, } m1:=map[string]interface{}{ \u0026#34;Name\u0026#34;: \u0026#34;笑傩\u0026#34;, \u0026#34;Age\u0026#34;: 18, \u0026#34;Address\u0026#34;: \u0026#34;四川\u0026#34;, } //渲染模板，通过嵌套的方法可以传过去多个数据 t.Execute(w,map[string]interface{}{ \u0026#34;u1\u0026#34;:u1, \u0026#34;m1\u0026#34;:m1, }) } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Hello {{.u1.Name}}\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;Age {{.u1.Age}}\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;{{.m1.Name}}\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;Age {{.m1.Age}}\u0026lt;/p\u0026gt; //with可以设定一个局部作用域，在此作用域中.代表.m1 {{with .m1}} \u0026lt;p\u0026gt;{{.Name}}\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;Age {{.Age}}\u0026lt;/p\u0026gt; {{end}} {{index .slice 2}} //取切片中索引为2的值 \u0026lt;/body\u0026gt; 变量 还可以在模板中声明变量，用来保存传入模板的数据或其他语句生成的结果。具体语法如下：\n1 2 {{ $v1 := 100}} {{ $age := .m1.age }} 其中 $age是变量的名字，在后续的代码中就可以使用该变量了。\n有时候我们在使用模板语法的时候会不可避免的引入一下空格或者换行符，这样模板最终渲染出来的内容可能就和我们想的不一样，这个时候可以使用 {{-语法去除模板内容左侧的所有空白符号， 使用 -}}去除模板内容右侧的所有空白符号。{{- .Name -}}, -要紧挨 {{和 }}，同时与模板值之间需要使用空格分隔。\n自定义函数 Go的模板支持自定义函数。\n1 2 3 4 5 6 7 //定义一个自定义的函数，返回值要么有一个，要么有两个且第二个返回值必须是error类型 kua := func(name string,address string)(string,error) { return name+\u0026#34;测试\u0026#34;+address,nil } //解析模板，采用链式操作在Parse之前调用Funcs添加自定义的kua函数，把kua存入FuncMap中 //添加自定义函数一定要在解析模板之前 t,_:=template.New(\u0026#34;hello.html\u0026#34;).Funcs(template.FuncMap{\u0026#34;kua\u0026#34;:kua}).ParseFiles(\u0026#34;./hello.html\u0026#34;) 然后在前端文件中则是通过 {{kua .Name .Address}} 来调用，模板也有管道符 {{ .Name | kua }}，意思是将Name的值作为后面函数的参数\n嵌套template 可以在template中嵌套其他的template。这个template可以是单独的文件，也可以是通过 define定义的template。\nt.tmpl文件内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;zh-CN\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;meta http-equiv=\u0026#34;X-UA-Compatible\u0026#34; content=\u0026#34;ie=edge\u0026#34;\u0026gt; \u0026lt;title\u0026gt;tmpl test\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;测试嵌套template语法\u0026lt;/h1\u0026gt; \u0026lt;hr\u0026gt; {{template \u0026#34;ul.tmpl\u0026#34;}} \u0026lt;hr\u0026gt; {{template \u0026#34;ol.tmpl\u0026#34;}} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; {{ define \u0026#34;ol.tmpl\u0026#34;}} \u0026lt;ol\u0026gt; \u0026lt;li\u0026gt;吃饭\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;睡觉\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;打豆豆\u0026lt;/li\u0026gt; \u0026lt;/ol\u0026gt; {{end}} ul.tmpl文件内容如下：\n1 2 3 4 5 \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;注释\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;日志\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;测试\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; 路由处理函数如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 func tmplDemo(w http.ResponseWriter, r *http.Request) { tmpl, err := template.ParseFiles(\u0026#34;./t.tmpl\u0026#34;, \u0026#34;./ul.tmpl\u0026#34;) if err != nil { fmt.Println(\u0026#34;create template failed, err:\u0026#34;, err) return } user := UserInfo{ Name: \u0026#34;笑傩\u0026#34;, Gender: \u0026#34;男\u0026#34;, Age: 18, } tmpl.Execute(w, user) } 注意：在解析模板时，被嵌套的模板一定要在后面解析，例如上面的示例中 t.tmpl模板中嵌套了 ul.tmpl，所以 ul.tmpl要在 t.tmpl后进行解析。\nblock定义块模板 1 {{block \u0026#34;name\u0026#34; pipeline}} T1 {{end}} block是定义模板 {{define \u0026quot;name\u0026quot;}} T1 {{end}}和执行 {{template \u0026quot;name\u0026quot; pipeline}}缩写，典型的用法是定义一组根模板，然后通过在其中重新定义块模板进行自定义。\n定义一个根模板 templates/base.tmpl，内容如下：\n1 2 3 4 5 6 7 8 9 10 11 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;zh-CN\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Go Templates\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div class=\u0026#34;container-fluid\u0026#34;\u0026gt; {{block \u0026#34;content\u0026#34; . }}{{end}} \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 然后定义一个 templates/index.tmpl，”继承”base.tmpl，然后重新定义区块 content，在区块中写入不同于其他网页的内容即可，这样就可以实现多个页面用同一个根模板，仅部分数据不同：\n1 2 3 4 5 6 7 //一定要在最后加上在根模板定义的pipeline，表示从根模板把数据拿过来 {{template \u0026#34;base.tmpl\u0026#34; .}} {{define \u0026#34;content\u0026#34;}} \u0026lt;div\u0026gt;Hello world!\u0026lt;/div\u0026gt; \u0026lt;p\u0026gt;Hello {{ . }}\u0026lt;/p\u0026gt; {{end}} 然后使用 template.ParseGlob按照正则匹配规则解析模板文件，然后通过 ExecuteTemplate渲染指定的模板：\n1 2 3 4 5 6 7 8 func index(w http.ResponseWriter, r *http.Request){ //也可以使用template.ParseFiles(\u0026#34;./templates/根模板\u0026#34;,\u0026#34;./templates/继承模板\u0026#34;) //根模板一定要写在前面 tmpl, _ := template.ParseGlob(\u0026#34;templates/*.tmpl\u0026#34;) //使用ExecuteTemplate的原因是前面解析了两个模板，以前使用Execute是因为只渲染一个模板，不需要指定也知道渲染哪个模板，现在有两个则必须要进行指定 name := \u0026#34;笑傩\u0026#34; tmpl.ExecuteTemplate(w, \u0026#34;index.tmpl\u0026#34;, name) } 如果我们的模板名称冲突了，例如不同业务线下都定义了一个 index.tmpl模板，我们可以通过下面两种方法来解决。\n在模板文件开头使用 {{define 模板名}}语句显式的为模板命名（不写默认将文件名作为模板名）。 可以把模板文件存放在 templates文件夹下面的不同目录中，然后使用 template.ParseGlob(\u0026quot;templates/**/*.tmpl\u0026quot;)解析模板。 修改默认的标识符 Go标准库的模板引擎使用的花括号 {{和 }}作为标识，而许多前端框架（如 Vue和 AngularJS）也使用 {{和 }}作为标识符，所以当我们同时使用Go语言模板引擎和以上前端框架时就会出现冲突，这个时候我们需要修改标识符，修改前端的或者修改Go语言的。这里演示如何修改Go语言模板引擎默认的标识符：\n1 2 template.New(\u0026#34;test\u0026#34;).Delims(\u0026#34;{[\u0026#34;, \u0026#34;]}\u0026#34;).ParseFiles(\u0026#34;./t.tmpl\u0026#34;) 然后在模板中就可以通过 {[ .name ]} 来引用数据 text/template与html/template的区别 html/template针对的是需要返回HTML内容的场景，在模板渲染过程中会对一些有风险的内容进行转义，以此来防范跨站脚本攻击。比如在输入框中输入sql语句造成拖库或者死循环，html/template可以对内容进行转移，保证输出的html内容都是安全的。但是在某些场景下，我们如果相信用户输入的内容，不想转义的话，可以自行编写一个safe函数，手动返回一个 template.HTML类型的内容，template.HTML可以将用户输入的内容转成html。\ngin框架渲染tmpl模板 Gin框架中使用 LoadHTMLGlob() 加载文件夹或者 LoadHTMLFiles() 加载单一文件夹 来进行HTML模板渲染。Glob通过正则表达式进行匹配，如 templates/**/*表示templates下的任意文件夹中的任意文件，不能省略，它代表任意文件夹。 templates/*是表示templates下的任意文件，它加载不到该文件夹下的某个文件夹中的文件，必须加上**。\ntmpl文件中最好在内容前加上define定义，避免在未来不同文件夹中有同名文件\n1 2 3 {{define \u0026#34;posts/index.html\u0026#34;}} ...... {{end}} 1 2 3 4 5 r.LoadHTMLGlob(\u0026#34;templates/**/*\u0026#34;) r.GET(\u0026#34;index\u0026#34;, func(c *gin.Context){ //第二个参数是defin定义的名字，如果没有则默认为文件名（即使是在二级目录templates/posts/下也只是仅有文件名，因此需要define定义） c.HTML(http.StatusOK,\u0026#34;index.html\u0026#34;,\u0026#34;zhangsan\u0026#34;) }) 自定义模板函数 定义一个不转义相应内容的 safe模板函数如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 func main() { router := gin.Default() router.SetFuncMap(template.FuncMap{ \u0026#34;safe\u0026#34;: func(str string) template.HTML{ return template.HTML(str) }, }) router.LoadHTMLFiles(\u0026#34;./index.tmpl\u0026#34;) router.GET(\u0026#34;/index\u0026#34;, func(c *gin.Context) { c.HTML(http.StatusOK, \u0026#34;index.tmpl\u0026#34;, \u0026#34;\u0026lt;a href=\u0026#39;https://baidu.com\u0026#39;\u0026gt;baidu\u0026lt;/a\u0026gt;\u0026#34;) }) router.Run(\u0026#34;:8080\u0026#34;) } 在 index.tmpl中使用定义好的 safe模板函数：\n1 2 3 4 5 6 7 8 9 10 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;zh-CN\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;修改模板引擎的标识符\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; //此处通过管道将取到的数据作为safe函数的参数 \u0026lt;div\u0026gt;{{ . | safe }}\u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 静态文件处理 当我们渲染的HTML文件中引用了静态文件时，我们只需要按照以下方式在渲染页面前调用 gin.Static方法即可。Static方法要在加载模板文件之前调用。第一个参数为在html文件中引用的目录 href=\u0026quot;/static/index.css\u0026quot;，第二个为本地静态文件存放路径\n1 2 3 4 5 6 func main() { r := gin.Default() r.Static(\u0026#34;/static\u0026#34;, \u0026#34;./statics\u0026#34;) r.LoadHTMLGlob(\u0026#34;templates/**/*\u0026#34;) r.Run(\u0026#34;:8080\u0026#34;) } 关于模板文件和静态文件的路径，需要根据公司/项目的要求进行设置。可以使用下面的函数获取当前执行程序的路径。\n1 2 3 4 5 6 func getCurrentPath() string { if ex, err := os.Executable(); err == nil { return filepath.Dir(ex) } return \u0026#34;./\u0026#34; } JSON渲染、XML渲染和YAML渲染 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 r.GET(\u0026#34;/someJSON\u0026#34;, func(c *gin.Context) { // 方式一：自己拼接JSON c.JSON(http.StatusOK, gin.H{\u0026#34;message\u0026#34;: \u0026#34;Hello world!\u0026#34;}) c.XML(http.StatusOK, gin.H{\u0026#34;message\u0026#34;: \u0026#34;Hello world!\u0026#34;}) c.YAML(http.StatusOK, gin.H{\u0026#34;message\u0026#34;: \u0026#34;ok\u0026#34;, \u0026#34;status\u0026#34;: http.StatusOK}) }) r.GET(\u0026#34;/moreJSON\u0026#34;, func(c *gin.Context) { // 方法二：使用结构体 type msg struct { Name string `json:\u0026#34;user\u0026#34;` Message string Age int } msg.Name = \u0026#34;小王子\u0026#34; msg.Message = \u0026#34;Hello world!\u0026#34; msg.Age = 18 //返回的数据中Name在前端显示为user c.JSON(http.StatusOK, msg) c.XML(http.StatusOK, msg) }) 获取参数 获取querystring参数 querystring指的是URL中 ?后面携带的参数，例如：/user/search?username=笑傩。 获取请求的querystring参数的方法如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 r.GET(\u0026#34;/user/search\u0026#34;, func(c *gin.Context) { //没取到值则为第二个参数 username := c.DefaultQuery(\u0026#34;username\u0026#34;, \u0026#34;小王子\u0026#34;) //没取到username为零值 username := c.Query(\u0026#34;username\u0026#34;) //没取到ok的值为false name,ok:=c.GetQuery(\u0026#34;name\u0026#34;) if !ok { name=\u0026#34;no name\u0026#34; } //输出json结果给调用方 c.JSON(http.StatusOK, gin.H{ \u0026#34;username\u0026#34;: username, }) }) 获取form参数 当前端请求的数据通过form表单提交时，例如向 /user/search发送一个POST请求，获取请求数据的方式如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 r.POST(\u0026#34;/user/search\u0026#34;, func(c *gin.Context) { // DefaultPostForm取不到值时会返回指定的默认值,但是如果前端是输入框，则用户不填时默认为空字符串，表示永远无法返回默认值 username:=c.DefaultPostForm(\u0026#34;xxx\u0026#34;,\u0026#34;something\u0026#34;) //getPostform取不到则返回false username,ok:=c.GetPostForm(\u0026#34;xxx\u0026#34;) if !ok{ username=\u0026#34;somebody\u0026#34; } username := c.PostForm(\u0026#34;username\u0026#34;) address := c.PostForm(\u0026#34;address\u0026#34;) //输出json结果给调用方 c.JSON(http.StatusOK, gin.H{ \u0026#34;message\u0026#34;: \u0026#34;ok\u0026#34;, \u0026#34;username\u0026#34;: username, \u0026#34;address\u0026#34;: address, }) }) 获取path参数 请求的参数通过URL路径传递，例如：/user/search/xiaoNuo/abc。 获取请求URL路径中的参数的方式如下。\n1 2 3 4 5 6 7 8 9 10 11 r.GET(\u0026#34;/user/search/:username/:address\u0026#34;, func(c *gin.Context) { //获取URI参数就只有一个Param方法，通常用在博客日期统计，如blog/19/03 username := c.Param(\u0026#34;username\u0026#34;) address := c.Param(\u0026#34;address\u0026#34;) //输出json结果给调用方 c.JSON(http.StatusOK, gin.H{ \u0026#34;message\u0026#34;: \u0026#34;ok\u0026#34;, \u0026#34;username\u0026#34;: username, \u0026#34;address\u0026#34;: address, }) }) 参数绑定 为了能够更方便的获取请求相关参数，提高开发效率，可以基于请求的 Content-Type识别请求数据类型并利用反射机制自动提取请求中 QueryString、form表单、JSON、XML等参数到结构体中。 下面的示例代码演示了 .ShouldBind()强大的功能，它能够基于请求自动提取 JSON、form表单和 QueryString类型的数据(一个方法提取多种类型)，并把值绑定到指定的结构体对象。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 // Binding from JSON type Login struct { //binding:\u0026#34;required\u0026#34;为前端的必选字段，如果接受到的值是空值则会报错,不加则表示该字段前端可以为空 //通常只需要写一个form就可以同时获取到GET POST以及json格式的数据，但如果在form的基础上加了json的tag，则发往后端的json名必须和tag定义的相同，不加则以form为准 //这个方法是通过反射实现，因此结构体名和字段名皆必须为大写 User string `form:\u0026#34;user\u0026#34; json:\u0026#34;user\u0026#34; binding:\u0026#34;required\u0026#34;` Password string `form:\u0026#34;password\u0026#34; json:\u0026#34;password\u0026#34; binding:\u0026#34;required\u0026#34;` } func main() { router := gin.Default() // 绑定JSON的示例 router.POST(\u0026#34;/loginJSON\u0026#34;, func(c *gin.Context) { var login Login if err := c.ShouldBind(\u0026amp;login); err == nil { fmt.Printf(\u0026#34;login info:%#v\\n\u0026#34;, login) c.JSON(http.StatusOK, gin.H{ \u0026#34;user\u0026#34;: login.User, \u0026#34;password\u0026#34;: login.Password, }) } else { c.JSON(http.StatusBadRequest, gin.H{\u0026#34;error\u0026#34;: err.Error()}) } }) // 绑定form表单示例 router.POST(\u0026#34;/loginForm\u0026#34;, func(c *gin.Context) { var login Login // ShouldBind()会根据请求的Content-Type自行选择绑定器 if err := c.ShouldBind(\u0026amp;login); err == nil { c.JSON(http.StatusOK, gin.H{ \u0026#34;user\u0026#34;: login.User, \u0026#34;password\u0026#34;: login.Password, }) } else { c.JSON(http.StatusBadRequest, gin.H{\u0026#34;error\u0026#34;: err.Error()}) } }) // 绑定QueryString示例 (/loginQuery?user=xiaonuo\u0026amp;password=123456) router.GET(\u0026#34;/loginForm\u0026#34;, func(c *gin.Context) { var login Login // ShouldBind()会根据请求的Content-Type自行选择绑定器 if err := c.ShouldBind(\u0026amp;login); err == nil { c.JSON(http.StatusOK, gin.H{ \u0026#34;user\u0026#34;: login.User, \u0026#34;password\u0026#34;: login.Password, }) } else { c.JSON(http.StatusBadRequest, gin.H{\u0026#34;error\u0026#34;: err.Error()}) } }) // Listen and serve on 0.0.0.0:8080 router.Run(\u0026#34;:8080\u0026#34;) } ShouldBind会按照下面的顺序解析请求中的数据完成绑定：\n如果是 GET 请求，只使用 Form 绑定引擎（query）。 如果是 POST 请求，首先检查 content-type 是否为 JSON 或 XML，然后再使用 Form（form-data）。 文件上传 单个文件上传 文件上传前端页面代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;zh-CN\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;上传文件示例\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;!--enctype=\u0026#34;multipart/form-data\u0026#34;:以二进制形式传输数据，传输文件必须要使用此参数 --\u0026gt; \u0026lt;form action=\u0026#34;/upload\u0026#34; method=\u0026#34;post\u0026#34; enctype=\u0026#34;multipart/form-data\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;file\u0026#34; name=\u0026#34;f1\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;submit\u0026#34; value=\u0026#34;上传\u0026#34;\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 后端gin框架部分代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // 处理multipart forms提交文件时默认的内存限制是32 MiB // 可以通过下面的方式修改 // router.MaxMultipartMemory = 8 \u0026lt;\u0026lt; 20 // 8 MiB router.POST(\u0026#34;/upload\u0026#34;, func(c *gin.Context) { //从请求中读取文件 f,_:=c.FormFile(\u0026#34;f1\u0026#34;) //Join将任意数量的路径元素拼接成一条路径，用斜线将它们拼接。空元素将被忽略。如果都是空的则返回空字符串 dst:=path.Join(\u0026#34;./\u0026#34;,f.Filename) //将读取到的文件保存到服务端 c.SaveUploadedFile(f,dst) c.JSON(http.StatusOK, gin.H{ \u0026#34;message\u0026#34;: fmt.Sprintf(\u0026#34;\u0026#39;%s\u0026#39; uploaded!\u0026#34;, file.Filename), }) }) 多个文件上传 1 2 3 4 5 6 7 8 9 10 r.POST(\u0026#34;/index\u0026#34;, func(c *gin.Context) { //从请求中读取文件 form,_:=c.MultipartForm() //根据input标签的name属性获取内容，一定要注意的是上传多个文件，其name属性要一致 files:=form.File[\u0026#34;f1\u0026#34;] for _,file:=range files{ dst:=path.Join(\u0026#34;./\u0026#34;,file.Filename) c.SaveUploadedFile(file,dst) } }) 重定向 HTTP重定向 HTTP 重定向很容易。 内部、外部重定向均支持。\n1 2 3 4 r.GET(\u0026#34;/login\u0026#34;, func(c *gin.Context) { //跳转到站外，URL也会改变。属于请求重定向 c.Redirect(http.StatusMovedPermanently,\u0026#34;https://www.baidu.com\u0026#34;) }) 路由重定向 路由重定向，使用 HandleContext：\n1 2 3 4 5 6 r.GET(\u0026#34;/a\u0026#34;, func(c *gin.Context) { //跳转到/b对应的路由处理函数 //这种属于内部跳转，属于请求转发，网页上的URL始终不会改变的，依然是/a c.Request.URL.Path=\u0026#34;/b\u0026#34;\t//把请求的URL修改成/b r.HandleContext(c)\t//继续进行后续的操作 }) Gin路由 普通路由 1 2 3 r.GET(\u0026#34;/index\u0026#34;, func(c *gin.Context) {...}) r.GET(\u0026#34;/login\u0026#34;, func(c *gin.Context) {...}) r.POST(\u0026#34;/login\u0026#34;, func(c *gin.Context) {...}) 此外，还有一个可以匹配所有请求方法的 Any方法如下：\n1 2 3 4 5 6 7 8 9 10 //请求方法的大杂烩 r.Any(\u0026#34;/login\u0026#34;, func(c *gin.Context) { switch c.Request.Method { //case可以是字符串也可以使用http包的常量 case \u0026#34;GET\u0026#34;: c.JSON(http.StatusOK,gin.H{\u0026#34;method\u0026#34;:\u0026#34;GET\u0026#34;}) case http.MethodPost: c.JSON(http.StatusOK,gin.H{\u0026#34;method\u0026#34;:\u0026#34;POST\u0026#34;}) } }) 为没有配置处理函数的路由添加处理程序，默认情况下它返回404代码，下面的代码为没有匹配到路由的请求都返回 views/404.html页面。\n1 2 3 r.NoRoute(func(c *gin.Context) { c.HTML(http.StatusNotFound, \u0026#34;views/404.html\u0026#34;, nil) }) 路由组 我们可以将拥有共同URL前缀的路由划分为一个路由组。习惯性一对 {}包裹同组的路由，这只是为了看着清晰，你用不用 {}包裹功能上没什么区别。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 func main() { r := gin.Default() //把公用的前缀提取出来，创建一个路由组 userGroup := r.Group(\u0026#34;/user\u0026#34;) { userGroup.GET(\u0026#34;/index\u0026#34;, func(c *gin.Context) {...}) userGroup.GET(\u0026#34;/login\u0026#34;, func(c *gin.Context) {...}) userGroup.POST(\u0026#34;/login\u0026#34;, func(c *gin.Context) {...}) } shopGroup := r.Group(\u0026#34;/shop\u0026#34;) { shopGroup.GET(\u0026#34;/index\u0026#34;, func(c *gin.Context) {...}) shopGroup.GET(\u0026#34;/cart\u0026#34;, func(c *gin.Context) {...}) shopGroup.POST(\u0026#34;/checkout\u0026#34;, func(c *gin.Context) {...}) } r.Run() } 路由组也是支持嵌套的，例如：\n1 2 3 4 5 6 7 8 9 shopGroup := r.Group(\u0026#34;/shop\u0026#34;) { shopGroup.GET(\u0026#34;/index\u0026#34;, func(c *gin.Context) {...}) shopGroup.GET(\u0026#34;/cart\u0026#34;, func(c *gin.Context) {...}) shopGroup.POST(\u0026#34;/checkout\u0026#34;, func(c *gin.Context) {...}) // 嵌套路由组,访问URL为/shop/xx/oo xx := shopGroup.Group(\u0026#34;xx\u0026#34;) xx.GET(\u0026#34;/oo\u0026#34;, func(c *gin.Context) {...}) } 通常我们将路由分组用在划分业务逻辑或划分API版本时。\nGin框架中的路由使用的是httprouter这个库。\n其基本原理就是构造一个路由地址的前缀树。\nGin中间件 Gin框架允许开发者在处理请求的过程中，加入用户自己的钩子（Hook）函数。这个钩子函数就叫中间件，中间件适合处理一些公共的业务逻辑，比如登录认证、权限校验、数据分页、记录日志、耗时统计等。它的作用范围是作用于中间件之后的路由，中间件之前的不会被波及。\n定义中间件 Gin中的中间件必须是一个 gin.HandlerFunc类型。例如像下面的代码一样定义一个统计请求耗时的中间件。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // StatCost 是一个统计耗时请求耗时的中间件 func StatCost() gin.HandlerFunc { return func(c *gin.Context) { start := time.Now() c.Set(\u0026#34;name\u0026#34;, \u0026#34;小王子\u0026#34;) // 可以通过c.Set在请求上下文中设置值，后续的处理函数能够取到该值c.Get() //中间件执行完后默认会继续执行对应的处理函数，但当需要对响应做修改时就需要c.Next()临时调用执行函数，然后回来继续执行中间件 //如果有多个中间件存在，Next的作用是执行下一个中间件函数 //（若下一个中间件中也有next，则继续向后执行），当下一个是处理函数，则是执行处理函数。 //遵循先执行后退出的运行原则 c.Next()\t//调用后续的处理函数 //c.Abort() //阻止调用后续的处理函数，自身执行完后直接返回上一级，没有上一级则直接返回response //下面是response之前需要执行的代码 cost:=time.Since(start) log.Println(cost) } } 如果用中间件来进行登录判断，则可以通过if判断是否登录，是则next()，不是则abort()。千万不能在不是时使用return。return只是退出当前函数，但是下一个函数依然会继续执行\n注册中间件 在gin框架中，我们可以为每个路由添加任意数量的中间件。\n为全局路由注册 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 func main() { // 新建一个没有任何默认中间件的路由 r := gin.New() //注册全局中间件 r.Use(StatCost()) r.GET(\u0026#34;/test\u0026#34;, func(c *gin.Context) { name := c.MustGet(\u0026#34;name\u0026#34;).(string) // 从上下文取值 log.Println(name) c.JSON(http.StatusOK, gin.H{ \u0026#34;message\u0026#34;: \u0026#34;Hello world!\u0026#34;, }) }) r.Run() } 为某个路由单独注册 1 2 3 4 5 6 7 8 // 给/test2路由单独注册中间件StatCost()（可注册多个） r.GET(\u0026#34;/test2\u0026#34;, StatCost(), func(c *gin.Context) { name := c.MustGet(\u0026#34;name\u0026#34;).(string) // 从上下文取值 log.Println(name) c.JSON(http.StatusOK, gin.H{ \u0026#34;message\u0026#34;: \u0026#34;Hello world!\u0026#34;, }) }) 为路由组注册中间件 为路由组注册中间件有以下两种写法。\n写法1：\n1 2 3 4 5 shopGroup := r.Group(\u0026#34;/shop\u0026#34;, StatCost()) { shopGroup.GET(\u0026#34;/index\u0026#34;, func(c *gin.Context) {...}) ... } 写法2：\n1 2 3 4 5 6 shopGroup := r.Group(\u0026#34;/shop\u0026#34;) shopGroup.Use(StatCost()) { shopGroup.GET(\u0026#34;/index\u0026#34;, func(c *gin.Context) {...}) ... } 中间件注意事项 gin默认中间件 gin.Default()默认使用了 Logger和 Recovery中间件，其中：\nLogger中间件将日志写入 gin.DefaultWriter，即使配置了 GIN_MODE=release。 Recovery中间件会recover任何 panic。如果有panic的话，会写入500响应码。 如果不想使用上面两个默认的中间件，可以使用 gin.New()新建一个没有任何默认中间件的路由。\ngin中间件中使用goroutine 当在中间件或 handler中启动新的 goroutine时，不能使用原始的上下文（c *gin.Context），必须使用其只读副本 c.Copy()。避免并发不安全的问题\n运行多个服务 我们可以在多个端口启动服务，例如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 package main import ( \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;golang.org/x/sync/errgroup\u0026#34; ) var ( g errgroup.Group ) func router01() http.Handler { e := gin.New() e.Use(gin.Recovery()) e.GET(\u0026#34;/\u0026#34;, func(c *gin.Context) { c.JSON( http.StatusOK, gin.H{ \u0026#34;code\u0026#34;: http.StatusOK, \u0026#34;error\u0026#34;: \u0026#34;Welcome server 01\u0026#34;, }, ) }) return e } func router02() http.Handler { e := gin.New() e.Use(gin.Recovery()) e.GET(\u0026#34;/\u0026#34;, func(c *gin.Context) { c.JSON( http.StatusOK, gin.H{ \u0026#34;code\u0026#34;: http.StatusOK, \u0026#34;error\u0026#34;: \u0026#34;Welcome server 02\u0026#34;, }, ) }) return e } func main() { server01 := \u0026amp;http.Server{ Addr: \u0026#34;:8080\u0026#34;, Handler: router01(), ReadTimeout: 5 * time.Second, WriteTimeout: 10 * time.Second, } server02 := \u0026amp;http.Server{ Addr: \u0026#34;:8081\u0026#34;, Handler: router02(), ReadTimeout: 5 * time.Second, WriteTimeout: 10 * time.Second, } // 借助errgroup.Group或者自行开启两个goroutine分别启动两个服务 g.Go(func() error { return server01.ListenAndServe() }) g.Go(func() error { return server02.ListenAndServe() }) if err := g.Wait(); err != nil { log.Fatal(err) } } ","date":"2020-11-17T00:00:00Z","permalink":"https://xiaonuoz.github.io/p/gin%E6%A1%86%E6%9E%B6%E5%88%9D%E4%BA%86%E8%A7%A3/","title":"gin框架初了解"},{"content":"垂直扩展和水平扩展：\n在垂直扩展模型中，想要增加系统负荷就意味着要在系统现有的部件上下工夫，即通过提高系统部件的能力来实现。不增加系统的成员数，但是我们通过增加系统成员的生产效率来获得期望的负荷量。\n在水平扩展模型中，我们不是通过增加单个系统成员的负荷而是简单的通过增加更多的系统成员来实现。系统每个成员的生产力依然没变，通过增加更多的成员来提高系统的能力。\nRPC 远程过程调用RPC是一种轻量级的计算机通信协议，它允许运行于一台计算机的程序调用另一台计算机的子程序，而程序员无需额外的为这个交互作用编程，如果涉及的软件采用面向对象编程，那么远程过程调用亦可称作远程调用或远程方法调用\nRPC就是像调用本地函数一样去调用远程函数。通过rpc协议传递远程函数的函数名、参数等。达到在本地调用远端函数，得返回值到本地的目标。\ngolang有RPC的官方库net/rpc，支持encoding/gob进行编解码，支持tcp和http数据传输方式，由于其他语言不支持gob，因此golang的rpc只适用于golang开发的服务器客户端交互(如用nc工具充当服务端会导致接受数据乱码)，想避免乱码可以使用jsonrpc包，其方法和rpc一致\ngolang的rpc必须符合4个条件:\n结构体字段首字母要大写（因为需要跨域访问，这个结构体是函数接收参数的结构体） 函数名首字母大写 函数第一个参数是接收参数，第二个参数是返回给客户端的参数，且第二个参数必须是指针类型 函数必须有一个返回值error 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 //服务端，求矩形周长 //声明矩形对象 type Rect struct { } //声明从客户接受的参数结构体 type Params struct { Width,Height int } //声明返回给客户端的参数结构体 type Ret struct { Area int Perimeter int } //定义求矩形面积和周长的方法,第二个参数必须是指针，因为是其他服务调用此服务 //第二个参数也可以定义一个普通类型进行返回 //返回值error如果不为空，那么无论接受参数是否已经有值，服务端都不会返回数据，error和传出参数只会有一个是非空 func (r *Rect)Area(p Params,ret *Ret) error{ ret.Area=p.Width*p.Height ret.Perimeter=(p.Width+p.Height)*2 return nil } func main(){ //1.注册服务 rpc.Register(new(Rect)) //2.把服务绑定到http协议上 rpc.HandleHTTP() //3.监听服务，等待客户端调用方法 err:=http.ListenAndServe(\u0026#34;:8989\u0026#34;,nil) if err != nil { return } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 //客户端，请求服务端，获取结果 //声明参数结构体 type Params struct { Width,Height int } type Ret struct { Area int Perimeter int } func main() { //1. 连接远程的RPC服务 rp, err := rpc.DialHTTP(\u0026#34;tcp\u0026#34;, \u0026#34;127.0.0.1:8989\u0026#34;) if err != nil { log.Fatal(err) } //2. 调用远程方法 var ret Ret //第一个参数为方法名，为服务器对应的结构体名.方法名，第二个参数为传过去的参数，第三个参数为接受到的参数，与服务器对应的参数类型要一致 err = rp.Call(\u0026#34;Rect.Area\u0026#34;, Params{10, 20}, \u0026amp;ret) if err != nil { log.Fatal(err) } fmt.Printf(\u0026#34;面积：%v\\n周长：%v\\n\u0026#34;,ret.Area,ret.Perimeter) } 假如结构体定义的方法参数和返回值不符合rpc的规范，我们是无法在编译期发现的，只有在运行期远程调用时才能发现这个方法的问题。如果希望在编译期就发现其对象是否合法，那就需要在此结构体之上定义一个rpc规范的接口，让这个结构体实现这个接口，这样子如果没有全部实现就无法通过编译\n官方 还提供了net/rpc/jsonrpc库实现rpc方法，jsonrpc采用json进行数据编解码，因此支持跨语言调用，目前jsonrpc库是基于tcp协议实现的，暂不支持http传输方式\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 //服务端 //1.注册服务 rpc.Register(new(Rect)) //2. 监听端口 listen, err := net.Listen(\u0026#34;tcp\u0026#34;, \u0026#34;127.0.0.1:8989\u0026#34;) if err != nil { log.Fatal(err) } //3. 循环监听服务 for { conn, err := listen.Accept() if err != nil { log.Fatal(err) continue } go func(conn net.Conn) { //客户端只需要把rpc.DialHTTP变成jsonrpc.Dial即可 fmt.Println(\u0026#34;create new a Client\u0026#34;) jsonrpc.ServeConn(conn) }(conn) } gRPC gRPC简介 grpc由google开发，是一款语言中立，平台中立、开源的远程过程调用系统 grpc客户端和服务端可以在多种环境中运行和交互，例如用java写一个服务端，可以用go语言客户端调用 gRPC和protobuf介绍 微服务架构中，由于每个服务对应的代码库是独立运行的，无法直接调用，彼此间的通信就是个大问题 gRPC可以实现微服务，将大的项目拆分为多个小且独立的业务模块，即服务，各服务间使用高效的protobuf协议进行gRPC调用，gRPC默认使用protocol buffers，这是google开源的一套成熟的结果数据序列化机制（也可以使用其他数据格式，如json） 可以用proto files创建gRPC服务，用message类型来定义方法参数和返回类型 从protobuf的角度来看，gRPC只不过是一个针对service接口生成代码的生成器(即在生成命令中添加 plugins=grpc ，不加这段，生成是无法生成service相关的代码的) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 go get -u -v github.com/golang/protobuf/proto go get google.golang.org/grpc（如果无法使用， 用如下命令代替） git clone https://github.com/grpc/grpc-go.git$GOPATH/src/google.golang.org/grpc git clone https://github.com/golang/net.git$GOPATH/src/google.golang.org/grpc git clone https://github.com/golang/net.git$GOPATH/src/golang.org/x/net git clone https://github.com/golang/text.git$GOPATH/src/golang.org/x/text go get -u github.com/golang/protobuf/{proto,protoc-gen-go} git clone https://github.com/google/go-genproto.git GOPATH/src/golang.org/x/text go get −u github.com/golang/protobuf/proto,protoc−gen−go git clone https://github.com/google/go−genproto.gitGOPATH/src/google.golang.org/genproto cd $GOPATH/src/ go install google.golang.org/grpc go get github.com/golang/protobuf/protoc-gen-go 上面安装好后， 会在 GOPATH/bin 下生成 protoc-gen-go.exe 但还需要一个 protoc.exe， windows 平台编译受限， 很难自己手动编译， 直接去网 站下载一个， 地址： https://github.com/protocolbuffers/protobuf/releases/tag/v3.9.0 ， 同样放到 GOPATH/bin 下 通过定义好的.proto文件生成Java, Python, C++, Go, Ruby,JavaNano, Objective-C, or C# 代码，需要安装编译器protoc。当使用protocol buffer编译器运行.proto文件时，编译器将生成所选语言的代码，用于使用在.proto文件中定义的消息类型、服务接口约定等。（Go: 生成一个.pb.go文件，每个消息类型对应一个结构体）\nprotobuf语法 基本规范 文件以**.proto作为文件后缀**，除结构体定义外的语句以分号结尾 结构定义可以包含：message（结构体）、service（接口）、enum（枚举类） rpc方法定义结尾的分号可有可无 字段中的数字并不是赋值，它代表了在编译时字段的位置，不能重复 Message命名采用驼峰命名方式，字段命名采用小写字母加下划线分隔方式\n1 2 3 message SongServerRequest { required string song_name = 1; } enum 枚举类型名采用驼峰命名方式，字段命名采用大写字母加下划线分隔方式\n1 2 3 4 enum Foo { FIRST_VALUE = 0;\t// 枚举值必须从0开始 SECOND_VALUE = 1; } Service与rpc方法名统一采用驼峰式命名\n字段规则 字段格式：限定修饰符 | 数据类型 | 字段名称 | = | 字段编码值 | [字段默认值] 限定修饰符包含required\\optional\\repeated Required: 表示是一个必须字段，必须相对于发送方，在发送消息之前必须设置该字段的值，对于接收方，必须能够识别该字段的意思。发送之前没有设置required字段或者无法识别required字段都会引发编解码异常，导致消息被丢弃 Optional：表示是一个可选字段，可选对于发送方，在发送消息时，可以有选择性的设置或者不设置该字段的值。对于接收方，如果能够识别可选字段就进行相应的处理，如果无法识别，则忽略该字段，消息中的其它字段正常处理。\u0026mdash;因为optional字段的特性，很多接口在升级版本中都把后来添加的字段都统一的设置为optional字段，这样老的版本无需升级程序也可以正常的与新的软件进行通信，只不过新的字段无法识别而已，因为并不是每个节点都需要新的功能，因此可以做到按需升级和平滑过渡 Repeated：表示该字段可以包含0~N个元素。其特性和optional一样，但是每一次可以包含多个值。可以看作是在传递一个数组的值(go是转换成切片) 数据类型 Protobuf定义了一套基本数据类型。几乎都可以映射到C++\\Java等语言的基础数据类型 表示打包的字节并不是固定。而是根据数据的大小或者长度 关于 fixed32 和int32的区别。fixed32的打包效率比int32的效率高，但是使用的空间一般比int32多。因此一个属于时间效率高，一个属于空间效率高 字段名称的命名与C、C++、Java等语言的变量命名方式几乎是相同的 protobuf建议字段的命名采用以下划线分割的驼峰式。例如first_name 而不是firstName 字段编码值 有了该值，通信双方才能互相识别对方的字段，相同的编码值，其限定修饰符和数据类型必须相同，编码值的取值范围为 1~2^32（4294967296） 其中1~15的编码时间和空间效率都是最高的，编码值越大，其编码的时间和空间效率就越低，所以建议把经常要传递的值把其字段编码设置为1-15之间的值 1900~2000编码值为Google protobuf 系统内部保留值，建议不要在自己的项目中使用 当在传递数据时，对于required数据类型，如果用户没有设置值，则使用默认值传递到对端\nservice定义 如果想要将消息类型用在RPC系统中，可以在.proto文件中定义一个RPC服务接口，protocol buffer编译器会根据所选择的不同语言生成服务接口代码。例如，想要定义一个RPC服务并具有一个方法（Search），该方法接收SearchRequest并返回一个SearchResponse(两个都是proto中定义的message)，此时可以在.proto文件中进行如下定义： 1 2 3 4 service SearchService { rpc Search (SearchRequest) returns (SearchResponse); } 生成的接口代码作为客户端与服务端的约定，服务端必须实现定义的所有接口方法，客户端直接调用同名方法向服务端发起请求，比较麻烦的是，即便业务上不需要参数也必须指定一个请求消息，一般会定义一个空message message定义 一个message类型定义描述了一个请求或响应的消息格式，可以包含多种类型字段。例如定义一个搜索请求的消息格式，每个请求包含查询字符串、页码、每页数目，字段名用小写，转为go文件后自动变为大写，message就相当于结构体 1 2 3 4 5 6 syntax =\u0026#34;proto3\u0026#34;; message SearchRequest { string query = 1; // 查询字符串 int32 page_number = 2; // 页码 int32 result_per_page = 3; // 每页条数 } SearchRequest 定义了三个字段，每个字段声明以分号结尾，.proto文件支持双斜线 // 添加单行注释 首行声明使用的protobuf版本为proto3（必须声明） 一个.proto文件中可以定义多个消息类型，一般用于同时定义多个相关的消息，例如在同一个.proto文件中同时定义搜索请求和响应消息 message支持嵌套使用，作为另一message中的字段类型 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 message SearchResponse { repeated Result results = 1; } message Result { string url = 1; string title = 2; repeated string snippets = 3; } //内部声明的message类型名称只可在内部直接使用 message SearchResponse { message Result { string url = 1; string title = 2; repeated string snippets = 3; } repeated Result results = 1; } map类型的定义 1 2 3 //map\u0026lt;key_type, value_type\u0026gt; map_field = N; message Project {...} map\u0026lt;string, Project\u0026gt; projects = 1; 键、值类型可以是内置的类型，也可以是自定义message类型 字段不支持repeated属性 示例： user.proto 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 syntax = \u0026#34;proto3\u0026#34;; package proto; // 编译到当前目录下的proto目录下，包名也会叫proto option go_package=\u0026#34;./proto\u0026#34;; message UserRequest{ string name = 1; } message UserResponse{ int32 id = 1; string name = 2; int32 age = 3; repeated string hobby = 4; } service UserInfoService{ rpc GetUserInfo(UserRequest) returns (UserResponse){} } 生成go文件： protoc -I . --go_out=plugins=grpc:. ./user.proto\nserver.go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;google.golang.org/grpc\u0026#34; pb \u0026#34;main/gRPC/proto\u0026#34; \u0026#34;net\u0026#34; ) //定义一个结构体用于实现protobuf生成出来的接口的方法,这样子就可以将这个结构体通过传参的方式进行注册 type UserInfoService struct {} //实现接口中的函数 func (u *UserInfoService)GetUserInfo(ctx context.Context, req *pb.UserRequest) (resp *pb.UserResponse,err error){ name:=req.Name if name==\u0026#34;zs\u0026#34;{ resp=\u0026amp;pb.UserResponse{ Id: 1, Name: name, Age: 22, Hobby: []string{\u0026#34;run\u0026#34;,\u0026#34;game\u0026#34;}, } } return } var UserInfo = UserInfoService{} func main(){ addr:=\u0026#34;127.0.0.1:8999\u0026#34; // 1.监听端口 listen, err := net.Listen(\u0026#34;tcp\u0026#34;, addr) if err != nil { fmt.Println(\u0026#34;监听失败,\u0026#34;,err) return } defer listen.Close() // 2.实例化gRPC s := grpc.NewServer() // 3.在gRPC上注册微服务 pb.RegisterUserInfoServiceServer(s,\u0026amp;UserInfo) // 4.启动服务端 s.Serve(listen) } client.go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;google.golang.org/grpc\u0026#34; pb \u0026#34;main/gRPC/proto\u0026#34; ) func main(){ // 1.连接,必须设置传输安全WithInsecure(),作用是关闭传输安全。不加则会报错 conn, err := grpc.Dial(\u0026#34;127.0.0.1:8999\u0026#34;,grpc.WithInsecure()) if err != nil { fmt.Println(\u0026#34;连接异常,\u0026#34;,err) return } defer conn.Close() // 2.实例化客户端 client := pb.NewUserInfoServiceClient(conn) // 3.组装一个请求参数 req:=new(pb.UserRequest) req.Name=\u0026#34;zs\u0026#34; // 4.调用接口 resp, err := client.GetUserInfo(context.Background(), req) if err != nil { fmt.Println(\u0026#34;响应异常，\u0026#34;,err) } fmt.Printf(\u0026#34;响应结果%v\\n\u0026#34;,resp) } Go Micro Go Micro是一个插件化的基础框架，基于此可以构建微服务，Micro的设计哲学是可插拔的插件化架构。\n在架构之外，它默认实现了mdns作为服务发现，通过http进行通信，通过protobuf和json进行编解码\ngo-micro框架的一个缺点是从1.0到2.0,在到3.0版本都不兼容，每个版本都有很大的改动。\ngo-micro的3.0版本升级为一个云原生开发平台，原来的go-micro项目改名字了。\n云原生开发平台官网地址：https://m3o.com/\n可能是由于go-micro框架应用的人比较多，后来作者又把项目改回了go-micro,现在依然在维护。\n所以现在关于go-micro项目是有些混乱的，我们可以这样认为:\ngo-micro云原生开发平台版本现在叫micro。项目源码地址：https://github.com/micro/micro 。主要作用是用来生成micro的脚手架 go-micro依然是go-micro，不过项目地址变了，变成了https://github.com/asim/go-micro 。这个才是开发需要的框架，且导包需要导的是 \u0026ldquo;go-micro.dev/v4\u0026rdquo; 其实这两个项目的发起人都是asim，不过micro版本的现在是商业化了，变成了一个云原生开发平台，我们使用micro生成脚手架，项目开发框架是asim的go-micro。\ngo-micro依然是一个微服务框架，现在说go-micro v3准确的讲是指https://github.com/asim/go-micro 这个项目，micro v3是指https://github.com/micro/micro\n所以用micro生成的脚手架后要将micro/v3相关的改成 go-micro.dev/v4\n现在网络上很多博客和文章都是过时的，没人讲清楚两者的区别，给初学者带来很多困扰。\n主要功能 服务发现：自动服务注册和名称解析。服务发现是微服务开发的核心。当服务A需要与服务B通话时，它需要该服务的位置。默认发现机制是多播DNS（mdns），一种零配置系统。可以选择使用SWIM协议为p2p网络设置八卦，或者为弹性云原生设置设置consul 每个server启动时都会将自己的ip、port和服务名 注册给 服务发现，由服务发现来管理多个服务的信息 client通过借助服务发现来访问service。当client需要某个服务时，会先请求服务发现，服务发现找到一个可用的服务，返回其对应服务的信息，然后client再借助服务发现返回的服务信息直接访问service 负载均衡：基于服务发现构建的客户端负载均衡。一旦我们获得了服务的任意数量实例的地址，我们现在需要一种方法来决定要路由到哪个节点。我们使用随机散列负载均衡来提供跨服务的均匀分布，并在出现问题时重试不同的节点 消息编码：基于内容类型的动态消息编码。客户端和服务器将使用编解码器和内容类型为您无缝编码和解码Go类型。可以编码任何种类的消息并从不同的客户端发送。客户端和服务器默认处理此问题。这包括默认的protobuf和json 请求/响应：基于RPC的请求/响应，支持双向流。我们提供了同步通信的抽象。对服务的请求将自动解决，负载平衡，拨号和流式传输。启用tls时，默认传输为http / 1.1或http2 Async Messaging（异步处理）：PubSub是异步通信和事件驱动架构的一流公民。事件通知是微服务开发的核心模式。启用tls时，默认消息传递是点对点http/ 1.1或http2 可插拔接口：Go Micro为每个分布式系统抽象使用Go接口，因此，这些接口是可插拔的，并允许Go Micro与运行时无关，可以插入任何基础技术（插件地址：https://github.com/micro/go-plugins） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;github.com/micro/go-micro\u0026#34; \u0026#34;log\u0026#34; pb \u0026#34;mico/proto\u0026#34; ) type Hello struct {} func (h *Hello)Info(ctx context.Context,req *pb.UserRequest,resp *pb.UserResponse) error{ resp.Msg=\u0026#34;hello\u0026#34;+req.Name return nil } func main() { // 1.得到服务端实例，Name()用于设置微服务的名，用来访问 //cmd命令： micro call hello Hello.Info {\u0026#34;username\u0026#34;:\u0026#34;zs\u0026#34;} service := micro.NewService(micro.Name(\u0026#34;hello\u0026#34;)) // 2.初始化 service.Init() // 3.服务注册 err := pb.RegisterUserInfoServiceHandler(service.Server(), new(Hello)) if err != nil { fmt.Println(err) } // 4.启动服务 err = service.Run() if err != nil { log.Fatal(err) } } syntax = \u0026#34;proto3\u0026#34;; package proto; message UserRequest{ string name = 1; } message UserResponse{ string msg = 1; } service UserInfoService{ rpc Info(UserRequest) returns (UserResponse){} } protoc -I . --micro_out=. --go_out=. ./user.proto 服务发现-consul 服务发现也可以看作是一个服务，是给 “具体业务服务”提供服务的（从这个角度来看，服务发现就属于具体业务的server端，具体业务服务是client端）。通常一台正常运行的服务崩溃的几率在2%，如果有三台则崩溃几率是2% * %2 *%2。\nconsul 常用命令 consul agent -dev以一个开发者模式去运行一个consul代理，走的都是consul中一系列默认配置\n服务端代码： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 // \u0026#34;github.com/hashicorp/consul/api\u0026#34; //把grpc注册到consul上 // 1.初始化consul配置 consulConfig := api.DefaultConfig() // 2.创建consul对象,此时server对于consul是客户端，因此是NewClient consulClient, err := api.NewClient(consulConfig) if err != nil { fmt.Println(\u0026#34;err:\u0026#34;, err) return } // 3. 通知consul即将注册的服务的配置信息 reg := api.AgentServiceRegistration{ ID: \u0026#34;1\u0026#34;, Tags: []string{\u0026#34;server tag\u0026#34;}, Name: \u0026#34;server\u0026#34;, // 此处的ip port和check的ip port要与grpc监听的ip port一致 // 这里的ip端口是监听的目标端口，并不是consul自己的端口 Port: 8999, Address: \u0026#34;127.0.0.1\u0026#34;, Check: \u0026amp;api.AgentServiceCheck{ TCP: \u0026#34;127.0.0.1:8999\u0026#34;, Timeout: \u0026#34;1s\u0026#34;, Interval: \u0026#34;5s\u0026#34;, }, } // 4. 注册grpc服务到consul上 err = consulClient.Agent().ServiceRegister(\u0026amp;reg) if err != nil { fmt.Println(\u0026#34;err:\u0026#34;, err) return } fmt.Println(\u0026#34;注册成功\u0026#34;) //############以下为grpc远程调用#################### addr := \u0026#34;127.0.0.1:8999\u0026#34; // 1.监听端口 listen, err := net.Listen(\u0026#34;tcp\u0026#34;, addr) if err != nil { fmt.Println(\u0026#34;监听失败,\u0026#34;, err) return } defer listen.Close() // 2.实例化gRPC s := grpc.NewServer() // 3.在gRPC上注册微服务 teacher.RegisterSayServer(s, \u0026amp;SayStruct{}) // 4.启动服务端 s.Serve(listen) consulClient.Agent().ServiceDeregister(\u0026quot;serviceID\u0026quot;) 注销对应id的服务\n客户端代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 // \u0026#34;github.com/hashicorp/consul/api\u0026#34; // 1.初始化consul配置 consulConfig := api.DefaultConfig() // 2.创建consul对象,此时server对于consul是客户端，因此是NewClient consulClient, err := api.NewClient(consulConfig) if err != nil { fmt.Println(\u0026#34;err:\u0026#34;, err) return } // 参数： service：对应服务名; tag:别名，如有多个就任选一个；passingOnly; 是否通过健康检查，通常要找通过的，即true; QueryOptions：查询参数，通常传nil // 返回值：ServiceEntry:存储服务的切片，因为一个服务可以分成主从的同名多个服务, QueryMeta：查询参数的返回值，传nil返nil service, _, err := consulClient.Health().Service(\u0026#34;server\u0026#34;, \u0026#34;server tag\u0026#34;, true, nil) if err != nil || len(service) == 0 { return } addr := fmt.Sprintf(\u0026#34;%v:%v\u0026#34;, service[0].Service.Address, service[0].Service.Port) //############以下为grpc远程调用#################### // 1.连接,必须设置传输安全WithInsecure()，否则会报错 conn, err := grpc.Dial(addr, grpc.WithInsecure()) if err != nil { fmt.Println(\u0026#34;连接异常,\u0026#34;, err) return } defer conn.Close() // 2.实例化客户端 client := teacher.NewSayClient(conn) // 3.组装一个请求参数 req := new(teacher.Teacher) // 4.调用接口 resp, err := client.SayHellp(context.Background(), req) if err != nil { fmt.Println(\u0026#34;响应异常，\u0026#34;, err) } new [创建micro脚手架] 1 2 3 4 #指定服务的命名空间 --namespace \u0026#34;go.micro\u0026#34;\tNamespace for the service e.g com.example #服务类型，可以是微服务srv,或者web项目,或者api等 --type \u0026#34;srv\u0026#34;\tType of service e.g api, fnc, srv, web 服务端Test: micro默认的服务发现插件是mdns,这个插件是支持组播的，它必须和服务在同一局域网内，这导致如果服务分布在不同局域网就无法使用。因此要将插件改为consul micro.Registry(consulReg)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 // 初始化服务发现--consul: \u0026#34;github.com/asim/go-micro/plugins/registry/consul/v4\u0026#34; // registry: \u0026#34;go-micro.dev/v4/registry\u0026#34; consulReg := consul.NewRegistry( // 这个可以指定对应的consul地址,不加的话默认是连接本机的8500端口的consul func(o *registry.Options) { o.Addrs = []string{ // 配置consul服务地址 \u0026#34;127.0.0.1:8500\u0026#34;, } }) // Create service 初始化服务器对象 srv := micro.NewService( micro.Address(\u0026#34;127.0.0.1:8999\u0026#34;), // 指定ip和端口，如果不指定的话，micro会随机分配一个端口 micro.Name(\u0026#34;test-micro\u0026#34;), // 服务器名 micro.Version(\u0026#34;latest\u0026#34;), // 版本，latest为最终版本，最新 micro.Registry(consulReg), // 注册到consul上 // 设置命令行参数 --run_client,BoolFlag默认值是false。与init配套。如果不需要命令行可以不加 micro.Flags(\u0026amp;cli.BoolFlag{ Name: \u0026#34;run_client\u0026#34;, Usage: \u0026#34;Launch the client\u0026#34;, }), ) // init作用于对newService中的flags进行加工处理，如果没有flag可以不调用 // 建议不调用，因为init如果加了和newService中相同的参数，如micro.Name(\u0026#34;newName\u0026#34;)，它会覆盖掉之前的。导致name变成newName // 后续代码运行期想重新初始化才有必要调用此函数 // srv.Init(micro.Action(func(c *cli.Context) error { // if c.Bool(\u0026#34;run_client\u0026#34;) { // fmt.Println(\u0026#34;run_client cmd\u0026#34;) // os.Exit(0) // } // return nil // })) // Register handler 注册服务 pb.RegisterTestMicroHandler(srv.Server(), new(handler.TestMicro)) // Run service 运行服务 if err := srv.Run(); err != nil { return } 通过 micro new 服务包名 在当前文件夹下生成一个micro-service的脚手架包，然后通过makefile中的 protoc --proto_path=. --micro_out=. --go_out=:. proto/test-micro.proto 生成对应的proto代码，然后修改对应导入的包名，将micro包改成对应框架的 go-micro.dev/v4 包，最后go mod tidy 服务端就初始化完成可以运行了\n客户端Test 客户端使用gin框架来对服务端进行远程调用。因为micro的客户端过于臃肿且实现复杂，采用gin会更简单方便\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 func main() { route := gin.Default() route.GET(\u0026#34;/\u0026#34;, handlerFuncsTest) route.Run(\u0026#34;:8080\u0026#34;) } func handlerFuncsTest(c *gin.Context) { // 初始化服务发现consul，可以加参数指定consul的地址，不加默认为本机的8500端口 consulReg := consul.NewRegistry( func(o *registry.Options) { o.Addrs = []string{ // 配置consul服务地址 \u0026#34;127.0.0.1:8500\u0026#34;, } }, ) // 初始化micro对象，指定consul为服务发现 service := micro.NewService( micro.Registry(consulReg), ) // 初始化客户端，client.DefaultClient是go-micro.dev/v4/client包的 // 如果使用默认的mdns，则是client.DefaultClient // microClient := pb.NewTestMicroService(\u0026#34;test-micro\u0026#34;, client.DefaultClient) microClient := pb.NewTestMicroService(\u0026#34;test-micro\u0026#34;, service.Client()) // 可以将服务端的pb复制过来，也可以通过mod直接调用服务端的pb,在服务端中，handler后缀的是给服务端使用的注册，service是给客户端使用的调用 resp, _ := microClient.Call(context.TODO(), \u0026amp;pb.Request{Name: \u0026#34;zs\u0026#34;}) c.JSON(http.StatusOK, gin.H{ \u0026#34;resp\u0026#34;: resp, }) } ","date":"2020-11-15T00:00:00Z","permalink":"https://xiaonuoz.github.io/p/%E5%88%9D%E6%AD%A5%E4%BA%86%E8%A7%A3%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%A1%86%E6%9E%B6/","title":"初步了解微服务框架"},{"content":"Go从语言层面就支持了并发，虽然并发程序的内存管理是非常复杂的，但是GO提供了自动垃圾回收机制\n并行和并发的区别：并行指在同一时刻有多条指令在多个处理器上同时进行，并发指虽然在单个处理器上同一时刻只能有一条指令执行，但多个进程指令被快速的轮换执行，使得在宏观上具有多个进程同时执行的效果\ngoroutine(协程) 通常main函数在一个单独的协程中运行，成为主协程，新的goroutine用go语句来创建，称为子协程，如果主协程中有for死循环，子协程要在死循环前建立，否则一直死循环就无法运行到创建子协程的语句\n注意：主协程退出后，子协程会同时跟着退出\nruntime包 Gosched() Gosched()的作用就像linux中的进程优先级一样，但是如果在A协程中添加了runtime.Gosched()后，A协程会搁置到其他协程完成任务退出协程后再继续进行A协程\nGoexit() 调用runtime.Goexit()将立即终止当前协程的进行，即使写在协程中的调用函数里也会中止当前协程\nGOMAXPROCS() 调用runtime.GOMAXPROCS()用来设置 可以并发计算 的CPU核数的最大值，并返回之前的值\nn := runtime.GOMAXPROCS(4) //以四核并发计算，核数可以大于当前系统的最大核数\nchannel类型 定义了两个函数person1和person2，虽然协程是同时进行的，但是两个公用了一个公共资源，最后打印就会出现这边打印一个字母那边打印一个，就造成了资源竞争问题。channel属于引用传递，即调用的都是同一个channel。\n如果在person2中设置channel堵塞，它就会让此进程一直堵在channel步骤，而person1中先调用公共资源，person2暂停，当person1资源调用完毕后将int=666传入ch,子进程1结束，ch管道中有内容了不再堵塞，此时person2中的同一个ch管道将int传入函数并丢弃，然后继续执行后面的代码来调用公共资源，这样就可以避免资源竞争问题。\n如果希望在子协程工作完成后再关闭主协程的话（主协程关闭会导致子协程同时关闭），可以在子协程中设置管道 ch\u0026lt;- \u0026ldquo;子协程完毕\u0026rdquo;，然后主协程接受这段内容并丢弃( \u0026lt;-ch )，这样就可以实现子协程没有进行到发送信息到管道那一步时，主协程ch永远堵塞，只有完成子协程任务并关闭后，主协程channel才有信息不堵塞，然后才可以正常完成主协程（channel也可以用于发送接受数据，类似linux的竖线管道）\n无缓存通道和有缓存通道 channer分为无缓存通道和有缓存通道，无缓存channel没有接收或者没有发送都会造成堵塞，有缓存值的在写满缓存时就会造成堵塞，通道中没值时也无法取数据\n有缓存cannel属于异步处理，每当接收者从cannel取出一条数据时，cannel中就会丢弃这条数据，将空间闲置出来给新的数据使用，当数据取完或写满时就会造成阻塞\nclose(ch)可以关闭通道，接收者可以通过 value,ok := \u0026lt;-ch来获取值，value为管道中的数据，ok在当管道没有关闭时为true，管道关闭了则为false\n无缓冲通道是指在接收前没有能力保存任何值得通道。\n这种类型的通道要求发送goroutine和接收goroutine同时准备好，才能完成发送和接收操作。如果两个goroutine没有同时准备好，通道会导致先执行发送或接收操作的goroutine阻塞等待。 这种对通道进行发送和接收的交互行为本身就是同步的，其中任意一个操作都无法离开另一个操作单独存在。\n有缓冲通道指通道可以保存多个值。\n如果给定了一个缓冲区容量，那么通道就是异步的，只要缓冲区有未使用空间用于发送数据，或还包含可以接收的数据，那么其通信就会无阻塞地进行\n上图所示：\n右侧的goroutine正在从通道接收一个值。 右侧的goroutine独立完成了接手值得动作，而左侧的goroutine正在发送一个新值到通道里。 左侧的goroutine还在向通道发送新值，而右侧的goroutine正在从通道接收另一个值。这个步骤里的两个操作既不是同步，也不会互相阻塞。 所有的发送和接收都完成，而通道里还有几个值，也有一些空间可以存储更多的值 无缓冲的与有缓冲channel有着重大差别，那就是一个是同步的 一个是非同步的。\n1 2 3 4 比如 c1:=make(chan int) 无缓冲 c2:=make(chan int,1) 有缓冲 c1\u0026lt;-1 无缓冲： 不仅仅是向 c1 通道放 1，而是一直要等有别的携程 \u0026lt;-c1 接手了这个参数，那么c1\u0026lt;-1才会继续下去，要不然就一直阻塞着。 有缓冲： c2\u0026lt;-1 则不会阻塞，因为缓冲大小是1(其实是缓冲大小为0)，只有当放第二个值的时候，第一个还没被人拿走，这时候才会阻塞。\n单项channel管道 双向channel可以隐式的转换为单向channel ( var writeCan chan\u0026lt;- int = ch ),单向无法转换为双向\n案例：\nchannel可以通过range来依次读取通道内的数据，它的参数只有num，并非两个值。且必须搭配close(ch)使用，不然继续迭代下去，没有值但是还在进行\u0026lt;-ch，这会造成通道阻塞，出现死锁问题。在写入channel的函数中最后加上 close(ch) 就可以给它发送一个信号，它会在将数据全部写入channel后关闭管道，关闭一个已经关闭的channel会导致panic，当对一个关闭后的通道进行取值直到取完后，再进行取值返回的就是类型零值，如int返回的是0\n对一个已经关闭的管道是可以继续取值的，但是取到的值是对应类型的零值，如果管道没关闭继续取值就会造成死锁\n1 2 3 4 5 6 7 8 ch1 := make(chan int, 2) ch1 \u0026lt;- 10 ch1 \u0026lt;- 20 close(ch1) \u0026lt;-ch1 \u0026lt;-ch1 x, ok := \u0026lt;-ch1 fmt.Println(x, ok) 由于channel属于引用传递，所以虽然函数的参数是单项通道，但是最终修改的依然是本来的双向通道ch，这可以避免在函数中又读又写造成逻辑混乱\nSelect (可以监听channel通道的数据流动) 注意：select如果任意某个通信可以进行，它就执行，其他被忽略。如果同时有多个case满足，它会随机选择一个，都没有满足时会阻塞等待（避免造成饥饿，因为如果某个case一直被满足，它之后的case偶尔满足，我们不能每次都让首个case通过，这样会造成后面的case长期得不到资源的分配）\n注意：如果写了default，即每次都能判定成功，会导致select语句完成判定然后结束，不写就会（一直）阻塞直到case判定成功执行某一个case语句然后结束\n如果select语句不加for循环，那么它只会判定一次并只将数据写入一次管道，监听一次就结束显然不符合监听的目的，所以需要往select外套一个for死循环来实现监听操作\n第二个case语句，写入通道的操作必须要有一个读的操作（\u0026lt;-chan2）可以接收它的数据，只有写没有读是不能写成功的，有读没写也是不能读成功，都会造成管道死锁问题，这样就可以通过select实现在外部写入，select中的case读操作就判断成功。\n注意：case不止是判断，它判断后面的语句能否读写成功，那么在判断成功的同时它也会往通道中读写数据\n斐波那契数列 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 func fibonacci(ch chan\u0026lt;- int, quit \u0026lt;-chan bool) { x, y := 1, 1 for { select { case ch \u0026lt;- x: x, y = y, x+y case flag := \u0026lt;-quit: return } } } func main() { ch := make(chan int) quit := make(chan bool) //输出数字 go func() { for i := 0; i \u0026lt; 8; i++ { num := \u0026lt;-ch fmt.Println(num) } quit \u0026lt;- true }() //产生数字，写入管道 fibonacci(ch, quit) } 输出结果为：1\t1\t2\t3\t5\t8\t13\t21 除去第二个数，其他数为前两个数相加 如果select语句不加for循环，那么它只会判定一次并只将数据写入一次管道，而fibonacci函数处于主协程，当判定成功后就会直接完成主协程，那么子协程也会退出，后面的数据都无法继续输出\n用select实现超时退出 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 ch := make(chan int) quit := make(chan bool) //监听管道数据 go func() { for { select { case num := \u0026lt;-ch: fmt.Println(num) case \u0026lt;-time.NewTimer(3 * time.Second).C: fmt.Println(\u0026#34;3s没有输出数据\u0026#34;) quit \u0026lt;- true return } } }() go func() { for i := 0; i \u0026lt; 4; i++ { ch \u0026lt;- i time.Sleep(time.Second) } \u0026lt;-quit fmt.Println(\u0026#34;程序结束\u0026#34;) }() for {} 当ch中没有数据时，case ch会堵塞，然后三秒后case time会有数据，执行case2，往quit管道中写入数据，最下面的读取quit就不会堵塞，程序就会继续执行，如果不希望主程序结束，可以将quit管道放到一个子协程中（且主程序有for循环之类的不会结束），那么三秒后子协程运行完自动退出，不会波及主协程\n注意：case语句是会执行之后的语句的，所以time.NewTimer()会在3s后继续有值，且会再输出fmt，然后此时quit管道没接收者，会一直堵塞在这里，子协程会一直存在直到主协程关闭，所以加上return语句让它在第一次就关闭此函数，或者break跳出for循环\n","date":"2020-11-10T00:00:00Z","permalink":"https://xiaonuoz.github.io/p/go%E5%A4%9A%E7%BA%BF%E7%A8%8B/","title":"Go多线程"},{"content":"Go语言项目中的性能优化主要有以下几个方面：\nCPU profile：报告程序的 CPU 使用情况，按照一定频率去采集应用程序在 CPU 和寄存器上面的数据 Memory Profile（Heap Profile）：报告程序的内存使用情况 Block Profiling：报告 goroutines 不在运行状态的情况，可以用来分析和查找死锁等性能瓶颈 Goroutine Profiling：报告 goroutines 的使用情况，有哪些 goroutine，它们的调用关系是怎样的 采集性能数据 Go语言内置了获取程序的运行数据的工具，包括以下两个标准库：\nruntime/pprof：采集工具型应用运行数据进行分析（只运行一次就结束的使用这个） net/http/pprof：采集服务型应用运行时数据进行分析（持续运行如服务程序使用这个） pprof开启后，每隔一段时间（10ms）就会收集下当前的堆栈信息，获取各个函数占用的CPU以及内存资源；最后通过对这些采样数据进行分析，形成一个性能分析报告。\n注意，我们只应该在性能测试的时候才在代码中引入pprof。\n工具型应用 如果你的应用程序是运行一段时间就结束退出类型。那么最好的办法是在应用退出的时候把 profiling 的报告保存到文件中，进行分析。对于这种情况，可以使用 runtime/pprof库。 首先在代码中导入 runtime/pprof工具：\n1 import \u0026#34;runtime/pprof\u0026#34; CPU性能分析 开启CPU性能分析：\n1 pprof.StartCPUProfile(w io.Writer) 停止CPU性能分析：\n1 pprof.StopCPUProfile() 应用执行结束后，就会生成一个文件，保存了我们的 CPU profiling 数据。得到采样数据之后，使用 go tool pprof工具进行CPU性能分析。\n内存性能优化 记录程序的堆栈信息\n1 pprof.WriteHeapProfile(w io.Writer) 得到采样数据之后，使用 go tool pprof工具进行内存性能分析。\ngo tool pprof默认是使用 -inuse_space进行统计，还可以使用 -inuse-objects查看分配对象的数量。\n服务型应用 如果你的应用程序是一直运行的，比如 web 应用，那么可以使用 net/http/pprof库，它能够在提供 HTTP 服务进行分析。\n如果使用了默认的 http.DefaultServeMux（通常是代码直接使用 http.ListenAndServe(“0.0.0.0:8000”, nil)），只需要在你的web server端代码中按如下方式导入 net/http/pprof\n1 import _ \u0026#34;net/http/pprof\u0026#34; 如果你使用自定义的 Mux，则需要手动注册一些路由规则：\n1 2 3 4 5 r.HandleFunc(\u0026#34;/debug/pprof/\u0026#34;, pprof.Index) r.HandleFunc(\u0026#34;/debug/pprof/cmdline\u0026#34;, pprof.Cmdline) r.HandleFunc(\u0026#34;/debug/pprof/profile\u0026#34;, pprof.Profile) r.HandleFunc(\u0026#34;/debug/pprof/symbol\u0026#34;, pprof.Symbol) r.HandleFunc(\u0026#34;/debug/pprof/trace\u0026#34;, pprof.Trace) 如果你使用的是gin框架，那么推荐使用github.com/gin-contrib/pprof，在代码中通过以下命令注册pprof相关路由。\n1 pprof.Register(router) 不管哪种方式，你的 HTTP 服务都会多出 /debug/pprof endpoint，访问它会得到类似下面的内容：\n这个路径下还有几个子页面：\n/debug/pprof/profile：访问这个链接会自动进行 CPU profiling，持续 30s，并生成一个文件供下载 /debug/pprof/heap： Memory Profiling 的路径，访问这个链接会得到一个内存 Profiling 结果的文件 /debug/pprof/block：block Profiling 的路径 /debug/pprof/goroutines：运行的 goroutines 列表，以及调用关系 go tool pprof命令 不管是工具型应用还是服务型应用，我们使用相应的pprof库获取数据之后，下一步的都要对这些数据进行分析，可以使用 go tool pprof命令行工具。\ngo tool pprof最简单的使用方式为:\n1 go tool pprof [binary] [source] 其中：\nbinary 是应用的二进制文件，用来解析各种符号； source 表示 profile 数据的来源，可以是本地的文件，也可以是 http 地址。 注意事项： 获取的 Profiling 数据是动态的，要想获得有效的数据，请保证应用处于较大的负载（比如正在生成中运行的服务，或者通过其他工具模拟访问压力）。否则如果应用处于空闲状态，得到的结果可能没有任何意义。\n具体示例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 package main import ( \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;runtime/pprof\u0026#34; \u0026#34;time\u0026#34; ) // 一段有问题的代码 func logicCode() { var c chan int for { //chan类型只定义没初始化，读取都会阻塞，因此会一直进行for循环 select { case v := \u0026lt;-c: fmt.Printf(\u0026#34;recv from chan, value:%v\\n\u0026#34;, v) default: } } } func main() { var isCPUPprof bool var isMemPprof bool flag.BoolVar(\u0026amp;isCPUPprof, \u0026#34;cpu\u0026#34;, false, \u0026#34;turn cpu pprof on\u0026#34;) flag.BoolVar(\u0026amp;isMemPprof, \u0026#34;mem\u0026#34;, false, \u0026#34;turn mem pprof on\u0026#34;) flag.Parse() if isCPUPprof { file, err := os.Create(\u0026#34;./cpu.pprof\u0026#34;) if err != nil { fmt.Printf(\u0026#34;create cpu pprof failed, err:%v\\n\u0026#34;, err) return } pprof.StartCPUProfile(file) defer pprof.StopCPUProfile() } for i := 0; i \u0026lt; 8; i++ { go logicCode() } time.Sleep(20 * time.Second) if isMemPprof { file, err := os.Create(\u0026#34;./mem.pprof\u0026#34;) if err != nil { fmt.Printf(\u0026#34;create mem pprof failed, err:%v\\n\u0026#34;, err) return } pprof.WriteHeapProfile(file) file.Close() } } 通过flag我们可以在命令行控制是否开启CPU和Mem的性能分析。 将上面的代码保存并编译成 runtime_pprof可执行文件，执行时加上 -cpu命令行参数如下：\n1 ./runtime_pprof -cpu=true 等待20秒后会在当前目录下生成一个 cpu.pprof文件。\n命令行交互界面 我们使用go工具链里的 pprof来分析一下。\n1 go tool pprof cpu.pprof 执行上面的代码会进入交互界面如下：\n1 2 3 4 5 6 runtime_pprof $ go tool pprof cpu.pprof Type: cpu Time: Jun 28, 2019 at 11:28am (CST) Duration: 20.13s, Total samples = 1.91mins (568.60%) Entering interactive mode (type \u0026#34;help\u0026#34; for commands, \u0026#34;o\u0026#34; for options) (pprof) 我们可以在交互界面输入 top3来查看程序中占用CPU前3位的函数：\n1 2 3 4 5 6 7 8 (pprof) top3 Showing nodes accounting for 100.37s, 87.68% of 114.47s total Dropped 17 nodes (cum \u0026lt;= 0.57s) Showing top 3 nodes out of 4 flat flat% sum% cum cum% 42.52s 37.15% 37.15% 91.73s 80.13% runtime.selectnbrecv 35.21s 30.76% 67.90% 39.49s 34.50% runtime.chanrecv 22.64s 19.78% 87.68% 114.37s 99.91% main.logicCode 其中：\nflat：当前函数占用CPU的耗时 flat：:当前函数占用CPU的耗时百分比 sun%：函数占用CPU的耗时累计百分比 cum：当前函数加上调用当前函数的函数占用CPU的总耗时 cum%：当前函数加上调用当前函数的函数占用CPU的总耗时百分比 最后一列：函数名称 在大多数的情况下，我们可以通过分析这五列得出一个应用程序的运行情况，并对程序进行优化。\n我们还可以使用 list 函数名命令查看具体的函数分析，例如执行 list logicCode查看我们编写的函数的详细分析。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 (pprof) list logicCode Total: 1.91mins ROUTINE ================ main.logicCode in .../runtime_pprof/main.go 22.64s 1.91mins (flat, cum) 99.91% of Total . . 12:func logicCode() { . . 13: var c chan int . . 14: for { . . 15: select { . . 16: case v := \u0026lt;-c: 22.64s 1.91mins 17: fmt.Printf(\u0026#34;recv from chan, value:%v\\n\u0026#34;, v) . . 18: default: . . 19: . . 20: } . . 21: } . . 22:} 通过分析发现大部分CPU资源被17行占用，我们分析出select语句中的default没有内容会导致上面的 case v:=\u0026lt;-c:一直执行。我们在default分支添加一行 time.Sleep(time.Second)即可。\n图形化 或者可以直接输入web，通过svg图的方式查看程序中详细的CPU占用情况。 想要查看图形化的界面首先需要安装graphviz图形化工具。\nMac：\n1 brew install graphviz Windows: 下载graphviz 将 graphviz安装目录下的bin文件夹添加到Path环境变量中。 在终端输入 dot -version查看是否安装成功。\n关于图形的说明： 每个框代表一个函数，理论上框的越大表示占用的CPU资源越多。 方框之间的线条代表函数之间的调用关系。 线条上的数字表示函数调用的次数。 方框中的第一行数字表示当前函数占用CPU的百分比，第二行数字表示当前函数累计占用CPU的百分比。\n除了分析CPU性能数据，pprof也支持分析内存性能数据。比如，使用下面的命令分析http服务的heap性能数据，查看当前程序的内存占用以及热点内存对象使用的情况。\n1 2 3 4 5 6 # 查看内存占用数据 go tool pprof -inuse_space http://127.0.0.1:8080/debug/pprof/heap go tool pprof -inuse_objects http://127.0.0.1:8080/debug/pprof/heap # 查看临时内存分配数据 go tool pprof -alloc_space http://127.0.0.1:8080/debug/pprof/heap go tool pprof -alloc_objects http://127.0.0.1:8080/debug/pprof/heap go-torch和火焰图 火焰图（Flame Graph）是 Bredan Gregg 创建的一种性能分析图表，因为它的样子近似 🔥而得名。上面的 profiling 结果也能转换成火焰图，如果对火焰图比较了解可以手动来操作，不过这里我们要介绍一个工具：go-torch。这是 uber 开源的一个工具，可以直接读取 golang profiling 数据，并生成一个火焰图的 svg 文件。\n安装go-torch 1 go get -v github.com/uber/go-torch 火焰图 svg 文件可以通过浏览器打开，它对于调用图的最优点是它是动态的：可以通过点击每个方块来 zoom in 分析它上面的内容。\n火焰图的调用顺序从下到上，每个方块代表一个函数，它上面一层表示这个函数会调用哪些函数，方块的大小代表了占用 CPU 使用的长短。火焰图的配色并没有特殊的意义，默认的红、黄配色是为了更像火焰而已。\ngo-torch 工具的使用非常简单，没有任何参数的话，它会尝试从 http://localhost:8080/debug/pprof/profile获取 profiling 数据。它有三个常用的参数可以调整：\n-u –url：要访问的 URL，这里只是主机和端口部分 -s –suffix：pprof profile 的路径，默认为 /debug/pprof/profile –seconds：要执行 profiling 的时间长度，默认为 30s 安装 FlameGraph 要生成火焰图，需要事先安装 FlameGraph工具，这个工具的安装很简单（需要perl环境支持），只要把对应的可执行文件加入到环境变量中即可。\n下载安装perl：https://www.perl.org/get.html 下载FlameGraph：git clone https://github.com/brendangregg/FlameGraph.git 将 FlameGraph目录加入到操作系统的环境变量中。 Windows平台的同学，需要把 go-torch/render/flamegraph.go文件中的 GenerateFlameGraph按如下方式修改，然后在 go-torch目录下执行 go install即可。 1 2 3 4 5 6 7 8 9 10 // GenerateFlameGraph runs the flamegraph script to generate a flame graph SVG. func GenerateFlameGraph(graphInput []byte, args ...string) ([]byte, error) { flameGraph := findInPath(flameGraphScripts) if flameGraph == \u0026#34;\u0026#34; { return nil, errNoPerlScript } if runtime.GOOS == \u0026#34;windows\u0026#34; { return runScript(\u0026#34;perl\u0026#34;, append([]string{flameGraph}, args...), graphInput) } return runScript(flameGraph, args, graphInput) } 压测工具wrk 推荐使用https://github.com/wg/wrk 或 https://github.com/adjust/go-wrk\n使用go-torch 使用wrk进行压测:\n1 go-wrk -n 50000 http://127.0.0.1:8080/book/list 在上面压测进行的同时，打开另一个终端执行:\n1 go-torch -u http://127.0.0.1:8080 -t 30 30秒之后终端会出现如下提示：Writing svg to torch.svg\n然后我们使用浏览器打开 torch.svg就能看到如下火焰图了。火焰图的y轴表示cpu调用方法的先后，x轴表示在每个采样调用时间内，方法所占的时间百分比，越宽代表占据cpu时间越多。通过火焰图我们就可以更清楚的找出耗时长的函数调用，然后不断的修正代码，重新采样，不断优化。\n此外还可以借助火焰图分析内存性能数据：\n1 2 3 4 go-torch -inuse_space http://127.0.0.1:8080/debug/pprof/heap go-torch -inuse_objects http://127.0.0.1:8080/debug/pprof/heap go-torch -alloc_space http://127.0.0.1:8080/debug/pprof/heap go-torch -alloc_objects http://127.0.0.1:8080/debug/pprof/heap pprof与性能测试结合 go test命令有两个参数和 pprof 相关，它们分别指定生成的 CPU 和 Memory profiling 保存的文件：\n-cpuprofile：cpu profiling 数据要保存的文件地址 -memprofile：memory profiling 数据要报文的文件地址 我们还可以选择将pprof与性能测试相结合，比如：\n比如下面执行测试的同时，也会执行 CPU profiling，并把结果保存在 cpu.prof 文件中：\n1 go test -bench . -cpuprofile=cpu.prof 比如下面执行测试的同时，也会执行 Mem profiling，并把结果保存在 cpu.prof 文件中：\n1 go test -bench . -memprofile=./mem.prof 需要注意的是，Profiling 一般和性能测试一起使用，只有应用在负载高的情况下 Profiling 才有意义。\n","date":"2020-11-10T00:00:00Z","permalink":"https://xiaonuoz.github.io/p/go%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/","title":"Go性能优化"},{"content":"在Go代码中可能会存在多个 goroutine同时操作一个资源（临界区），这种情况会发生 竞态问题（数据竞态）。类比现实生活中的例子有十字路口被各个方向的的汽车竞争；还有火车上的卫生间被车厢里的人竞争。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 var x int64 var wg sync.WaitGroup func add() { for i := 0; i \u0026lt; 50000; i++ { x = x + 1 } wg.Done() } func main() { wg.Add(2) go add() go add() wg.Wait() fmt.Println(x) } 上面的代码中我们开启了两个 goroutine去累加变量x的值，这两个 goroutine在访问和修改 x变量的时候就会存在数据竞争，导致最后的结果与期待的不符(小于十万)。\n互斥锁 互斥锁是一种常用的控制共享资源访问的方法，它能够保证同时只有一个 goroutine可以访问共享资源。Go语言中使用 sync包的 Mutex类型来实现互斥锁。 使用互斥锁来修复上面代码的问题：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 var x int64 var wg sync.WaitGroup var lock sync.Mutex func add() { for i := 0; i \u0026lt; 5000; i++ { lock.Lock() // 加锁 x = x + 1 lock.Unlock() // 解锁 } wg.Done() } func main() { wg.Add(2) go add() go add() wg.Wait() fmt.Println(x) } 使用互斥锁能够保证同一时间有且只有一个 goroutine进入临界区，其他的 goroutine则在等待锁；当互斥锁释放后，等待的 goroutine才可以获取锁进入临界区，多个 goroutine同时等待一个锁时，唤醒的策略是随机的。\nMutex是一个结构体类型，如果要作为参数时一定要使用指针成为引用传递，不然传递的就是副本，是另一个锁了\n读写互斥锁 互斥锁的本质是当一个goroutine访问的时候，其他goroutine都不能访问。这样在资源同步，避免竞争的同时也降低了程序的并发性能。程序由原来的并行执行变成了串行执行。\n互斥锁是完全互斥的，但是有很多实际的场景下是读多写少的，当我们并发的去读取一个资源不涉及资源修改的时候是没有必要加互斥锁的，这种场景下使用读写锁是更好的一种选择。读写锁在Go语言中使用 sync包中的 RWMutex类型。(读的时候如果什么锁都不加，可能读取的资源是修改前的资源)\n**读写锁可以让多个读操作并发，同时读取，但是对于写操作是完全互斥的。**也就是说，当一个goroutine进行写操作的时候，其他goroutine既不能进行读操作，也不能进行写操作。当一个goroutine进行读操作的时候，其他goroutine也可以进行读操作，但是任何一个goroutine都不能进行写操作。处于读锁定状态，那么针对它的写锁定操作将永远不会成功，且相应的goroutine也会被一直阻塞，直到读锁解锁。因为它们是互斥的。\n读写锁示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 var ( x int64 wg sync.WaitGroup lock sync.Mutex rwlock sync.RWMutex ) func write() { // lock.Lock() // 加互斥锁 rwlock.Lock() // 加写锁 x = x + 1 time.Sleep(10 * time.Millisecond) // 假设读操作耗时10毫秒 rwlock.Unlock() // 解写锁 // lock.Unlock() // 解互斥锁 wg.Done() } func read() { // lock.Lock() // 加互斥锁 rwlock.RLock() // 加读锁 time.Sleep(time.Millisecond) // 假设读操作耗时1毫秒 rwlock.RUnlock() // 解读锁 // lock.Unlock() // 解互斥锁 wg.Done() } func main() { start := time.Now() for i := 0; i \u0026lt; 10; i++ { wg.Add(1) go write() } for i := 0; i \u0026lt; 1000; i++ { wg.Add(1) go read() } wg.Wait() end := time.Now() fmt.Println(end.Sub(start)) } 需要注意的是读写锁非常适合读多写少的场景，如果读和写的操作差别不大，读写锁的优势就发挥不出来。\nsync.WaitGroup 在代码中生硬的使用 time.Sleep肯定是不合适的，Go语言中可以使用 sync.WaitGroup来实现并发任务的同步。 sync.WaitGroup有以下几个方法：\nsync.WaitGroup内部维护着一个计数器，计数器的值可以增加和减少。例如当我们启动了N 个并发任务时，就将计数器值增加N。每个任务完成时通过调用Done()方法将计数器减1。通过调用Wait()来等待并发任务执行完，当计数器值为0时，表示所有并发任务已经完成。\n1 2 3 4 5 6 7 8 9 10 11 12 var wg sync.WaitGroup func hello() { defer wg.Done() fmt.Println(\u0026#34;Hello Goroutine!\u0026#34;) } func main() { wg.Add(1) go hello() // 启动另外一个goroutine去执行hello函数 wg.Wait() //等待子协程完成才继续运行下面的代码 fmt.Println(\u0026#34;main goroutine done!\u0026#34;) } 需要注意 sync.WaitGroup是一个结构体，传递的时候要传递指针。\nsync.Once 在编程的很多场景下我们需要确保某些操作在高并发的场景下只执行一次，例如只加载一次配置文件、只关闭一次通道等。\nGo语言中的 sync包中提供了一个针对只执行一次场景的解决方案–sync.Once。\nsync.Once只有一个 Do方法，其签名如下：\n1 func (o *Once) Do(f func()) {} 备注：如果要执行的函数 f需要传递参数就需要搭配闭包来使用。因为once方法使用是十分苛刻的，它的参数只能是无参函数，因此如果要用在关闭一次通道时，就需要用 func (){close(ch1)}\n加载配置文件示例 延迟一个开销很大的初始化操作到真正用到它的时候再执行是一个很好的实践。因为预先初始化一个变量（比如在init函数中完成初始化）会增加程序的启动耗时，而且有可能实际执行过程中这个变量没有用上，那么这个初始化操作就不是必须要做的。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 var icons map[string]image.Image func loadIcons() { icons = map[string]image.Image{ \u0026#34;left\u0026#34;: loadIcon(\u0026#34;left.png\u0026#34;), \u0026#34;up\u0026#34;: loadIcon(\u0026#34;up.png\u0026#34;), \u0026#34;right\u0026#34;: loadIcon(\u0026#34;right.png\u0026#34;), \u0026#34;down\u0026#34;: loadIcon(\u0026#34;down.png\u0026#34;), } } // Icon 被多个goroutine调用时不是并发安全的 // 可能同时有多个goroutine都判断其为nil，都进行了一次初始化操作 func Icon(name string) image.Image { if icons == nil { loadIcons() } return icons[name] } 多个 goroutine并发调用Icon函数时不是并发安全的，现代的编译器和CPU可能会在保证每个 goroutine都满足串行一致的基础上自由地重排访问内存的顺序。loadIcons函数可能会被重排为以下结果：\n1 2 3 4 5 6 7 func loadIcons() { icons = make(map[string]image.Image) icons[\u0026#34;left\u0026#34;] = loadIcon(\u0026#34;left.png\u0026#34;) icons[\u0026#34;up\u0026#34;] = loadIcon(\u0026#34;up.png\u0026#34;) icons[\u0026#34;right\u0026#34;] = loadIcon(\u0026#34;right.png\u0026#34;) icons[\u0026#34;down\u0026#34;] = loadIcon(\u0026#34;down.png\u0026#34;) } 在这种情况下就会出现即使判断了 icons不是nil也不意味着变量初始化完成了（第一个goroutine刚进行完make操作第二个goroutine就进行了判断，此时icons不是nil，但是它并没有进行赋值）。考虑到这种情况，我们能想到的办法就是添加互斥锁，保证初始化 icons的时候不会被其他的 goroutine操作，但是这样做又会引发性能问题。因此可以使用once来只加一次锁。\nonce是一个结构体，它有两个成员，一个互斥锁和一个标志位，当第一个goroutine执行once时，标志位为false，那么它就对其加互斥锁，此时其他goroutine只能等待第一个goroutine解锁，once执行完后标志位变为true并解锁，然后后面的goroutine进来判断，此时标志位是true，就不会执行once了。这就实现了多个协程只运行一次once\n使用 sync.Once改造的示例代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 var icons map[string]image.Image var loadIconsOnce sync.Once func loadIcons() { icons = map[string]image.Image{ \u0026#34;left\u0026#34;: loadIcon(\u0026#34;left.png\u0026#34;), \u0026#34;up\u0026#34;: loadIcon(\u0026#34;up.png\u0026#34;), \u0026#34;right\u0026#34;: loadIcon(\u0026#34;right.png\u0026#34;), \u0026#34;down\u0026#34;: loadIcon(\u0026#34;down.png\u0026#34;), } } // Icon 是并发安全的 func Icon(name string) image.Image { loadIconsOnce.Do(loadIcons) return icons[name] } sync.Map Go语言中内置的map不是并发安全的。开启少量几个 goroutine进行并发读写的时候可能没什么问题，当并发多了之后（超过20次后）执行上面的代码就会报 fatal error: concurrent map writes错误。\n像这种场景下就需要为map加锁来保证并发的安全性了，Go语言的 sync包中提供了一个开箱即用的并发安全版map–sync.Map。**开箱即用表示不用像内置的map一样使用make函数初始化就能直接使用。**同时 sync.Map内置了诸如 Store、Load、LoadOrStore、Delete、Range等操作方法。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 var m = sync.Map{} func main() { wg := sync.WaitGroup{} for i := 0; i \u0026lt; 20; i++ { wg.Add(1) go func(n int) { key := strconv.Itoa(n) m.Store(key, n) value, _ := m.Load(key) fmt.Printf(\u0026#34;k=:%v,v:=%v\\n\u0026#34;, key, value) wg.Done() }(i) } wg.Wait() } 原子操作 代码中的加锁操作因为涉及内核态的上下文切换会比较耗时、代价比较高。针对基本数据类型我们还可以使用原子操作来保证并发安全，因为原子操作是Go语言提供的方法，它在用户态就可以完成，**因此性能比加锁操作更好。**Go语言中原子操作由内置的标准库 sync/atomic提供。\natomic包 比较并交换操作是比较addr和old的值是否相等，如果相等返回true，并将new的值赋值给addr\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 var x int64 var wg sync.WaitGroup func main() { wg.Add(10000) for i := 0; i \u0026lt; 10000; i++ { go func() { atomic.AddInt64(\u0026amp;x, 1) wg.Done() }() } wg.Wait() fmt.Println(x) } atomic包提供了底层的原子级内存操作，对于同步算法的实现很有用。这些函数必须谨慎地保证正确使用。除了某些特殊的底层应用，使用通道或者sync包的函数/类型实现同步更好。\n","date":"2020-11-10T00:00:00Z","permalink":"https://xiaonuoz.github.io/p/go%E4%B8%AD%E7%9A%84%E5%B9%B6%E5%8F%91%E5%AE%89%E5%85%A8%E9%94%81oncemap/","title":"go中的并发安全锁oncemap"},{"content":"早期go依赖管理很混乱，自己的包和第三方的都是放在%GOPATH/src中，无法导入不同版本的一个包，因此后面推出了go module\nGO111MODULE 要启用 go module支持首先要设置环境变量 GO111MODULE，通过它可以开启或关闭模块支持，它有三个可选值：off、on、auto，默认值是 auto。\nGO111MODULE=off禁用模块支持，编译时会从 GOPATH和 vendor文件夹中查找包。 GO111MODULE=on启用模块支持，编译时会忽略 GOPATH和 vendor文件夹，只根据 go.mod下载依赖。 GO111MODULE=auto，当项目在 $GOPATH/src外且项目根目录有 go.mod文件时，开启模块支持。 简单来说，设置 GO111MODULE=on之后就可以使用 go module了，以后就没有必要在GOPATH中创建项目了，并且还能够很好的管理项目依赖的第三方包信息。(如果要使用go module就不能将项目放到GOPATH中，会导致包混乱)\n使用 go module 管理依赖后会在项目根目录下生成两个文件 go.mod和 go.sum。\nGOPROXY Go1.11之后设置GOPROXY命令为：\n1 export GOPROXY=https://goproxy.cn Go1.13之后 GOPROXY默认值为 https://proxy.golang.org，在国内是无法访问的，所以设置GOPROXY为goproxy.cn。（中转）\n1 go env -w GOPROXY=https://goproxy.cn,direct go mod命令 常用的 go mod命令如下：\n1 2 3 4 5 6 7 8 go mod download 下载依赖的module到本地cache（默认为$GOPATH/pkg/mod目录） go mod edit 编辑go.mod文件 go mod graph 打印模块依赖图 go mod init 初始化当前文件夹, 创建go.mod文件 go mod tidy 增加缺少的module，删除无用的module go mod vendor 将依赖复制到vendor下 go mod verify 校验依赖 go mod why 解释为什么需要依赖 go.mod go.mod文件记录了项目所有的依赖信息，其结构大致如下：\n1 2 3 4 5 6 7 8 9 10 11 12 module github.com/Q1mi/studygo/blogger go 1.12 require ( github.com/DeanThompson/ginpprof v0.0.0-20190408063150-3be636683586 github.com/gin-gonic/gin v1.4.0 github.com/go-sql-driver/mysql v1.4.1 github.com/jmoiron/sqlx v1.2.0 github.com/satori/go.uuid v1.2.0 google.golang.org/appengine v1.6.1 // indirect ) 其中，\nmodule用来定义包名 require用来定义依赖包及版本 indirect表示间接引用（自己的包没使用，但是引用的包又引用了这个包就是间接引用） 依赖的版本 go mod支持语义化版本号，比如 go get foo@v1.2.3，也可以跟git的分支或tag，比如 go get foo@master，当然也可以跟git提交哈希，比如 go get foo@e3702bed2。关于依赖的版本支持以下几种格式：\n1 2 3 4 5 gopkg.in/tomb.v1 v1.0.0-20141024135613-dd632973f1e7 gopkg.in/vmihailenco/msgpack.v2 v2.9.1 gopkg.in/yaml.v2 \u0026lt;=v2.2.1 github.com/tatsushid/go-fastping v0.0.0-20160109021039-d7bb493dee3e latest replace 在国内访问golang.org/x的各个包都需要翻墙，可以在go.mod中使用replace替换成github上对应的库。（go在github中有搭镜像，也可以直接导入github中相对应的包）\n1 2 3 4 5 replace ( golang.org/x/crypto v0.0.0-20180820150726-614d502a4dac =\u0026gt; github.com/golang/crypto v0.0.0-20180820150726-614d502a4dac golang.org/x/net v0.0.0-20180821023952-922f4815f713 =\u0026gt; github.com/golang/net v0.0.0-20180826012351-8a410e7b638d golang.org/x/text v0.3.0 =\u0026gt; github.com/golang/text v0.3.0 ) go get 在项目中执行 go get命令可以下载依赖包，并且还可以指定下载的版本。\n运行 go get -u将会升级到最新的次要版本或者修订版本(x.y.z, z是修订版本号， y是次要版本号) 运行 go get -u=patch将会升级到最新的修订版本 运行 go get package@version将会升级到指定的版本号version 如果下载所有依赖可以使用 go mod download命令。\n整理依赖 我们在代码中删除依赖代码后，相关的依赖库并不会在 go.mod文件中自动移除。这种情况下我们可以使用 go mod tidy命令更新 go.mod中的依赖关系。\ngo mod edit 格式化 因为我们可以手动修改go.mod文件，所以有些时候需要格式化该文件。Go提供了一下命令：\n1 go mod edit -fmt 添加依赖项 1 go mod edit -require=golang.org/x/text 移除依赖项 如果只是想修改 go.mod文件中的内容，那么可以运行 go mod edit -droprequire=package path，比如要在 go.mod中移除 golang.org/x/text包，可以使用如下命令：\n1 go mod edit -droprequire=golang.org/x/text 关于 go mod edit的更多用法可以通过 go help mod edit查看。\n在项目中使用go module 既有项目 如果需要对一个已经存在的项目启用 go module，可以按照以下步骤操作：\n在项目目录下执行 go mod init，生成一个 go.mod文件。 执行 go get，查找并记录当前项目的依赖，同时生成一个 go.sum记录每个依赖库的版本和哈希值。 新项目 对于一个新创建的项目，我们可以在项目文件夹下按照以下步骤操作：\n执行 go mod init 项目名命令，在当前项目文件夹下创建一个 go.mod文件。 手动编辑 go.mod中的require依赖项或执行 go get自动发现、维护依赖。 ","date":"2020-11-08T00:00:00Z","permalink":"https://xiaonuoz.github.io/p/go%E7%9A%84%E4%BE%9D%E8%B5%96%E7%AE%A1%E7%90%86/","title":"Go的依赖管理"},{"content":"go test工具 Go语言中的测试依赖 go test命令。编写测试代码和编写普通的Go代码过程是类似的，并不需要学习新的语法、规则或工具。\ngo test命令是一个按照一定约定和组织的测试代码的驱动程序。在包目录内，所有以 _test.go为后缀名的源代码文件都是 go test测试的一部分(测试函数的要求：文件名以_test.go结尾，函数名以Test等特殊单词开头，参数为*testing.T等)，不会被 go build编译到最终的可执行文件中。\n在 *_test.go文件中有三种类型的函数，单元测试函数、基准测试函数和示例函数。\ngo test命令会遍历所有的 *_test.go文件中符合上述命名规则的函数，然后生成一个临时的main包用于调用相应的测试函数，然后构建并运行、报告测试结果，最后清理测试中生成的临时文件。\n测试函数 测试函数的格式 每个测试函数必须导入 testing包，测试函数的基本格式（签名）如下：\n1 2 3 func TestName(t *testing.T){ // ... } 测试函数的名字必须以 Test开头，可选的后缀名必须以大写字母开头，举几个例子：\n1 2 3 func TestAdd(t *testing.T){ ... } func TestSum(t *testing.T){ ... } func TestLog(t *testing.T){ ... } 其中参数 t用于报告测试失败和附加的日志信息。 testing.T的拥有的方法如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 func (c *T) Error(args ...interface{}) func (c *T) Errorf(format string, args ...interface{}) func (c *T) Fail() func (c *T) FailNow() func (c *T) Failed() bool func (c *T) Fatal(args ...interface{}) func (c *T) Fatalf(format string, args ...interface{}) func (c *T) Log(args ...interface{}) func (c *T) Logf(format string, args ...interface{}) func (c *T) Name() string func (t *T) Parallel() func (t *T) Run(name string, f func(t *T)) bool func (c *T) Skip(args ...interface{}) func (c *T) SkipNow() func (c *T) Skipf(format string, args ...interface{}) func (c *T) Skipped() bool 就像细胞是构成我们身体的基本单位，**一个软件程序也是由很多单元组件构成的。**单元组件可以是函数、结构体、方法和最终用户可能依赖的任意东西。总之我们需要确保这些组件是能够正常运行的。单元测试是一些利用各种方法测试单元组件的程序，它会将结果与预期输出进行比较。\n接下来，我们定义一个 split的包，包中定义了一个 Split函数，具体实现如下：\n1 2 3 4 5 6 7 8 9 10 11 12 //以关键字分割字符串，如abc分割后就是[a c] func Split(str string,sep string) []string{ var ret []string index:=strings.Index(str,sep) for index\u0026gt;=0{ ret=append(ret,str[:index]) str=str[index+len(sep):] index=strings.Index(str,sep) } ret=append(ret,str) return ret } 在当前目录下，创建一个 split_test.go的测试文件，并定义一个测试函数如下：\n1 2 3 4 5 6 7 func TestSplit(t *testing.T) { // 测试函数名必须以Test开头，必须接收一个*testing.T类型参数 got := Split(\u0026#34;a:b:c\u0026#34;, \u0026#34;:\u0026#34;) // 程序输出的结果 want := []string{\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;} // 期望的结果 if !reflect.DeepEqual(want, got) { // 因为slice不能比较直接，借助反射包中的方法比较 t.Errorf(\u0026#34;excepted:%v, got:%v\u0026#34;, want, got) // 测试失败输出错误提示 } } 在 split包路径下，执行 go test命令，可以看到输出结果如下：\n1 2 3 go test PASS ok github.com/Q1mi/studygo/code_demo/test_demo/split 0.005s 也可以用 go test -run TestAdd来执行单个测试函数，可以为 go test命令添加 -v参数，查看测试函数名称和运行时间：\n1 2 3 4 5 6 7 go test -v === RUN TestAdd === RUN TestAdd/case1 --- PASS: TestAdd (0.00s) --- PASS: TestAdd/case1 (0.00s) PASS ok test 0.029s 注意：当我们修改了我们的代码之后不仅仅要执行那些失败的测试函数，我们应该完整的运行所有的测试，保证不会因为修改代码而引入了新的问题。\n测试组 我们现在还想要测试一下 split函数对中文字符串的支持，这个时候我们可以再编写一个 TestChineseSplit测试函数，但是我们也可以使用如下更友好的一种方式来添加更多的测试用例。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 func TestSplit(t *testing.T) { // 定义一个测试用例类型 type test struct { input string sep string want []string } // 定义一个存储测试用例的切片 tests := []test{ {input: \u0026#34;a:b:c\u0026#34;, sep: \u0026#34;:\u0026#34;, want: []string{\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;}}, {input: \u0026#34;a:b:c\u0026#34;, sep: \u0026#34;,\u0026#34;, want: []string{\u0026#34;a:b:c\u0026#34;}}, {input: \u0026#34;abcd\u0026#34;, sep: \u0026#34;bc\u0026#34;, want: []string{\u0026#34;a\u0026#34;, \u0026#34;d\u0026#34;}}, {input: \u0026#34;沙河有沙又有河\u0026#34;, sep: \u0026#34;沙\u0026#34;, want: []string{\u0026#34;河有\u0026#34;, \u0026#34;又有河\u0026#34;}}, } // 遍历切片，逐一执行测试用例 for _, tc := range tests { got := Split(tc.input, tc.sep) if !reflect.DeepEqual(got, tc.want) { t.Errorf(\u0026#34;excepted:%v, got:%v\u0026#34;, tc.want, got) } } } 我们通过上面的代码把多个测试用例合到一起，再次执行 go test命令。\n1 2 3 4 5 6 7 split $ go test -v === RUN TestSplit --- FAIL: TestSplit (0.00s) split_test.go:42: excepted:[河有 又有河], got:[ 河有 又有河] FAIL exit status 1 FAIL github.com/Q1mi/studygo/code_demo/test_demo/split 0.006s 我们的测试出现了问题，仔细看打印的测试失败提示信息：excepted:[河有 又有河], got:[ 河有 又有河]，你会发现 [ 河有 又有河]中有个不明显的空串，这种情况下十分推荐使用 %#v的格式化方式。\n子测试 如果测试用例比较多的时候，我们是没办法一眼看出来具体是哪个测试用例失败了（只会报出错误信息，是什么函数的问题还得自己找）。所以可以使用map函数：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 func TestSplit(t *testing.T) { type test struct { // 定义test结构体 input string sep string want []string } tests := map[string]test{ // 测试用例使用map存储 \u0026#34;simple\u0026#34;: {input: \u0026#34;a:b:c\u0026#34;, sep: \u0026#34;:\u0026#34;, want: []string{\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;}}, \u0026#34;wrong sep\u0026#34;: {input: \u0026#34;a:b:c\u0026#34;, sep: \u0026#34;,\u0026#34;, want: []string{\u0026#34;a:b:c\u0026#34;}}, \u0026#34;more sep\u0026#34;: {input: \u0026#34;abcd\u0026#34;, sep: \u0026#34;bc\u0026#34;, want: []string{\u0026#34;a\u0026#34;, \u0026#34;d\u0026#34;}}, \u0026#34;leading sep\u0026#34;: {input: \u0026#34;沙河有沙又有河\u0026#34;, sep: \u0026#34;沙\u0026#34;, want: []string{\u0026#34;河有\u0026#34;, \u0026#34;又有河\u0026#34;}}, } for name, tc := range tests { got := Split(tc.input, tc.sep) if !reflect.DeepEqual(got, tc.want) { t.Errorf(\u0026#34;name:%s excepted:%#v, got:%#v\u0026#34;, name, tc.want, got) // 将测试用例的name格式化输出 } } } 上面的做法是能够解决问题的。同时Go1.7+中新增了子测试，我们可以按照如下方式使用 t.Run执行子测试：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 func TestAdd(t *testing.T){\t// 测试函数名必须以Test开头，必须接收一个*testing.T类型参数 type testCase struct {\t// 定义test结构体 str string ret string want []string } testGroup:=map[string]testCase{\t// 测试用例使用map存储 \u0026#34;case1\u0026#34;:{\u0026#34;abce\u0026#34;,\u0026#34;b\u0026#34;,[]string{\u0026#34;a\u0026#34;,\u0026#34;ce\u0026#34;}}, \u0026#34;case2\u0026#34;:{\u0026#34;a:b:c\u0026#34;,\u0026#34;:\u0026#34;,[]string{\u0026#34;a\u0026#34;,\u0026#34;b\u0026#34;,\u0026#34;c\u0026#34;}}, \u0026#34;case3\u0026#34;:{\u0026#34;abce\u0026#34;,\u0026#34;bc\u0026#34;,[]string{\u0026#34;a\u0026#34;,\u0026#34;e\u0026#34;}}, \u0026#34;case4\u0026#34;:{\u0026#34;沙河有沙又有河\u0026#34;,\u0026#34;沙\u0026#34;,[]string{\u0026#34;\u0026#34;,\u0026#34;河有\u0026#34;,\u0026#34;又有河\u0026#34;}}, } for key,testCa :=range testGroup{ t.Run(key,func(t *testing.T){\t// 使用t.Run()执行子测试 got:=Split(testCa.str,testCa.ret) if !reflect.DeepEqual(got,testCa.want){ t.Errorf(\u0026#34;error,you want %#v,but it is %#v\\n\u0026#34;,testCa.want,got) } }) } } 此时我们再执行 go test命令就能够看到更清晰的输出内容了：\n1 2 3 4 5 6 7 8 9 10 11 12 13 go test -v === RUN TestAdd === RUN TestAdd/case1 === RUN TestAdd/case2 === RUN TestAdd/case3 === RUN TestAdd/case4 --- PASS: TestAdd (0.00s) --- PASS: TestAdd/case1 (0.00s) --- PASS: TestAdd/case2 (0.00s) --- PASS: TestAdd/case3 (0.00s) --- PASS: TestAdd/case4 (0.00s) PASS ok test 0.156s 我们都知道可以通过 -run=RegExp来指定运行的测试用例，还可以通过 /来指定要运行的子测试用例，例如：go test -v -run=Split/case1只会运行 case1对应的子测试用例。\n测试覆盖率 测试覆盖率是你的代码被测试套件覆盖的百分比。通常我们使用的都是语句的覆盖率，也就是在测试中至少被运行一次的代码占总代码的比例。\nGo提供内置功能来检查你的代码覆盖率。我们可以使用 go test -cover来查看测试覆盖率。\n1 2 3 4 go test -cover PASS coverage: 100.0% of statements ok test 0.184s 从上面的结果可以看到我们的测试用例覆盖了100%的代码。\nGo还提供了一个额外的 -coverprofile参数，用来将覆盖率相关的记录信息输出到一个文件。\n1 2 3 4 go test -cover -coverprofile=c.out PASS coverage: 100.0% of statements ok github.com/Q1mi/studygo/code_demo/test_demo/split 0.005s 上面的命令会将覆盖率相关的信息输出到当前文件夹下面的 c.out文件中，然后我们执行 go tool cover -html=c.out，使用 cover工具来处理生成的记录信息，该命令会打开本地的浏览器窗口生成一个HTML报告。上图中每个用绿色标记的语句块表示被覆盖了，而红色的表示没有被覆盖。\n通常测试函数覆盖率要求达到100%，测试覆盖率要达到60%，如果没到60%要么就是代码考虑不周全，要么就是永远不可能执行到的语句有很多\n基准测试 基准测试函数格式 基准测试就是在一定的工作负载之下检测程序性能的一种方法。基准测试的基本格式如下：\n1 2 3 func BenchmarkName(b *testing.B){ // ... } 基准测试以 Benchmark为前缀，需要一个 *testing.B类型的参数b，基准测试必须要执行 b.N次，这样的测试才有对照性，b.N的值是系统根据实际情况去调整的，从而保证测试的稳定性（至少运行一秒钟）。 b.N不是固定的数，它会尽量去跑，能跑多少次就是多少次**，** testing.B拥有的方法如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 func (c *B) Error(args ...interface{}) func (c *B) Errorf(format string, args ...interface{}) func (c *B) Fail() func (c *B) FailNow() func (c *B) Failed() bool func (c *B) Fatal(args ...interface{}) func (c *B) Fatalf(format string, args ...interface{}) func (c *B) Log(args ...interface{}) func (c *B) Logf(format string, args ...interface{}) func (c *B) Name() string func (b *B) ReportAllocs() func (b *B) ResetTimer() func (b *B) Run(name string, f func(b *B)) bool func (b *B) RunParallel(body func(*PB)) func (b *B) SetBytes(n int64) func (b *B) SetParallelism(p int) func (c *B) Skip(args ...interface{}) func (c *B) SkipNow() func (c *B) Skipf(format string, args ...interface{}) func (c *B) Skipped() bool func (b *B) StartTimer() func (b *B) StopTimer() 为split包中的 Split函数编写基准测试如下：\n1 2 3 4 5 func BenchmarkSplit(b *testing.B) { for i := 0; i \u0026lt; b.N; i++ { Split(\u0026#34;沙河有沙又有河\u0026#34;, \u0026#34;沙\u0026#34;) } } **基准测试并不会默认执行，需要增加 -bench参数，**所以我们通过执行 go test -bench=Split命令执行基准测试，输出结果如下：\n1 2 3 4 5 6 7 go test -bench=Split goos: darwin goarch: amd64 pkg: github.com/Q1mi/studygo/code_demo/test_demo/split BenchmarkSplit-8 10000000 203 ns/op PASS ok github.com/Q1mi/studygo/code_demo/test_demo/split 2.255s 其中 BenchmarkSplit-8表示对Split函数进行基准测试，数字 8表示 GOMAXPROCS的值，这个对于并发基准测试很重要。10000000和 203ns/op表示每次调用 Split函数耗时 203ns，这个结果是 10000000次调用的平均值。\n我们还可以为基准测试添加 -benchmem参数，来获得内存分配的统计数据。\n1 2 3 4 5 6 7 split $ go test -bench=Split -benchmem goos: darwin goarch: amd64 pkg: github.com/Q1mi/studygo/code_demo/test_demo/split BenchmarkSplit-8 10000000 215 ns/op 112 B/op 3 allocs/op PASS ok github.com/Q1mi/studygo/code_demo/test_demo/split 2.394s 其中，112 B/op表示每次操作内存分配了112字节，3 allocs/op则表示每次操作进行了3次内存分配。然后提前使用make函数将result初始化为一个容量足够大的切片，而不再像之前一样通过调用append函数来追加。（Count用于统计一个字符在字符串中出现的总次数）\n1 2 3 4 5 6 7 8 9 10 11 func Split(s, sep string) (result []string) { result = make([]string, 0, strings.Count(s, sep)+1) i := strings.Index(s, sep) for i \u0026gt; -1 { result = append(result, s[:i]) s = s[i+len(sep):] // 这里使用len(sep)获取sep的长度 i = strings.Index(s, sep) } result = append(result, s) return } 1 2 3 4 5 6 7 split $ go test -bench=Split -benchmem goos: darwin goarch: amd64 pkg: github.com/Q1mi/studygo/code_demo/test_demo/split BenchmarkSplit-8 10000000 127 ns/op 48 B/op 1 allocs/op PASS ok github.com/Q1mi/studygo/code_demo/test_demo/split 1.423s 这个使用make函数提前分配内存的改动，减少了2/3的内存分配次数，并且减少了一半的内存分配。因此尽量少让系统动态分配切片或者map的大小，它在map或切片不足时进行扩容就会再申请一次内存，会使程序运行变慢\n性能比较函数 上面的基准测试只能得到给定操作的绝对耗时，但是在很多性能问题是发生在两个不同操作之间的相对耗时，比如同一个函数处理1000个元素的耗时与处理1万甚至100万个元素的耗时的差别是多少？再或者对于同一个任务究竟使用哪种算法性能最佳？我们通常需要对两个不同算法的实现使用相同的输入来进行基准比较测试。\n性能比较函数通常是一个带有参数的函数，被多个不同的Benchmark函数传入不同的值来调用。举个例子如下：\n1 2 3 4 func benchmark(b *testing.B, size int){/* ... */} func Benchmark10(b *testing.B){ benchmark(b, 10) } func Benchmark100(b *testing.B){ benchmark(b, 100) } func Benchmark1000(b *testing.B){ benchmark(b, 1000) } 1 2 3 4 5 6 7 8 9 10 11 func benchmarkFib(b *testing.B, n int) { for i := 0; i \u0026lt; b.N; i++ { Fib(n) } } func BenchmarkFib1(b *testing.B) { benchmarkFib(b, 1) } func BenchmarkFib2(b *testing.B) { benchmarkFib(b, 2) } func BenchmarkFib3(b *testing.B) { benchmarkFib(b, 3) } func BenchmarkFib10(b *testing.B) { benchmarkFib(b, 10) } func BenchmarkFib20(b *testing.B) { benchmarkFib(b, 20) } func BenchmarkFib40(b *testing.B) { benchmarkFib(b, 40) } 运行基准测试：\n1 2 3 4 5 6 7 8 9 10 11 12 split $ go test -bench=. goos: darwin goarch: amd64 pkg: github.com/Q1mi/studygo/code_demo/test_demo/fib BenchmarkFib1-8 1000000000 2.03 ns/op BenchmarkFib2-8 300000000 5.39 ns/op BenchmarkFib3-8 200000000 9.71 ns/op BenchmarkFib10-8 5000000 325 ns/op BenchmarkFib20-8 30000 42460 ns/op BenchmarkFib40-8 2 638524980 ns/op PASS ok github.com/Q1mi/studygo/code_demo/test_demo/fib 12.944s **这里需要注意的是，默认情况下，每个基准测试至少运行1秒。**如果在Benchmark函数返回时没有到1秒，则b.N的值会按1,2,5,10,20,50，…增加，并且函数再次运行。\n最终的BenchmarkFib40只运行了两次，每次运行的平均值只有不到一秒。像这种情况下我们应该可以使用 -benchtime标志增加最小基准时间，以产生更准确的结果。例如：\n1 2 3 4 5 6 7 split $ go test -bench=Fib40 -benchtime=20s goos: darwin goarch: amd64 pkg: github.com/Q1mi/studygo/code_demo/test_demo/fib BenchmarkFib40-8 50 663205114 ns/op PASS ok github.com/Q1mi/studygo/code_demo/test_demo/fib 33.849s 这一次 BenchmarkFib40函数运行了50次，结果就会更准确一些了。\n**使用性能比较函数做测试的时候一个容易犯的错误就是把 b.N作为输入的大小，**b.N是系统跑满一秒的次数，它是不确定的，不能这样使用\n重置时间 b.ResetTimer之前的处理不会放到执行时间里，也不会输出到报告中，所以可以在之前做一些不计划作为测试报告的操作。例如：\n1 2 3 4 5 6 7 func BenchmarkSplit(b *testing.B) { time.Sleep(5 * time.Second) // 假设需要做一些耗时的无关操作 b.ResetTimer() // 重置计时器 for i := 0; i \u0026lt; b.N; i++ { Split(\u0026#34;沙河有沙又有河\u0026#34;, \u0026#34;沙\u0026#34;) } } 并行测试 func (b *B) RunParallel(body func(*PB))会以并行的方式执行给定的基准测试。\nRunParallel会创建出多个 goroutine，并将 b.N分配给这些 goroutine执行， 其中 goroutine数量的默认值为 GOMAXPROCS。用户如果想要增加非CPU受限（non-CPU-bound）基准测试的并行性， 那么可以在 RunParallel之前调用 SetParallelism 。RunParallel通常会与 -cpu标志一同使用。\n1 2 3 4 5 6 7 8 func BenchmarkSplitParallel(b *testing.B) { // b.SetParallelism(1) // 设置使用的CPU数 b.RunParallel(func(pb *testing.PB) { for pb.Next() { Split(\u0026#34;沙河有沙又有河\u0026#34;, \u0026#34;沙\u0026#34;) } }) } 执行一下基准测试：\n1 2 3 4 5 6 7 8 split $ go test -bench=. goos: darwin goarch: amd64 pkg: github.com/Q1mi/studygo/code_demo/test_demo/split BenchmarkSplit-8 10000000 131 ns/op BenchmarkSplitParallel-8 50000000 36.1 ns/op PASS ok github.com/Q1mi/studygo/code_demo/test_demo/split 3.308s 还可以通过在测试命令后添加 -cpu参数如 go test -bench=. -cpu 1来指定使用的CPU数量。\n","date":"2020-11-07T00:00:00Z","permalink":"https://xiaonuoz.github.io/p/go%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95/","title":"Go单元测试"},{"content":"Redis是一个开源的内存数据库，Redis提供了多种不同类型的数据结构，很多业务场景下的问题都可以很自然地映射到这些数据结构上。除此之外，通过复制、持久化和客户端分片等特性，我们可以很方便地将Redis扩展成一个能够包含数百GB数据、每秒处理上百万次请求的系统。\n可以读《redis实战》这本书来进阶\nRedis支持的数据结构 Redis支持诸如字符串（strings）、哈希（hashes）、列表（lists）、集合（sets）、带范围查询的排序集合（sorted sets）、位图（bitmaps）、hyperloglogs、带半径查询和流的地理空间索引等数据结构（geospatial indexes）。\nRedis应用场景 缓存系统，减轻主数据库（MySQL）的压力。 计数场景，比如微博、抖音中的关注数和粉丝数。 热门排行榜，需要排序的场景特别适合使用ZSET。 利用LIST可以实现队列的功能。 准备Redis环境 这里直接使用Docker启动一个redis环境。\ndocker启动一个名为redis507的5.0.7版本的redis server示例：\n1 docker run --name redis507 -p 6379:6379 -d redis:5.0.7 **注意：**此处的版本、容器名和端口号请根据自己需要设置。\n启动一个redis-cli连接上面的redis server:\n1 docker run -it --network host --rm redis:5.0.7 redis-cli go-redis库 安装 区别于另一个比较常用的Go语言redis client库：redigo，我们这里采用https://github.com/go-redis/redis连接Redis数据库并进行操作，因为 go-redis支持连接哨兵及集群模式的Redis。\n使用以下命令下载并安装:\n1 go get -u github.com/go-redis/redis 连接 普通连接 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // 声明一个全局的rdb变量 var rdb *redis.Client // 初始化连接 func initClient() (err error) { rdb = redis.NewClient(\u0026amp;redis.Options{ Addr: \u0026#34;localhost:6379\u0026#34;, Password: \u0026#34;\u0026#34;, // no password set DB: 0, // use default DB }) _, err = rdb.Ping().Result() if err != nil { return err } return nil } V8新版本相关 最新版本的 go-redis库的相关命令都需要传递 context.Context参数，例如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/go-redis/redis/v8\u0026#34; // 注意导入的是新版本 ) var ( rdb *redis.Client ) // 初始化连接 func initClient() (err error) { rdb = redis.NewClient(\u0026amp;redis.Options{ Addr: \u0026#34;localhost:16379\u0026#34;, Password: \u0026#34;\u0026#34;, // no password set DB: 0, // use default DB PoolSize: 100, // 连接池大小 }) ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second) defer cancel() _, err = rdb.Ping(ctx).Result() return err } func V8Example() { ctx := context.Background() if err := initClient(); err != nil { return } err := rdb.Set(ctx, \u0026#34;key\u0026#34;, \u0026#34;value\u0026#34;, 0).Err() if err != nil { panic(err) } val, err := rdb.Get(ctx, \u0026#34;key\u0026#34;).Result() if err != nil { panic(err) } fmt.Println(\u0026#34;key\u0026#34;, val) val2, err := rdb.Get(ctx, \u0026#34;key2\u0026#34;).Result() if err == redis.Nil { fmt.Println(\u0026#34;key2 does not exist\u0026#34;) } else if err != nil { panic(err) } else { fmt.Println(\u0026#34;key2\u0026#34;, val2) } // Output: key value // key2 does not exist } 连接Redis哨兵模式 1 2 3 4 5 6 7 8 9 10 11 func initClient()(err error){ rdb := redis.NewFailoverClient(\u0026amp;redis.FailoverOptions{ MasterName: \u0026#34;master\u0026#34;, SentinelAddrs: []string{\u0026#34;x.x.x.x:26379\u0026#34;, \u0026#34;xx.xx.xx.xx:26379\u0026#34;, \u0026#34;xxx.xxx.xxx.xxx:26379\u0026#34;}, }) _, err = rdb.Ping().Result() if err != nil { return err } return nil } 连接Redis集群 1 2 3 4 5 6 7 8 9 10 func initClient()(err error){ rdb := redis.NewClusterClient(\u0026amp;redis.ClusterOptions{ Addrs: []string{\u0026#34;:7000\u0026#34;, \u0026#34;:7001\u0026#34;, \u0026#34;:7002\u0026#34;, \u0026#34;:7003\u0026#34;, \u0026#34;:7004\u0026#34;, \u0026#34;:7005\u0026#34;}, }) _, err = rdb.Ping().Result() if err != nil { return err } return nil } 基本使用 set/get示例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 func redisExample() { err := rdb.Set(\u0026#34;score\u0026#34;, 100, 0).Err() if err != nil { fmt.Printf(\u0026#34;set score failed, err:%v\\n\u0026#34;, err) return } val, err := rdb.Get(\u0026#34;score\u0026#34;).Result() if err != nil { fmt.Printf(\u0026#34;get score failed, err:%v\\n\u0026#34;, err) return } fmt.Println(\u0026#34;score\u0026#34;, val) val2, err := rdb.Get(\u0026#34;name\u0026#34;).Result() if err == redis.Nil { fmt.Println(\u0026#34;name does not exist\u0026#34;) } else if err != nil { fmt.Printf(\u0026#34;get name failed, err:%v\\n\u0026#34;, err) return } else { fmt.Println(\u0026#34;name\u0026#34;, val2) } } zset示例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 func redisExample2() { zsetKey := \u0026#34;language_rank\u0026#34; languages := []redis.Z{ redis.Z{Score: 90.0, Member: \u0026#34;Golang\u0026#34;}, redis.Z{Score: 98.0, Member: \u0026#34;Java\u0026#34;}, redis.Z{Score: 95.0, Member: \u0026#34;Python\u0026#34;}, redis.Z{Score: 97.0, Member: \u0026#34;JavaScript\u0026#34;}, redis.Z{Score: 99.0, Member: \u0026#34;C/C++\u0026#34;}, } // ZADD num, err := rdb.ZAdd(zsetKey, languages...).Result() if err != nil { fmt.Printf(\u0026#34;zadd failed, err:%v\\n\u0026#34;, err) return } fmt.Printf(\u0026#34;zadd %d succ.\\n\u0026#34;, num) // 把Golang的分数加10 newScore, err := rdb.ZIncrBy(zsetKey, 10.0, \u0026#34;Golang\u0026#34;).Result() if err != nil { fmt.Printf(\u0026#34;zincrby failed, err:%v\\n\u0026#34;, err) return } fmt.Printf(\u0026#34;Golang\u0026#39;s score is %f now.\\n\u0026#34;, newScore) // 取分数最高的3个 ret, err := rdb.ZRevRangeWithScores(zsetKey, 0, 2).Result() if err != nil { fmt.Printf(\u0026#34;zrevrange failed, err:%v\\n\u0026#34;, err) return } for _, z := range ret { fmt.Println(z.Member, z.Score) } // 取95~100分的 op := redis.ZRangeBy{ Min: \u0026#34;95\u0026#34;, Max: \u0026#34;100\u0026#34;, } ret, err = rdb.ZRangeByScoreWithScores(zsetKey, op).Result() if err != nil { fmt.Printf(\u0026#34;zrangebyscore failed, err:%v\\n\u0026#34;, err) return } for _, z := range ret { fmt.Println(z.Member, z.Score) } } 输出结果如下：\n1 2 3 4 5 6 7 8 9 10 $ ./06redis_demo zadd 0 succ. Golang\u0026#39;s score is 100.000000 now. Golang 100 C/C++ 99 Java 98 JavaScript 97 Java 98 C/C++ 99 Golang 100 根据前缀获取Key 1 vals, err := rdb.Keys(ctx, \u0026#34;prefix*\u0026#34;).Result() 执行自定义命令 1 res, err := rdb.Do(ctx, \u0026#34;set\u0026#34;, \u0026#34;key\u0026#34;, \u0026#34;value\u0026#34;).Result() 按通配符删除key 当通配符匹配的key的数量不多时，可以使用 Keys()得到所有的key在使用 Del命令删除。 如果key的数量非常多的时候，我们可以搭配使用 Scan命令和 Del命令完成删除。\n1 2 3 4 5 6 7 8 9 10 11 ctx := context.Background() iter := rdb.Scan(ctx, 0, \u0026#34;prefix*\u0026#34;, 0).Iterator() for iter.Next(ctx) { err := rdb.Del(ctx, iter.Val()).Err() if err != nil { panic(err) } } if err := iter.Err(); err != nil { panic(err) } Pipeline Pipeline 主要是一种网络优化。它本质上意味着客户端缓冲一堆命令并一次性将它们发送到服务器。这些命令不能保证在事务中执行。这样做的好处是节省了每个命令的网络往返时间（RTT）。\nPipeline 基本示例如下：\n1 2 3 4 5 6 7 pipe := rdb.Pipeline() incr := pipe.Incr(\u0026#34;pipeline_counter\u0026#34;) pipe.Expire(\u0026#34;pipeline_counter\u0026#34;, time.Hour) _, err := pipe.Exec() fmt.Println(incr.Val(), err) 上面的代码相当于将以下两个命令一次发给redis server端执行，与不使用 Pipeline相比能减少一次RTT。\n1 2 INCR pipeline_counter EXPIRE pipeline_counts 3600 也可以使用 Pipelined：\n1 2 3 4 5 6 7 var incr *redis.IntCmd _, err := rdb.Pipelined(func(pipe redis.Pipeliner) error { incr = pipe.Incr(\u0026#34;pipelined_counter\u0026#34;) pipe.Expire(\u0026#34;pipelined_counter\u0026#34;, time.Hour) return nil }) fmt.Println(incr.Val(), err) 在某些场景下，当我们有多条命令要执行时，就可以考虑使用pipeline来优化。\n事务 Redis是单线程的，因此单个命令始终是原子的，但是来自不同客户端的两个给定命令可以依次执行，例如在它们之间交替执行。但是，Multi/exec能够确保在 multi/exec两个语句之间的命令之间没有其他客户端正在执行命令。\n在这种场景我们需要使用 TxPipeline。TxPipeline总体上类似于上面的 Pipeline，但是它内部会使用 MULTI/EXEC包裹排队的命令。例如：\n1 2 3 4 5 6 7 pipe := rdb.TxPipeline() incr := pipe.Incr(\u0026#34;tx_pipeline_counter\u0026#34;) pipe.Expire(\u0026#34;tx_pipeline_counter\u0026#34;, time.Hour) _, err := pipe.Exec() fmt.Println(incr.Val(), err) 上面代码相当于在一个RTT下执行了下面的redis命令：\n1 2 3 4 MULTI INCR pipeline_counter EXPIRE pipeline_counts 3600 EXEC 还有一个与上文类似的 TxPipelined方法，使用方法如下：\n1 2 3 4 5 6 7 var incr *redis.IntCmd _, err := rdb.TxPipelined(func(pipe redis.Pipeliner) error { incr = pipe.Incr(\u0026#34;tx_pipelined_counter\u0026#34;) pipe.Expire(\u0026#34;tx_pipelined_counter\u0026#34;, time.Hour) return nil }) fmt.Println(incr.Val(), err) Watch 在某些场景下，我们除了要使用 MULTI/EXEC命令外，还需要配合使用 WATCH命令。在用户使用 WATCH命令监视某个键之后，直到该用户执行 EXEC命令的这段时间里，如果有其他用户抢先对被监视的键进行了替换、更新、删除等操作，那么当用户尝试执行 EXEC的时候，事务将失败并返回一个错误，用户可以根据这个错误选择重试事务或者放弃事务。\n1 Watch(fn func(*Tx) error, keys ...string) error Watch方法接收一个函数和一个或多个key作为参数。基本使用示例如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 // 监视watch_count的值，并在值不变的前提下将其值+1 key := \u0026#34;watch_count\u0026#34; err = client.Watch(func(tx *redis.Tx) error { n, err := tx.Get(key).Int() if err != nil \u0026amp;\u0026amp; err != redis.Nil { return err } _, err = tx.Pipelined(func(pipe redis.Pipeliner) error { pipe.Set(key, n+1, 0) return nil }) return err }, key) 最后看一个官方文档中使用GET和SET命令以事务方式递增Key的值的示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 const routineCount = 100 increment := func(key string) error { txf := func(tx *redis.Tx) error { // 获得当前值或零值 n, err := tx.Get(key).Int() if err != nil \u0026amp;\u0026amp; err != redis.Nil { return err } // 实际操作（乐观锁定中的本地操作） n++ // 仅在监视的Key保持不变的情况下运行 _, err = tx.Pipelined(func(pipe redis.Pipeliner) error { // pipe 处理错误情况 pipe.Set(key, n, 0) return nil }) return err } for retries := routineCount; retries \u0026gt; 0; retries-- { err := rdb.Watch(txf, key) if err != redis.TxFailedErr { return err } // 乐观锁丢失 } return errors.New(\u0026#34;increment reached maximum number of retries\u0026#34;) } var wg sync.WaitGroup wg.Add(routineCount) for i := 0; i \u0026lt; routineCount; i++ { go func() { defer wg.Done() if err := increment(\u0026#34;counter3\u0026#34;); err != nil { fmt.Println(\u0026#34;increment error:\u0026#34;, err) } }() } wg.Wait() n, err := rdb.Get(\u0026#34;counter3\u0026#34;).Int() fmt.Println(\u0026#34;ended with\u0026#34;, n, err) ","date":"2020-11-04T00:00:00Z","permalink":"https://xiaonuoz.github.io/p/go%E6%93%8D%E4%BD%9Credis/","title":"Go操作Redis"},{"content":"为什么需要Context 在 Go http包的Server中，每一个请求都有一个对应的 goroutine 去处理。请求处理函数通常会启动额外的 goroutine 用来访问后端服务，比如数据库和RPC服务。用来处理一个请求的 goroutine 通常需要访问一些与请求特定的数据，比如终端用户的身份认证信息、验证相关的token、请求的截止时间。 当一个请求被取消或超时时，所有用来处理该请求的 goroutine 都应该迅速退出，然后系统才能释放这些 goroutine 占用的资源。那么如何通知程序同时退出多个关联的goroutine呢？\n通道方式 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; ) var wg sync.WaitGroup // 管道方式存在的问题： // 1. 使用全局变量在跨包调用时不容易实现规范和统一，需要维护一个共用的channel func worker(exitChan chan struct{}) { LOOP: for { fmt.Println(\u0026#34;worker\u0026#34;) time.Sleep(time.Second) select { case \u0026lt;-exitChan: // 等待接收上级通知 break LOOP default: } } wg.Done() } func main() { var exitChan = make(chan struct{}) wg.Add(1) go worker(exitChan) time.Sleep(time.Second * 3) // sleep3秒以免程序过快退出 exitChan \u0026lt;- struct{}{} // 给子goroutine发送退出信号 close(exitChan) wg.Wait() fmt.Println(\u0026#34;over\u0026#34;) } 官方版的方案 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; ) var wg sync.WaitGroup func worker(ctx context.Context) { LOOP: for { fmt.Println(\u0026#34;worker\u0026#34;) time.Sleep(time.Second) select { case \u0026lt;-ctx.Done(): // 等待上级通知 break LOOP default: } } wg.Done() } func main() { ctx, cancel := context.WithCancel(context.Background()) wg.Add(1) go worker(ctx) time.Sleep(time.Second * 3) cancel() // 通知子goroutine结束 wg.Wait() fmt.Println(\u0026#34;over\u0026#34;) } 当子goroutine又开启另外一个goroutine时，只需要将ctx传入即可：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; ) var wg sync.WaitGroup func worker(ctx context.Context) { go worker2(ctx) LOOP: for { fmt.Println(\u0026#34;worker\u0026#34;) time.Sleep(time.Second) select { case \u0026lt;-ctx.Done(): // 等待上级通知 break LOOP default: } } wg.Done() } func worker2(ctx context.Context) { LOOP: for { fmt.Println(\u0026#34;worker2\u0026#34;) time.Sleep(time.Second) select { case \u0026lt;-ctx.Done(): // 等待上级通知 break LOOP default: } } } func main() { ctx, cancel := context.WithCancel(context.Background()) wg.Add(1) go worker(ctx) time.Sleep(time.Second * 3) cancel() // 通知子goroutine结束 wg.Wait() fmt.Println(\u0026#34;over\u0026#34;) } Context初识 Go1.7加入了一个新的标准库 context，它定义了 Context类型，专门用来简化 对于处理单个请求的多个 goroutine 之间与请求域的数据、取消信号、截止时间等相关操作，这些操作可能涉及多个 API 调用。\n对服务器传入的请求应该创建上下文，而对服务器的传出调用应该接受上下文。它们之间的函数调用链必须传递上下文，或者可以使用 WithCancel、WithDeadline、WithTimeout或 WithValue创建的派生上下文。当一个上下文被取消时，它派生的所有上下文也被取消。\nContext接口 context.Context是一个接口，该接口定义了四个需要实现的方法。具体签名如下：\n1 2 3 4 5 6 type Context interface { Deadline() (deadline time.Time, ok bool) Done() \u0026lt;-chan struct{} Err() error Value(key interface{}) interface{} } 其中：\nDeadline方法需要返回当前 Context被取消的时间，也就是完成工作的截止时间（deadline）； Done方法需要返回一个 Channel，这个Channel会在当前工作完成或者上下文被取消之后关闭，多次调用 Done方法会返回同一个Channel； Err方法会返回当前 Context结束的原因，它只会在 Done返回的Channel被关闭时才会返回非空的值； 如果当前 Context被取消就会返回 Canceled错误； 如果当前 Context超时就会返回 DeadlineExceeded错误； Value方法会从 Context中返回键对应的值，对于同一个上下文来说，多次调用 Value 并传入相同的 Key会返回相同的结果，该方法仅用于传递跨API和进程间跟请求域的数据； Background()和TODO() Go内置两个函数：Background()和 TODO()，这两个函数分别返回一个实现了 Context接口的 background和 todo。我们代码中最开始都是以这两个内置的上下文对象作为最顶层的 partent context，衍生出更多的子上下文对象。\nBackground()主要用于main函数、初始化以及测试代码中，作为Context这个树结构的最顶层的Context，也就是根Context。\nTODO()，它目前还不知道具体的使用场景，如果我们不知道该使用什么Context的时候，可以使用这个。\nbackground和 todo本质上都是 emptyCtx结构体类型，是一个不可取消，没有设置截止时间，没有携带任何值的Context。\nWith系列函数 此外，context包中还定义了四个With系列函数。\nWithCancel WithCancel的函数签名如下：\n1 func WithCancel(parent Context) (ctx Context, cancel CancelFunc) WithCancel返回带有新Done通道的父节点的副本。当调用返回的cancel函数或当关闭父上下文的Done通道时，将关闭返回上下文的Done通道，无论先发生什么情况。\n取消此上下文将释放与其关联的资源，因此代码应该在此上下文中运行的操作完成后立即调用cancel。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 func gen(ctx context.Context) \u0026lt;-chan int { dst := make(chan int) n := 1 go func() { for { select { case \u0026lt;-ctx.Done(): return // return结束该goroutine，防止泄露 case dst \u0026lt;- n: n++ } } }() return dst } func main() { ctx, cancel := context.WithCancel(context.Background()) defer cancel() // 当我们取完需要的整数后调用cancel for n := range gen(ctx) { fmt.Println(n) if n == 5 { break } } } 上面的示例代码中，gen函数在单独的goroutine中生成整数并将它们发送到返回的通道。 gen的调用者在使用生成的整数之后需要取消上下文，以免 gen启动的内部goroutine发生泄漏。\n调用cancel()函数就会往Done()中发送信息，使Done中有值，进而通过select语句实现退出\nWithDeadline WithDeadline的函数签名如下：\n1 func WithDeadline(parent Context, deadline time.Time) (Context, CancelFunc) 返回父上下文的副本，并将deadline调整为不迟于d。如果父上下文的deadline已经早于d，则WithDeadline(parent, d)在语义上等同于父上下文。当截止日过期时，**当调用返回的cancel函数时，或者当父上下文的Done通道关闭时，返回上下文的Done通道将被关闭，**以最先发生的情况为准。\n取消此上下文将释放与其关联的资源，因此代码应该在此上下文中运行的操作完成后立即调用cancel。不调用cancel会让其上下文存活时间过长，直到垃圾回收处理它\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 func main() { d := time.Now().Add(50 * time.Millisecond) ctx, cancel := context.WithDeadline(context.Background(), d) // 尽管ctx会过期，但在任何情况下调用它的cancel函数都是很好的实践。 // 如果不这样做，可能会使上下文及其父类存活的时间超过必要的时间。 defer cancel() select { case \u0026lt;-time.After(1 * time.Second): fmt.Println(\u0026#34;overslept\u0026#34;) case \u0026lt;-ctx.Done(): fmt.Println(ctx.Err()) } } 上面的代码中，定义了一个50毫秒之后过期的deadline，然后我们调用 context.WithDeadline(context.Background(), d)得到一个上下文（ctx）和一个取消函数（cancel），然后使用一个select让主程序陷入等待：等待1秒后打印 overslept退出或者等待ctx过期后退出。 因为ctx50秒后就过期，所以 ctx.Done()会先接收到值，上面的代码会打印ctx.Err()取消原因。\nWithTimeout WithTimeout的函数签名如下：\n1 func WithTimeout(parent Context, timeout time.Duration) (Context, CancelFunc) WithTimeout= WithDeadline(parent, time.Now().Add(timeout))。\n取消此上下文将释放与其相关的资源，因此代码应该在此上下文中运行的操作完成后立即调用cancel，通常用于数据库或者网络连接的超时控制。具体示例如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; ) // context.WithTimeout var wg sync.WaitGroup func worker(ctx context.Context) { LOOP: for { fmt.Println(\u0026#34;db connecting ...\u0026#34;) time.Sleep(time.Millisecond * 10) // 假设正常连接数据库耗时10毫秒 select { case \u0026lt;-ctx.Done(): // 50毫秒后自动调用 break LOOP default: } } fmt.Println(\u0026#34;worker done!\u0026#34;) wg.Done() } func main() { // 设置一个50毫秒的超时 ctx, cancel := context.WithTimeout(context.Background(), time.Millisecond*50) wg.Add(1) go worker(ctx) time.Sleep(time.Second * 5) cancel() // 通知子goroutine结束 wg.Wait() fmt.Println(\u0026#34;over\u0026#34;) } WithValue WithValue函数能够将请求作用域的数据与 Context 对象建立关系。声明如下：\n1 func WithValue(parent Context, key, val interface{}) Context WithValue返回父节点的副本，其中与key关联的值为val。\n仅对API和进程间传递请求域的数据使用上下文值，而不是使用它来传递可选参数给函数。\n所提供的键必须是可比较的，并且不应该是 string类型或任何其他内置类型，以避免使用上下文在包之间发生冲突。WithValue的用户应该为键定义自己的类型。为了避免在分配给interface{}时进行分配，上下文键通常具有具体类型 struct{}。或者，导出的上下文关键变量的静态类型应该是指针或接口。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; ) // context.WithValue type TraceCode string var wg sync.WaitGroup func worker(ctx context.Context) { key := TraceCode(\u0026#34;TRACE_CODE\u0026#34;) traceCode, ok := ctx.Value(key).(string) // 在子goroutine中获取trace code if !ok { fmt.Println(\u0026#34;invalid trace code\u0026#34;) } LOOP: for { fmt.Printf(\u0026#34;worker, trace code:%s\\n\u0026#34;, traceCode) time.Sleep(time.Millisecond * 10) // 假设正常连接数据库耗时10毫秒 select { case \u0026lt;-ctx.Done(): // 50毫秒后自动调用 break LOOP default: } } fmt.Println(\u0026#34;worker done!\u0026#34;) wg.Done() } func main() { // 设置一个50毫秒的超时 ctx, cancel := context.WithTimeout(context.Background(), time.Millisecond*50) // 在系统的入口中设置trace code传递给后续启动的goroutine实现日志数据聚合 ctx = context.WithValue(ctx, TraceCode(\u0026#34;TRACE_CODE\u0026#34;), \u0026#34;12512312234\u0026#34;) wg.Add(1) go worker(ctx) time.Sleep(time.Second * 5) cancel() // 通知子goroutine结束 wg.Wait() fmt.Println(\u0026#34;over\u0026#34;) } 使用Context的注意事项 推荐以参数的方式显示传递Context 以Context作为参数的函数方法，应该把Context作为第一个参数。 给一个函数方法传递Context的时候，不要传递nil，如果不知道传递什么，就使用context.TODO() Context的Value相关方法应该传递请求域的必要数据，不应该用于传递可选参数 Context是线程安全的，可以放心的在多个goroutine中传递 客户端超时取消示例 调用服务端API时如何在客户端实现超时控制？\nserver端 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 // context_timeout/server/main.go package main import ( \u0026#34;fmt\u0026#34; \u0026#34;math/rand\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;time\u0026#34; ) // server端，随机出现慢响应 func indexHandler(w http.ResponseWriter, r *http.Request) { number := rand.Intn(2) if number == 0 { time.Sleep(time.Second * 10) // 耗时10秒的慢响应 fmt.Fprintf(w, \u0026#34;slow response\u0026#34;) return } fmt.Fprint(w, \u0026#34;quick response\u0026#34;) } func main() { http.HandleFunc(\u0026#34;/\u0026#34;, indexHandler) err := http.ListenAndServe(\u0026#34;:8000\u0026#34;, nil) if err != nil { panic(err) } } client端 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 // context_timeout/client/main.go package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io/ioutil\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; ) // 客户端 type respData struct { resp *http.Response err error } func doCall(ctx context.Context) { transport := http.Transport{ // 请求频繁可定义全局的client对象并启用长链接 // 请求不频繁使用短链接 DisableKeepAlives: true, } client := http.Client{ Transport: \u0026amp;transport, } respChan := make(chan *respData, 1) req, err := http.NewRequest(\u0026#34;GET\u0026#34;, \u0026#34;http://127.0.0.1:8000/\u0026#34;, nil) if err != nil { fmt.Printf(\u0026#34;new requestg failed, err:%v\\n\u0026#34;, err) return } req = req.WithContext(ctx) // 使用带超时的ctx创建一个新的client request var wg sync.WaitGroup wg.Add(1) defer wg.Wait() go func() { resp, err := client.Do(req) fmt.Printf(\u0026#34;client.do resp:%v, err:%v\\n\u0026#34;, resp, err) rd := \u0026amp;respData{ resp: resp, err: err, } respChan \u0026lt;- rd wg.Done() }() select { case \u0026lt;-ctx.Done(): //transport.CancelRequest(req) fmt.Println(\u0026#34;call api timeout\u0026#34;) case result := \u0026lt;-respChan: fmt.Println(\u0026#34;call server api success\u0026#34;) if result.err != nil { fmt.Printf(\u0026#34;call server api failed, err:%v\\n\u0026#34;, result.err) return } defer result.resp.Body.Close() data, _ := ioutil.ReadAll(result.resp.Body) fmt.Printf(\u0026#34;resp:%v\\n\u0026#34;, string(data)) } } func main() { // 定义一个100毫秒的超时 ctx, cancel := context.WithTimeout(context.Background(), time.Millisecond*100) defer cancel() // 调用cancel释放子goroutine资源 doCall(ctx) } ","date":"2020-11-02T00:00:00Z","permalink":"https://xiaonuoz.github.io/p/go%E6%A0%87%E5%87%86%E5%BA%93context/","title":"Go标准库Context"},{"content":"数据库连接 Go语言中的 database/sql包提供了保证SQL或类SQL数据库的泛用接口，并不提供具体的数据库驱动。使用 database/sql包时必须注入（至少）一个数据库驱动。\n常用的数据库基本上都有完整的第三方实现。例如：MySQL驱动\n下载依赖 1 go get -u github.com/go-sql-driver/mysql 使用MySQL驱动 1 func Open(driverName, dataSourceName string) (*DB, error) DB是一个数据库（操作）句柄，代表一个具有零到多个底层连接的连接池(go原生支持连接池)。它可以安全的被多个go程同时使用。\nsql包会自动创建和释放连接；它也会维护一个闲置连接的连接池。\nOpen打开一个dirverName指定的数据库，dataSourceName指定数据源，一般至少包括数据库文件名和其它连接必要的信息。\n1 2 3 4 5 6 7 8 9 10 11 12 import ( \u0026#34;database/sql\u0026#34; _ \u0026#34;github.com/go-sql-driver/mysql\u0026#34; ) func main() { dsn := \u0026#34;user:password@tcp(127.0.0.1:3306)/dbname\u0026#34; db, err := sql.Open(\u0026#34;mysql\u0026#34;, dsn) if err != nil { panic(err) } defer db.Close() // 注意这行代码要写在上面err判断的下面 } 初始化连接 Open函数只是验证其参数格式是否正确，实际上并不创建与数据库的连接。如果要检查数据源的名称是否真实有效，应该调用Ping方法。\n返回的DB对象可以安全地被多个goroutine并发使用，并且维护其自己的空闲连接池。因此，Open函数应该仅被调用一次，很少需要关闭这个DB对象。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 import ( \u0026#34;database/sql\u0026#34; \u0026#34;fmt\u0026#34; //这个包中的init()方法中会以一个内容为\u0026#34;mysql\u0026#34;的字符串作为key来注册一个mysql驱动， //这样在open中输入mysql就会找到对应的mysql驱动 _ \u0026#34;github.com/go-sql-driver/mysql\u0026#34; ) // 定义一个全局对象db var db *sql.DB // 定义一个初始化数据库的函数 func initDB() (err error) { // DSN:Data Source Name dsn := \u0026#34;user:password@tcp(127.0.0.1:3306)/sql_test?charset=utf8mb4\u0026amp;parseTime=True\u0026#34; // 不会校验账号密码是否正确，只会验证格式是否正确 db, err = sql.Open(\u0026#34;mysql\u0026#34;, dsn) if err != nil { return err } // 尝试与数据库建立连接（校验dsn是否正确） err = db.Ping() if err != nil { return err } return nil } func main() { err := initDB() // 调用输出化数据库的函数 if err != nil { fmt.Printf(\u0026#34;init db failed,err:%v\\n\u0026#34;, err) return } } 其中 sql.DB是表示连接的数据库对象（结构体实例），它保存了连接数据库相关的所有信息。它内部维护着一个具有零到多个底层连接的连接池，它可以安全地被多个goroutine同时使用。\nSetMaxOpenConns 1 func (db *DB) SetMaxOpenConns(n int) SetMaxOpenConns设置与数据库建立连接的最大数目。 如果n大于0且小于最大闲置连接数，会将最大闲置连接数减小到匹配最大开启连接数的限制。 如果n\u0026lt;=0，不会限制最大开启连接数，默认为0（无限制）。\nSetMaxIdleConns 1 func (db *DB) SetMaxIdleConns(n int) SetMaxIdleConns设置连接池中的最大闲置连接数。 如果n大于最大开启连接数，则新的最大闲置连接数会减小到匹配最大开启连接数的限制。 如果n\u0026lt;=0，不会保留闲置连接。\nCRUD 查询 可以事先定义好一个结构体来存储user表的数据。\n1 2 3 4 5 type user struct { id int age int name string } 单行查询 单行查询 db.QueryRow()执行一次查询，并期望返回最多一行结果（即Row）。QueryRow总是返回非nil的值，直到返回值的Scan方法被调用时，才会返回被延迟的错误。（如：未找到结果）\n1 func (db *DB) QueryRow(query string, args ...interface{}) *Row 具体示例代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 func queryRowDemo() { sqlStr := \u0026#34;select id, name, age from user where id=?\u0026#34; var u user // 注意：QueryRow之后必须调用Scan方法，否则持有的数据库链接不会被释放（Scan中有释放row连接的代码） //调用完Scan后就会将连接放回连接池中，sqlStr中的?是一个占位符，这样可以在QueryRow中动态填写对应条件语句 //由于结构体是值传递，因此如果要修改外部的值就需要传指针进去，否则修改的就是结构体的拷贝副本 err := db.QueryRow(sqlStr, 1).Scan(\u0026amp;u.id, \u0026amp;u.name, \u0026amp;u.age) if err != nil { fmt.Printf(\u0026#34;scan failed, err:%v\\n\u0026#34;, err) return } fmt.Printf(\u0026#34;id:%d name:%s age:%d\\n\u0026#34;, u.id, u.name, u.age) } 多行查询 多行查询 db.Query()执行一次查询，返回多行结果（即Rows），一般用于执行select命令。参数args表示query中的占位参数。\n1 func (db *DB) Query(query string, args ...interface{}) (*Rows, error) 具体示例代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 func queryMultiRowDemo() { sqlStr := \u0026#34;select id, name, age from user where id \u0026gt; ?\u0026#34; rows, err := db.Query(sqlStr, 0) if err != nil { fmt.Printf(\u0026#34;query failed, err:%v\\n\u0026#34;, err) return } // 注意：要关闭rows来释放持有的数据库链接,它的scan方法不会帮忙释放，所以需要自己手动添加关闭方法 // 单行查询是row结构体，多行查询是rows结构体，两者不同，scan方法也不同，rows一定要手动释放连接 defer rows.Close() // 循环读取结果集中的数据，Next()方法判断rows下一行是否有数据，有则返回true for rows.Next() { var u user err := rows.Scan(\u0026amp;u.id, \u0026amp;u.name, \u0026amp;u.age) if err != nil { fmt.Printf(\u0026#34;scan failed, err:%v\\n\u0026#34;, err) return } fmt.Printf(\u0026#34;id:%d name:%s age:%d\\n\u0026#34;, u.id, u.name, u.age) } } 插入数据 插入、更新和删除操作都使用 Exec方法。\n1 func (db *DB) Exec(query string, args ...interface{}) (Result, error) Exec执行一次命令（包括查询、删除、更新、插入等），返回的Result是对已执行的SQL命令的总结。参数args表示query中的占位参数。\n具体插入数据示例代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // 插入数据 func insertRowDemo() { sqlStr := \u0026#34;insert into user(name, age) values (?,?)\u0026#34; ret, err := db.Exec(sqlStr, \u0026#34;王五\u0026#34;, 38) if err != nil { fmt.Printf(\u0026#34;insert failed, err:%v\\n\u0026#34;, err) return } theID, err := ret.LastInsertId() // 这个方法返回新插入的数据的id(主键) if err != nil { fmt.Printf(\u0026#34;get lastinsert ID failed, err:%v\\n\u0026#34;, err) return } fmt.Printf(\u0026#34;insert success, the id is %d.\\n\u0026#34;, theID) } 更新数据 具体更新数据示例代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // 更新数据 func updateRowDemo() { sqlStr := \u0026#34;update user set age=? where id = ?\u0026#34; ret, err := db.Exec(sqlStr, 39, 3) if err != nil { fmt.Printf(\u0026#34;update failed, err:%v\\n\u0026#34;, err) return } n, err := ret.RowsAffected() // 操作影响的行数，更新数据无法使用LastInsertId方法 if err != nil { fmt.Printf(\u0026#34;get RowsAffected failed, err:%v\\n\u0026#34;, err) return } fmt.Printf(\u0026#34;update success, affected rows:%d\\n\u0026#34;, n) } 删除数据 具体删除数据的示例代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // 删除数据 func deleteRowDemo() { sqlStr := \u0026#34;delete from user where id = ?\u0026#34; ret, err := db.Exec(sqlStr, 3) if err != nil { fmt.Printf(\u0026#34;delete failed, err:%v\\n\u0026#34;, err) return } n, err := ret.RowsAffected() // 操作影响的行数 if err != nil { fmt.Printf(\u0026#34;get RowsAffected failed, err:%v\\n\u0026#34;, err) return } fmt.Printf(\u0026#34;delete success, affected rows:%d\\n\u0026#34;, n) } MySQL预处理 什么是预处理？ 普通SQL语句执行过程：\n客户端对SQL语句进行占位符替换得到完整的SQL语句。 客户端发送完整SQL语句到MySQL服务端 MySQL服务端执行完整的SQL语句并将结果返回给客户端。 预处理执行过程：\n把SQL语句分成两部分，命令部分与数据部分。 先把命令部分发送给MySQL服务端，MySQL服务端进行SQL预处理。 然后把数据部分发送给MySQL服务端，MySQL服务端对SQL语句进行占位符替换。 MySQL服务端执行完整的SQL语句并将结果返回给客户端。 为什么要预处理？ 优化MySQL服务器重复执行SQL的方法，可以提升服务器性能，提前让服务器编译，一次编译多次执行，节省后续编译的成本。(即可以只从池子中取一个连接重复利用) 避免SQL注入问题。 Go实现MySQL预处理 database/sql中使用下面的 Prepare方法来实现预处理操作。\n1 func (db *DB) Prepare(query string) (*Stmt, error) Prepare方法会先将sql语句发送给MySQL服务端，返回一个准备好的状态用于之后的查询和命令。返回值可以同时执行多个查询和命令。\n查询操作的预处理示例代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 // 预处理查询示例 func prepareQueryDemo() { sqlStr := \u0026#34;select id, name, age from user where id \u0026gt; ?\u0026#34; stmt, err := db.Prepare(sqlStr) if err != nil { fmt.Printf(\u0026#34;prepare failed, err:%v\\n\u0026#34;, err) return } defer stmt.Close() rows, err := stmt.Query(0) if err != nil { fmt.Printf(\u0026#34;query failed, err:%v\\n\u0026#34;, err) return } defer rows.Close() // 循环读取结果集中的数据 for rows.Next() { var u user err := rows.Scan(\u0026amp;u.id, \u0026amp;u.name, \u0026amp;u.age) if err != nil { fmt.Printf(\u0026#34;scan failed, err:%v\\n\u0026#34;, err) return } fmt.Printf(\u0026#34;id:%d name:%s age:%d\\n\u0026#34;, u.id, u.name, u.age) } } 插入、更新和删除操作的预处理十分类似，这里以插入操作的预处理为例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // 预处理插入示例 func prepareInsertDemo() { sqlStr := \u0026#34;insert into user(name, age) values (?,?)\u0026#34; stmt, err := db.Prepare(sqlStr) if err != nil { fmt.Printf(\u0026#34;prepare failed, err:%v\\n\u0026#34;, err) return } defer stmt.Close() _, err = stmt.Exec(\u0026#34;王五\u0026#34;, 18) if err != nil { fmt.Printf(\u0026#34;insert failed, err:%v\\n\u0026#34;, err) return } _, err = stmt.Exec(\u0026#34;赵六\u0026#34;, 18) if err != nil { fmt.Printf(\u0026#34;insert failed, err:%v\\n\u0026#34;, err) return } fmt.Println(\u0026#34;insert success.\u0026#34;) } Go实现MySQL事务 什么是事务？ **事务：一个最小的不可再分的工作单元；**通常一个事务对应一个完整的业务(例如银行账户转账业务，该业务就是一个最小的工作单元)，同时这个完整的业务需要执行多次的DML(insert、update、delete)语句共同联合完成。A转账给B，这里面就需要执行两次update操作。\n**在MySQL中只有使用了 Innodb数据库引擎的数据库或表才支持事务。**事务处理可以用来维护数据库的完整性，保证成批的SQL语句要么全部执行，要么全部不执行。\n事务的ACID 通常事务必须满足4个条件（ACID）：原子性（Atomicity，或称不可分割性）、一致性（Consistency）、隔离性（Isolation，又称独立性）、持久性（Durability）。\n条件 解释 原子性 一个事务（transaction）中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。 一致性 在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设规则，这包含资料的精确度、串联性以及后续数据库可以自发性地完成预定的工作。 隔离性 数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括读未提交（Read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（Serializable）。 持久性 事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 事务相关方法 Go语言中使用以下三个方法实现MySQL中的事务操作。 开始事务\n1 func (db *DB) Begin() (*Tx, error) 提交事务\n1 func (tx *Tx) Commit() error 回滚事务\n1 func (tx *Tx) Rollback() error 事务示例 下面的代码演示了一个简单的事务操作，该事物操作能够确保两次更新操作要么同时成功要么同时失败，不会存在中间状态。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 // 事务操作示例 func transactionDemo() { tx, err := db.Begin() // 开启事务 if err != nil { if tx != nil { tx.Rollback() // 回滚 } fmt.Printf(\u0026#34;begin trans failed, err:%v\\n\u0026#34;, err) return } sqlStr1 := \u0026#34;Update user set age=30 where id=?\u0026#34; ret1, err := tx.Exec(sqlStr1, 2) if err != nil { tx.Rollback() // 回滚 fmt.Printf(\u0026#34;exec sql1 failed, err:%v\\n\u0026#34;, err) return } //RowsAffected()返回数据的更新条数，如果为0则表示没更新数据 affRow1, err := ret1.RowsAffected() if err != nil { tx.Rollback() // 回滚 fmt.Printf(\u0026#34;exec ret1.RowsAffected() failed, err:%v\\n\u0026#34;, err) return } sqlStr2 := \u0026#34;Update user set age=40 where id=?\u0026#34; ret2, err := tx.Exec(sqlStr2, 3) if err != nil { tx.Rollback() // 回滚 fmt.Printf(\u0026#34;exec sql2 failed, err:%v\\n\u0026#34;, err) return } affRow2, err := ret2.RowsAffected() if err != nil { tx.Rollback() // 回滚 fmt.Printf(\u0026#34;exec ret1.RowsAffected() failed, err:%v\\n\u0026#34;, err) return } fmt.Println(affRow1, affRow2) if affRow1 == 1 \u0026amp;\u0026amp; affRow2 == 1 { fmt.Println(\u0026#34;事务提交啦...\u0026#34;) tx.Commit() // 提交事务 } else { tx.Rollback() fmt.Println(\u0026#34;事务回滚啦...\u0026#34;) } fmt.Println(\u0026#34;exec trans success!\u0026#34;) SQL注入问题 我们任何时候都不应该自己拼接SQL语句！\n编写一个根据name字段查询user表的函数如下：\n1 2 3 4 5 6 7 8 9 10 11 12 // sql注入示例 func sqlInjectDemo(name string) { sqlStr := fmt.Sprintf(\u0026#34;select id, name, age from user where name=\u0026#39;%s\u0026#39;\u0026#34;, name) fmt.Printf(\u0026#34;SQL:%s\\n\u0026#34;, sqlStr) var u user err := db.QueryRow(sqlStr).Scan(\u0026amp;u.id, \u0026amp;u.name, \u0026amp;u.age) if err != nil { fmt.Printf(\u0026#34;exec failed, err:%v\\n\u0026#34;, err) return } fmt.Printf(\u0026#34;user:%#v\\n\u0026#34;, u) } 此时以下输入字符串都可以引发SQL注入问题：\n1 2 3 sqlInjectDemo(\u0026#34;xxx\u0026#39; or 1=1#\u0026#34;)\t//1=1就是true，就可以查询到库的所有数据 sqlInjectDemo(\u0026#34;xxx\u0026#39; union select * from user #\u0026#34;) sqlInjectDemo(\u0026#34;xxx\u0026#39; and (select count(*) from user) \u0026lt;10 #\u0026#34;) 它在xxx后面加个 '会和前面的配套，然后最后的单引号用注释去掉。这样就能查询到其他数据。\n要避免注入情况，可以用预处理先将语句传给mysql。尽量少让用户自己输入数据，用选项替代。\n","date":"2020-11-02T00:00:00Z","permalink":"https://xiaonuoz.github.io/p/go%E6%93%8D%E4%BD%9Cmysql/","title":"Go操作MySQL"},{"content":"\n组件介绍 LogAgent:日志收集客户端，用来收集服务器上所有服务的日志，然后发往kafka。它根据配置项来收集日志，将配置项存储etcd中。用web管理页面来管理配置项，当扩容或增加服务时只需要在web上设置就行\nKafka:高吞吐量的分布式队列\nElasticSearch:开源的搜索引擎，提供基于HTTP RESTful的web接口，给日志信息提供索引，让日志支持检索\nKibaa:开源的ES数据分析和可视化工具。\nHadoop:分布式计算框架，能够对大量数据进行分布式处理的平台\nStorm:一个免费并开源的分布式实时计算系统\n通过Hadoop和Storm可以进行实时数据计算，来分析数据。\n消息队列的两种通信模式 点对点模式（queue） 消息生产者生产消息发送到queue中，然后消息消费者从queue中取出并且消费消息。一条消息被消费以后，queue中就没有了，不存在重复消费。\n发布/订阅模式（topic） 消息生产者（发布）将消息发布到topic中，同时有多个消费者（订阅）消费该消息。和点对点方式不同，发布到topic的消息会被所有订阅者消费（类似于关注了微信公众号的人都能收到推送的文章）\n注意：发布订阅模式下，如果发布者发布的消息量很大时，单个订阅者的处理能力是不足的（接受缓慢）。因此在现实场景中是多个订阅者节点组成一个订阅组负载均衡来消费topic消息**（分组订阅）**，这样订阅者很容易实现消费能力线性扩展。可以看成是一个topic下有多个Queue，每个Queue是点对点的方式，Queue之间是发布订阅方式（这样每个Queue都轮询接受topic的消息，每个Queue接受部分消息）\nKafka Kafka是一个分布式的数据流平台，可以运行在单台服务器上，也可以在多台服务器上部署形成集群，它提供了发布和订阅功能，使用者可以发送数据到Kafka中，也可以从Kafka中读取数据（以便进行后续的处理），Kafka具有高吞吐、低延迟、高容错等特点\n工作流程 一定要记住：Producer在写入数据的时候是把数据写入到leader中，不会直接将数据写入follower。\n选择partition的原则 ACK应答机制 Topic和数据日志 Partition结构 logAgent读取配置文件版：https://github.com/XiaoNuoZ/logAgentByConfig\netcd etcd是使用Go语言开发的一个开源的、高可用的分布式key-value存储系统，可以用于配置共享和服务的注册和发现（微服务）\netcd具有以下特点：\n完全复制：集群中的每个节点都可以使用完整的存档 高可用性：etcd可用于避免硬件的单点故障或网络问题 一致性：每次读取都会返回跨多主机的最新写入 简单：包括一个定义良好、面向用户的API（gRPC） 安全：实现了带有可选的客户端证书身份验证的自动化TLS 快速：每秒10000次写入的基准速度 可靠：使用Raft算法实现了强一致、高可用 etcd使用场景 将配置信息放到etcd上集中管理的使用方式：应用在启动的时候主动从etcd获取一次配置信息，同时在etcd节点上注册一个Watcher并等待，以后每次配置有更新的时候，etcd都会实时通知订阅者，一次达到获取最新配置信息的目的\netcd也经常用于服务发现，即在同一个分布式集群中的进程或服务，要如何才能找到对方并建立连接？本质上来说，服务发现就是了解集群中是否有进程在监听udp或tcp端口，并且通过名字就可以查找和连接\netcd原理 Raft协议：Raft协议原理详解\netcd的搭建 为了避免在生产环境中有多个集群，因此使用token来区分，CLUSTER_STATE是状态的意思，new表示新建一个节点，CLUSTER表示它属于哪个集群\netcd的下载启动 1 2 下载：https://github.com/etcd-io/etcd/releases 启动：解压后直接对etcd.exe右键管理员运行即可（普通的cmd不是管理员运行，可能会出错） 注意：连接etcd时，默认的etcdctl使用的是v2版本的命令，需要设置环境变量 SET ETCDCTL_API=3 来使用v3版本的API，而默认的 ETCDCTL_API=2是使用v2版本的API（针对老版本，最新版已修复）\netcd的常用的有put(设值)、get(取值)、watch(监听)\n1 2 etcdctl.exe --endpoints=http://127.0.0.1:2379 put xiaonuo 23\t//设置key和value etcdctl.exe --endpoints=http://127.0.0.1:2379 get xiaonuo\t//通过Key取值 watch用来获取未来更改 的通知，当值发生改变便会通知\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;go.etcd.io/etcd/clientv3\u0026#34; \u0026#34;time\u0026#34; ) func main(){ //etcd连接 cli,err:=clientv3.New(clientv3.Config{ Endpoints: []string{\u0026#34;127.0.0.1:2379\u0026#34;},\t//连接对象 DialTimeout: 5*time.Second,\t//5秒没连接上就超时返回err }) if err!=nil{ fmt.Println(\u0026#34;connect to etcd faild,err:\u0026#34;,err) return } defer cli.Close() //etcd Put ctx,cancel:=context.WithTimeout(context.Background(),time.Second) _,err=cli.Put(ctx,\u0026#34;xiaonuo\u0026#34;,\u0026#34;23\u0026#34;) //这里没开携程，因此会put执行完才执行cancel()，put如果一秒后还没有成功就会自动结束上下文，cancel()是避免等待直到垃圾回收结束它。 cancel() if err!=nil{ fmt.Println(\u0026#34;put to etcd faild,err:\u0026#34;,err) return } //etcd Get ctx,cancel=context.WithTimeout(context.Background(),time.Second) //Get(ctx context.Context, key string, opts ...OpOption)中的第三个选项是clientv3.WithPrefix()，它可以给key定义一个前缀，方便整理 resp,err:=cli.Get(ctx,\u0026#34;xiaonuo\u0026#34;) cancel() if err!=nil{ fmt.Println(\u0026#34;get to etcd faild,err:\u0026#34;,err) return } for _,ev:=range resp.Kvs{ fmt.Println(\u0026#34;key:\u0026#34;,string(ev.Key),\u0026#34;----value:\u0026#34;,string(ev.Value)) } //etcd Watch，watch会派一个哨兵一直监听着key的value的变化(新增、修改、删除) //watch返回的是一个channel,\u0026lt;-chan WatchResponse //也可以设置WithTimeout()来让它监视一个固定时间或设置条件让其取消监视（不过还是建议让其一直监视着） rch:=cli.Watch(context.Background(),\u0026#34;xiaonuo\u0026#34;) //从通道尝试取值，range会在没值的时候阻塞，有值后执行内部代码，然后循环回来继续阻塞等待下一个值 for wresp:=range rch{ for _,ev:=range wresp.Events{ //type可以取到类型，是删除修改还是新增 fmt.Println(\u0026#34;type:\u0026#34;,string(ev.Type),\u0026#34;----key:\u0026#34;,string(ev.Kv.Key),\u0026#34;----value:\u0026#34;,string(ev.Kv.Value)) } } } etcd的续期 etcd可以设置续期，如果续期时间超过则直接数据失效\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 cli, err := clientv3.New(clientv3.Config{ Endpoints: []string{\u0026#34;127.0.0.1:2379\u0026#34;}, DialTimeout: time.Second, }) if err != nil { log.Fatal(err) } defer cli.Close() //设置续期5秒 resp, err := cli.Grant(context.TODO(), 5) if err != nil { log.Fatal(err) } // 将 k-v 设置到etcd _, err = cli.Put(context.TODO(), \u0026#34;root\u0026#34;, \u0026#34;admin\u0026#34;, clientv3.WithLease(resp.ID)) if err != nil { log.Fatal(err) } // 若想一直有效，设置自动续期,关闭此程序后数据5s后消失 ch, err := cli.KeepAlive(context.TODO(), resp.ID) if err != nil { log.Fatal(err) } for { //返回续期信息 c := \u0026lt;-ch fmt.Println(\u0026#34;c:\u0026#34;, c) } Elasticsearch和Kibana 1 2 https://www.elastic.co/cn/elasticsearch/ https://www.elastic.co/cn/kibana 要注意的是：es和kibana版本要一致，否则可能出现不兼容的问题\nkibana是一个开源的分析和可视化平台，设计出来用于和Eleasticsearch一起工作。\n它可以查看、搜索并和存储在Eleastucsearch索引中的数据进行交互。以各种图标、表格和地图的形式可视化数据\nes解压后可以直接启动elasticsearch.bat。\nkibana解压后要先更改配置文件kibana.yml，将里面的elasticsearch.hosts设置为自己的elasticsearch的地址，然后修改i18n.locale的值为zh-CN\nlogAgent实现源代码： https://github.com/XiaoNuoZ/logAgentByConfig\n","date":"2020-11-02T00:00:00Z","permalink":"https://xiaonuoz.github.io/p/%E9%80%9A%E8%BF%87kafka%E5%92%8Cetcd%E5%AE%9E%E7%8E%B0%E5%A4%9A%E4%B8%AA%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86%E5%92%8C%E6%A3%80%E7%B4%A2/","title":"通过kafka和etcd实现多个服务器的日志收集和检索"},{"content":"sqlx可以认为是Go语言内置 database/sql的超集，它在内置 database/sql基础上提供了一组扩展。这些扩展中除了常用来查询的 Get(dest interface{}, ...) error和 Select(dest interface{}, ...) error外还有很多其他强大的功能。\n安装sqlx 1 go get github.com/jmoiron/sqlx 基本使用 连接数据库 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 var db *sqlx.DB func initDB() (err error) { dsn := \u0026#34;user:password@tcp(127.0.0.1:3306)/sql_test?charset=utf8mb4\u0026amp;parseTime=True\u0026#34; // 也可以使用MustConnect连接不成功就panic //sqlx不需要进行Ping()来判断是否连接成功，它在connect同时也进行了Ping() db, err = sqlx.Connect(\u0026#34;mysql\u0026#34;, dsn) if err != nil { fmt.Printf(\u0026#34;connect DB failed, err:%v\\n\u0026#34;, err) return } db.SetMaxOpenConns(20) db.SetMaxIdleConns(10) return } 查询 查询单行数据示例代码如下：\n1 2 3 4 5 6 7 8 9 10 11 // 查询单条数据示例 func queryRowDemo() { sqlStr := \u0026#34;select id, name, age from user where id=?\u0026#34; var u user err := db.Get(\u0026amp;u, sqlStr, 1) if err != nil { fmt.Printf(\u0026#34;get failed, err:%v\\n\u0026#34;, err) return } fmt.Printf(\u0026#34;id:%d name:%s age:%d\\n\u0026#34;, u.ID, u.Name, u.Age) } 查询多行数据示例代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 // 查询多条数据示例 func queryMultiRowDemo() { sqlStr := \u0026#34;select id, name, age from user where id \u0026gt; ?\u0026#34; //此处切片不make初始化是因为在select()中会帮忙进行初始化操作 var users []user err := db.Select(\u0026amp;users, sqlStr, 0) if err != nil { fmt.Printf(\u0026#34;query failed, err:%v\\n\u0026#34;, err) return } fmt.Printf(\u0026#34;users:%#v\\n\u0026#34;, users) } 注意：sqlx通过反射来获取结构体的成员或切片等变量，将查询出的结果赋值给结构体或切片，因此结构体成员必须首字母大写（公开的变量），并且反射中有判断如果参数不是指针类型就直接return，因此即使切片是引用传递也需要传指针进去。\n插入、更新和删除 sqlx中的exec方法与原生sql中的exec使用基本一致：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 // 插入数据 func insertRowDemo() { sqlStr := \u0026#34;insert into user(name, age) values (?,?)\u0026#34; ret, err := db.Exec(sqlStr, \u0026#34;笑傩\u0026#34;, 23) if err != nil { fmt.Printf(\u0026#34;insert failed, err:%v\\n\u0026#34;, err) return } theID, err := ret.LastInsertId() // 新插入数据的id if err != nil { fmt.Printf(\u0026#34;get lastinsert ID failed, err:%v\\n\u0026#34;, err) return } fmt.Printf(\u0026#34;insert success, the id is %d.\\n\u0026#34;, theID) } // 更新数据 func updateRowDemo() { sqlStr := \u0026#34;update user set age=? where id = ?\u0026#34; ret, err := db.Exec(sqlStr, 39, 6) if err != nil { fmt.Printf(\u0026#34;update failed, err:%v\\n\u0026#34;, err) return } n, err := ret.RowsAffected() // 操作影响的行数 if err != nil { fmt.Printf(\u0026#34;get RowsAffected failed, err:%v\\n\u0026#34;, err) return } fmt.Printf(\u0026#34;update success, affected rows:%d\\n\u0026#34;, n) } // 删除数据 func deleteRowDemo() { sqlStr := \u0026#34;delete from user where id = ?\u0026#34; ret, err := db.Exec(sqlStr, 6) if err != nil { fmt.Printf(\u0026#34;delete failed, err:%v\\n\u0026#34;, err) return } n, err := ret.RowsAffected() // 操作影响的行数 if err != nil { fmt.Printf(\u0026#34;get RowsAffected failed, err:%v\\n\u0026#34;, err) return } fmt.Printf(\u0026#34;delete success, affected rows:%d\\n\u0026#34;, n) } NamedExec DB.NamedExec方法用来绑定SQL语句与结构体或map中的同名字段。\n1 2 3 4 5 6 7 8 9 func insertUserDemo()(err error){ sqlStr := \u0026#34;INSERT INTO user (name,age) VALUES (:name,:age)\u0026#34; _, err = db.NamedExec(sqlStr, map[string]interface{}{ \u0026#34;name\u0026#34;: \u0026#34;笑傩\u0026#34;, \u0026#34;age\u0026#34;: 23, }) return } NamedQuery 与 DB.NamedExec同理，这里是支持查询。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 func namedQuery(){ sqlStr := \u0026#34;SELECT * FROM user WHERE name=:name\u0026#34; // 使用map做命名查询 rows, err := db.NamedQuery(sqlStr, map[string]interface{}{\u0026#34;name\u0026#34;: \u0026#34;笑傩\u0026#34;}) if err != nil { fmt.Printf(\u0026#34;db.NamedQuery failed, err:%v\\n\u0026#34;, err) return } defer rows.Close() for rows.Next(){ var u user err := rows.StructScan(\u0026amp;u) if err != nil { fmt.Printf(\u0026#34;scan failed, err:%v\\n\u0026#34;, err) continue } fmt.Printf(\u0026#34;user:%#v\\n\u0026#34;, u) } u := user{ Name: \u0026#34;笑傩\u0026#34;, } // 使用结构体命名查询，根据结构体字段的 db tag进行映射 rows, err = db.NamedQuery(sqlStr, u) if err != nil { fmt.Printf(\u0026#34;db.NamedQuery failed, err:%v\\n\u0026#34;, err) return } defer rows.Close() for rows.Next(){ var u user err := rows.StructScan(\u0026amp;u) if err != nil { fmt.Printf(\u0026#34;scan failed, err:%v\\n\u0026#34;, err) continue } fmt.Printf(\u0026#34;user:%#v\\n\u0026#34;, u) } } 事务操作 对于事务操作，可以使用 sqlx中提供的 db.Beginx()和 tx.Exec()方法。示例代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 func transactionDemo2()(err error) { tx, err := db.Beginx() // 开启事务 if err != nil { fmt.Printf(\u0026#34;begin trans failed, err:%v\\n\u0026#34;, err) return err } defer func() { if p := recover(); p != nil { tx.Rollback() panic(p) // re-throw panic after Rollback } else if err != nil { fmt.Println(\u0026#34;rollback\u0026#34;) tx.Rollback() // err is non-nil; don\u0026#39;t change it } else { err = tx.Commit() // err is nil; if Commit returns error update err fmt.Println(\u0026#34;commit\u0026#34;) } }() sqlStr1 := \u0026#34;Update user set age=20 where id=?\u0026#34; rs, err := tx.Exec(sqlStr1, 1) if err!= nil{ return err } n, err := rs.RowsAffected() if err != nil { return err } if n != 1 { return errors.New(\u0026#34;exec sqlStr1 failed\u0026#34;) } sqlStr2 := \u0026#34;Update user set age=50 where i=?\u0026#34; rs, err = tx.Exec(sqlStr2, 5) if err!=nil{ return err } n, err = rs.RowsAffected() if err != nil { return err } if n != 1 { return errors.New(\u0026#34;exec sqlStr1 failed\u0026#34;) } return err } sqlx.In sqlx.In是 sqlx提供的一个非常方便的函数。\nsqlx.In的批量插入示例 表结构 创建一个 user表，表结构如下：\n1 2 3 4 5 6 CREATE TABLE `user` ( `id` BIGINT(20) NOT NULL AUTO_INCREMENT, `name` VARCHAR(20) DEFAULT \u0026#39;\u0026#39;, `age` INT(11) DEFAULT \u0026#39;0\u0026#39;, PRIMARY KEY(`id`) )ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8mb4; 结构体 定义一个 user结构体，字段通过tag与数据库中user表的列一致。\n1 2 3 4 type User struct { Name string `db:\u0026#34;name\u0026#34;` Age int `db:\u0026#34;age\u0026#34;` } bindvars（绑定变量） 查询占位符 ?在内部称为bindvars（查询占位符）,它非常重要。你应该始终使用它们向数据库发送值，因为它们可以防止SQL注入攻击。database/sql不尝试对查询文本进行任何验证；它与编码的参数一起按原样发送到服务器。除非驱动程序实现一个特殊的接口，否则在执行之前，查询是在服务器上准备的。因此 bindvars是特定于数据库的:\nMySQL中使用 ? PostgreSQL使用枚举的 $1、$2等bindvar语法 SQLite中 ?和 $1的语法都支持 Oracle中使用 :name的语法 bindvars的一个常见误解是，它们用来在sql语句中插入值。它们其实仅用于参数化，不允许更改SQL语句的结构。例如，使用 bindvars尝试参数化列或表名将不起作用：\n1 2 3 4 5 // ？不能用来插入表名（做SQL语句中表名的占位符） db.Query(\u0026#34;SELECT * FROM ?\u0026#34;, \u0026#34;mytable\u0026#34;) // ？也不能用来插入列名（做SQL语句中列名的占位符） db.Query(\u0026#34;SELECT ?, ? FROM people\u0026#34;, \u0026#34;name\u0026#34;, \u0026#34;location\u0026#34;) 手动拼接语句实现批量插入 就是有多少个User就拼接多少个 (?, ?)。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // BatchInsertUsers 自行构造批量插入的语句 func BatchInsertUsers(users []*User) error { // 存放 (?, ?) 的slice valueStrings := make([]string, 0, len(users)) // 存放values的slice valueArgs := make([]interface{}, 0, len(users) * 2) // 遍历users准备相关数据 for _, u := range users { // 此处占位符要与插入值的个数对应 valueStrings = append(valueStrings, \u0026#34;(?, ?)\u0026#34;) valueArgs = append(valueArgs, u.Name) valueArgs = append(valueArgs, u.Age) } // 自行拼接要执行的具体语句 stmt := fmt.Sprintf(\u0026#34;INSERT INTO user (name, age) VALUES %s\u0026#34;, strings.Join(valueStrings, \u0026#34;,\u0026#34;)) _, err := DB.Exec(stmt, valueArgs...) return err } 使用sqlx.In实现批量插入 前提是需要结构体实现 driver.Valuer接口：\n1 2 3 func (u User) Value() (driver.Value, error) { return []interface{}{u.Name, u.Age}, nil } 使用 sqlx.In实现批量插入代码如下：\n1 2 3 4 5 6 7 8 9 10 11 // BatchInsertUsers2 使用sqlx.In帮我们拼接语句和参数, 注意传入的参数是[]interface{} func BatchInsertUsers2(users []interface{}) error { query, args, _ := sqlx.In( \u0026#34;INSERT INTO user (name, age) VALUES (?), (?), (?)\u0026#34;, users..., // 如果arg实现了 driver.Valuer, sqlx.In 会通过调用 Value()来展开它 ) fmt.Println(query) // 查看生成的querystring fmt.Println(args) // 查看生成的args _, err := DB.Exec(query, args...) return err } 使用NamedExec实现批量插入 在项目目录下执行以下命令下载并使用 master分支代码：\n1 go get github.com/jmoiron/sqlx@master 使用 NamedExec实现批量插入的代码如下：\n1 2 3 4 5 // BatchInsertUsers3 使用NamedExec实现批量插入 func BatchInsertUsers3(users []*User) error { _, err := DB.NamedExec(\u0026#34;INSERT INTO user (name, age) VALUES (:name, :age)\u0026#34;, users) return err } 把上面三种方法综合起来试一下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 func main() { err := initDB() if err != nil { panic(err) } defer DB.Close() u1 := User{Name: \u0026#34;笑傩\u0026#34;, Age: 18} u2 := User{Name: \u0026#34;xn\u0026#34;, Age: 28} u3 := User{Name: \u0026#34;笑笑\u0026#34;, Age: 38} // 方法1 users := []*User{\u0026amp;u1, \u0026amp;u2, \u0026amp;u3} err = BatchInsertUsers(users) if err != nil { fmt.Printf(\u0026#34;BatchInsertUsers failed, err:%v\\n\u0026#34;, err) } // 方法2 users2 := []interface{}{u1, u2, u3} err = BatchInsertUsers2(users2) if err != nil { fmt.Printf(\u0026#34;BatchInsertUsers2 failed, err:%v\\n\u0026#34;, err) } // 方法3 users3 := []*User{\u0026amp;u1, \u0026amp;u2, \u0026amp;u3} err = BatchInsertUsers3(users3) if err != nil { fmt.Printf(\u0026#34;BatchInsertUsers3 failed, err:%v\\n\u0026#34;, err) } } sqlx.In的查询示例 关于 sqlx.In这里再补充一个用法，在 sqlx查询语句中实现In查询和FIND_IN_SET函数。即实现 SELECT * FROM user WHERE id in (3, 2, 1);和 SELECT * FROM user WHERE id in (3, 2, 1) ORDER BY FIND_IN_SET(id, '3,2,1');。\nin查询 查询id在给定id集合中的数据。\n1 2 3 4 5 6 7 8 9 10 11 12 13 // QueryByIDs 根据给定ID查询 func QueryByIDs(ids []int)(users []User, err error){ // 动态填充id query, args, err := sqlx.In(\u0026#34;SELECT name, age FROM user WHERE id IN (?)\u0026#34;, ids) if err != nil { return } // sqlx.In 返回带 `?` bindvar的查询语句, 我们使用Rebind()重新绑定它 query = DB.Rebind(query) err = DB.Select(\u0026amp;users, query, args...) return } in查询和FIND_IN_SET函数 查询id在给定id集合的数据并维持给定id集合的顺序。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 // QueryAndOrderByIDs 按照指定id查询并维护顺序 func QueryAndOrderByIDs(ids []int)(users []User, err error){ // 动态填充id strIDs := make([]string, 0, len(ids)) for _, id := range ids { strIDs = append(strIDs, fmt.Sprintf(\u0026#34;%d\u0026#34;, id)) } query, args, err := sqlx.In(\u0026#34;SELECT name, age FROM user WHERE id IN (?) ORDER BY FIND_IN_SET(id, ?)\u0026#34;, ids, strings.Join(strIDs, \u0026#34;,\u0026#34;)) if err != nil { return } // sqlx.In 返回带 `?` bindvar的查询语句, 我们使用Rebind()重新绑定它 query = DB.Rebind(query) err = DB.Select(\u0026amp;users, query, args...) return } 也可以先使用 IN查询，然后通过代码按给定的ids对查询结果进行排序。\n","date":"2020-11-01T00:00:00Z","permalink":"https://xiaonuoz.github.io/p/%E6%9B%BF%E4%BB%A3databasesql%E7%9A%84sqlx%E7%AC%AC%E4%B8%89%E6%96%B9%E5%BA%93/","title":"替代databasesql的sqlx第三方库"},{"content":"raft是是consoul和etcd的核心算法\nRaft介绍 Raft提供了一种在计算系统集群中分布状态机的通用方法，确保集群中的每个节点都同意一系列相同的状态转换 它有许多开源参考实现，具有Go，C ++，Java和Scala中的完整规范实现 一个Raft集群包含若干个服务器节点，通常是5个（单数），这允许整个系统容忍2个节点的失效，每个节点处于以下三种状态之一 follower（跟随者） ：所有节点都以follower 的状态开始。如果没收到leader发来的消息（leader不存在或出现问题）则会变成candidate状态 candidate（候选人）：会向其他节点“拉选票”，如果得到大部分的票则成为leader，这个过程就叫做Leader选举(Leader Election)，当有candidate成为leader后，其他candidate变回follower leader（领导者）：所有对系统的修改都会先经过leader Raft一致性算法 Raft通过选出一个leader来简化日志副本的管理，例如，日志项(log entry)只允许从leader流向follower 基于leader的方法，Raft算法可以分解成三个子问题 Leader election (领导选举)：原来的leader挂掉后，必须选出一个新的leader 如果一个跟随者在一段时间里没有接收到任何消息，也就是选举超时，然后他就会认为系统中没有可用的领导者然后开始进行选举以选出新的领导者。从跟随者到候选人时有一个超时时间，通常在150ms-300ms之间，当在这个时间段没有收到来自leader的消息时（leader挂掉了）就会转换成candidate，不设置固定值是避免同时有多个候选人出现，他们会先投自己一票，然后再去拉选票，同时成为就都只投了自己，然后又重新超时拉选票，导致死循环，系统卡死。 当第一个candidate出现后，他会先给自己投一票，然后进行拉票，如果向其他candidate拉票，他会进行判断两者的任期，只有任期小才会拉取成功，否则会拒绝 如果同时有两个追随者成为候选者，它们会同时拉取选票，如果有4个节点，则两个候选者都是两票，此时在两个候选者之间又会有一个随机时间，最快的先成为leader Raft 使用一种心跳机制来触发领导人选举，当服务器程序启动时，节点都是 follower(跟随者) 身份 候选人的状态维持直到发生以下任何一个条件发生的时候： 他自己赢得了这次的选举 其他的服务器成为领导者 一段时间之后没有任何一个获胜的人 Log replication (日志复制)：leader从客户端接收日志，并复制到整个集群中 当客户端发送一条信息给leader，leader收到后会存入节点的log中，此时leader是未提交的状态，它会先将信息复制给其他follower进行同步，follower存入自己的节点日志后（未提交状态）给leader一个响应，leader接受到响应进行数据提交（此时可以设置主节点提交成功后则给客户端进行响应），提交后leader再发送一个响应给follower，follower进行提交，此时leader和follower都有相同的数据了，同步成功 假如网络波动，发生网络分区（脑裂），5个节点分成了三个子节点为一组，一个leader B一个子节点一组，两者不能相互通信，此时三个追随者会重新选举产生新的leader A，当网络恢复时，B会变为leader A的小弟（因为A只有一个小弟，B有两个），此时A是有4个追随者的leader，然后继续发送消息进行同步 Safety (安全性)：如果有任意的server将日志项回放到状态机中了，那么其他的server只会回放相同的日志项 ","date":"2020-10-28T00:00:00Z","permalink":"https://xiaonuoz.github.io/p/raft%E5%8E%9F%E7%90%86/","title":"Raft原理"},{"content":"Socket是BSD UNIX的进程通信机制，通常也称作”套接字”，用于描述IP地址和端口，是一个通信链的句柄。Socket可以理解为TCP/IP网络的API，它定义了许多函数或例程，程序员可以用它们来开发TCP/IP网络上的应用程序。电脑上运行的应用程序通常通过”套接字”向网络发出请求或者应答网络请求。\n五层协议只是OSI七层和TCP/IP四层的综合，实际应用还是TCP/IP的四层结构。应用层、传输层、网络层、链路层。\nSocket是应用层与TCP/IP协议族通信的中间软件抽象层。在设计模式中，Socket其实就是一个门面模式，它把复杂的TCP/IP协议族隐藏在 Socket后面，对用户来说只需要调用Socket规定的相关函数，让 Socket去组织符合指定的协议数据然后进行通信。\nc/s架构 服务器部分： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 //设置监听，此处及下面都会返回err，为了缩短代码量丢弃了err，工作中不要丢弃 listener, _ := net.Listen(\u0026#34;tcp\u0026#34;, \u0026#34;127.0.0.1:8888\u0026#34;) //阻塞等待用户数据 conn, _ := listener.Accept() //接收用户请求 buf := make([]byte, 1024) //用户数据最后返回到了buf切片中，n表示从用户数据那读取的字节数，最大值为切片的长度1024 n, _ := conn.Read(buf) fmt.Println(string(buf[:n])) //最后处理完数据记得关闭连接 defer func() { listener.Close() conn.Close() }() 服务器端先定义一个监听，表示将这个服务器以什么协议放置于什么位置，然后listener.Accept()让服务器阻塞等待用户向服务器端发送数据，用户发送数据后会存入conn中，通过conn.Read()来获取用户输入的数据并放到buf切片中，通过string(buf[:n])强转用户的字节数据为字符串，n表示数据量大于切片则返回切片最大值数据量，小于切片则返回全部数据，n返回的是Read所读取的总字节量，其值不会超过buf定义的1024字节\n客户端部分： 1 2 3 4 5 //主动连接服务器 conn, _ := net.Dial(\u0026#34;tcp\u0026#34;, \u0026#34;127.0.0.1:8888\u0026#34;) //发送数据 conn.Write([]byte(\u0026#34;are you ok?\u0026#34;)) defer conn.Close() 客户端部分只需要连接服务器并且发送数据，连接服务器需要指定服务器的ip端口和协议，发送的数据是字节切片类型\n如何多个客户端同时连接同一个服务器（重要） 服务器端：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 //conn的类型为net包里的Conn接口 func HandleConn(conn net.Conn) { //每个用户使用完毕后关闭协程 defer conn.Close() //获取客户端的网络信息,并以ip 端口的形式输出 addr := conn.RemoteAddr().String() fmt.Println(addr, \u0026#34;---连接成功\u0026#34;) buf := make([]byte, 2048) //用for循环套住Read()，用户发送一次数据，接收一次并赋值给buf， //运行下面的打印和回传，再次循环并阻塞在Read()处等待用户再次发送请求 //但是这样会让子进程一直存在，除非设置了err不为空时退出实现强制退出 //因此加一个用户输入exit退出的逻辑 for{ n, _ := conn.Read(buf) //打印用户发送过来的内容 fmt.Printf(\u0026#34;%s输入了: %s\\n\u0026#34;, addr, string(buf[:n])) //把用户信息转为大写发回给用户(先将小写的字节切片转为string，然后变成大写再强转为字节切片) //n-是因为在windows中输入的语句有一个\\r\\n换行符，需要-2去除它 //各个平台都不一样，因此可以在前面通过len(string(buf[:n]))来判断到底多了几个字符 if string(buf[:n-2]) == \u0026#34;exit\u0026#34; { fmt.Println(addr, \u0026#34; exit\u0026#34;) return } conn.Write([]byte(strings.ToUpper(string(buf[:n])))) } } func main() { //设置监听，此处及下面都会返回err，为了缩短代码量丢弃了err，工作中不要丢弃 listener, _ := net.Listen(\u0026#34;tcp\u0026#34;, \u0026#34;127.0.0.1:8888\u0026#34;) //利用for循环实现多个客户端连接同一个服务器 for { //循环阻塞等待用户请求，一个用户请求然后往下走，然后循环继续等下一个用户 conn, _ := listener.Accept() //开子协程处理多个用户请求，如果没有用户进入就会阻塞到第一步直到第一个用户请求， //然后往下开一个新协程给此用户，继续for循环等待下一个用户请求 go HandleConn(conn) } defer listener.Close() } 客户端：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 func main() { //主动连接服务端 conn, _ := net.Dial(\u0026#34;tcp\u0026#34;, \u0026#34;127.0.0.1:8888\u0026#34;) defer conn.Close() //从键盘获取输入并发往服务器端 go func() { str := make([]byte, 2048) //用for循环可以实现当os.Stdin.Read()中没有数据时（即没有进行输入）， //阻塞在这一步，直到用户输入内容才继续向下进行 for { //os.Stdin.Read可以提示键盘输入并且将输入的内容转换为字节切片，并赋值到str中，返回切片的长度 n, _ := os.Stdin.Read(str) //发送给服务器端 conn.Write(str[:n]) } }() //从服务器端获取数据 buf := make([]byte, 2048) for { //for循环实现当服务器未往客户端发送数据时，conn.Read(buf)为空，阻塞在这一步， //有数据循环一遍，然后等待下次服务器的数据 //当服务器端输入exit后，服务器端所对应的子协程结束，conn.Read()返回err， //通过return结束主协程，子协程同时结束，退出程序 n, err := conn.Read(buf) if err != nil { fmt.Println(err) return } fmt.Println(string(buf[:n])) } } 远程发送文件： 服务器端：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 func RecvFile(path string, conn net.Conn) { //创建文件 f, _ := os.Create(path) //循环接收文件的内容 buf := make([]byte, 1024*4) for { n, err := conn.Read(buf) if err != nil { if err == io.EOF { fmt.Println(\u0026#34;文件接收完毕\u0026#34;) } else { fmt.Println(err) } return } //往文件写入内容 f.Write(buf[:n]) } } func main() { //建立监听 listener, _ := net.Listen(\u0026#34;tcp\u0026#34;, \u0026#34;127.0.0.1:8888\u0026#34;) //阻塞等待用户请求 conn, _ := listener.Accept() //接收用户文件名 buf := make([]byte, 1024) n, _ := conn.Read(buf) path := string(buf[:n]) //返回消息 conn.Write([]byte(\u0026#34;开始发送\u0026#34;)) //接收文件内容 RecvFile(path, conn) defer listener.Close() defer conn.Close() } 客户端：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 func SendFile(path string, conn net.Conn) { //打开文件，读取文件 f, _ := os.Open(path) buf := make([]byte, 1024*4) for { n, err := f.Read(buf) if err != nil { if err == io.EOF { fmt.Println(\u0026#34;文件传输完成\u0026#34;) } else { fmt.Println(err) } return } //发送到服务器 conn.Write(buf[:n]) } defer f.Close() defer conn.Close() } func main() { fmt.Println(\u0026#34;请输入文件名：\u0026#34;) var path string fmt.Scan(\u0026amp;path) //os.Stat()返回FileInfo类型变量，可以获取文件信息,info.Name()获取文件名,没有此文件则会报错 info, err := os.Stat(path) if err != nil { fmt.Println(\u0026#34;没有这个文件\u0026#34;, err) return } //连接服务器,工作中err别丢空 conn, _ := net.Dial(\u0026#34;tcp\u0026#34;, \u0026#34;127.0.0.1:8888\u0026#34;) //给服务器先发送文件名,err和n都可以丢空 conn.Write([]byte(info.Name())) //服务器接收到文件名，向客户端发送消息,客户端进行判断开始进行发送 buf := make([]byte, 1024) n, _ := conn.Read(buf) if string(buf[:n]) == \u0026#34;开始发送\u0026#34; { SendFile(path, conn) } } 并发聊天服务器 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 type Clinet struct { C chan string //管道string类型，暂存用户发送的数据 Name string //用户名 Addr string //网络地址 } var onlineMap = make(map[string]Clinet) var message = make(chan string) func WriteMsgToClient(cli Clinet, conn net.Conn) { //这个是为了实现cli.C中有数据时向各自客户端发送数据，没有数据时阻塞在这一步 //且任意用户都会在登录时都会通过Manager()方法向每个用户的cli.C发送登录信息， //只要cli.C一有信息，这个子协程就会检测到cli.C不再阻塞，就能向客户端写入新的他人登录信息 //只要发送处的管道没有关闭，cli.Close()，这个for就不会检测到false，会一直堵塞在这里 for msg := range cli.C { conn.Write([]byte(msg + \u0026#34;\\n\u0026#34;)) } } //将用户存进在线用户变量onlineMap中 func HandleConn(conn net.Conn) { defer conn.Close() //获取网络地址 cliAddr := conn.RemoteAddr().String() //创建一个结构体,添加到map中 cli := Clinet{make(chan string), cliAddr, cliAddr} onlineMap[cliAddr] = cli //新开一个协程，专门给当前用户发送消息 go WriteMsgToClient(cli, conn) //广播某个人在线 message \u0026lt;- \u0026#34;[\u0026#34; + cli.Name + \u0026#34;]---login\u0026#34; //退出进行广播并且关闭子协程 isQuit := make(chan bool) hasData := make(chan bool) //新开一个协程，接收用户发送过来的请求 go func() { buf := make([]byte, 2048) //for循环可以避免输入一次就不再进行接收信息的问题 for { n, _ := conn.Read(buf) if n == 0 { //对方断开或者出问题 isQuit \u0026lt;- true return } msg := string(buf[:n-2]) //过滤window末尾的/r/n符号 //查询所有用户 if len(msg) == 3 \u0026amp;\u0026amp; msg == \u0026#34;who\u0026#34; { //避免whoami和who匹配 //遍历map，给当前用户发送所有成员 conn.Write([]byte(\u0026#34;user list:\\n\u0026#34;)) for _, tmp := range onlineMap { msg = tmp.Name + \u0026#34;-----is online\\n\u0026#34; conn.Write([]byte(msg)) } //给用户重命名，输入rename|mike } else if len(msg) \u0026gt;= 8 \u0026amp;\u0026amp; msg[:6] == \u0026#34;rename\u0026#34; { name := strings.Split(msg, \u0026#34;|\u0026#34;)[1] //将msg以|分割 cli.Name = name conn.Write([]byte(\u0026#34;u name is rename\u0026#34;)) } else { //message复用来给所有用户广播它发送的消息，包括自己也看见 message \u0026lt;- cli.Name + \u0026#34;:\u0026#34; + msg } hasData \u0026lt;- true } }() //用for循环让此子协程不会结束，避免发送消息后子协程结束，这个用户就通信结束了 //目的是让用户可以接收到后面登录和发送的信息，所以这个子协程就必须一直存在，除非用户退出 for { //通过select检测管道isQuit的流动 select { case \u0026lt;-isQuit: //删除用户并且广播谁下线了 delete(onlineMap, cliAddr) message \u0026lt;- cli.Name + \u0026#34;--is login out\u0026#34; return case \u0026lt;-hasData: //有数据不作处理 case \u0026lt;-time.After(60 * time.Second): //60s后超时执行此case,超时强制退出 delete(onlineMap, cliAddr) message \u0026lt;- cli.Name + \u0026#34;--is time out leave out\u0026#34; return } } } //新开一个协程，转发消息，只要消息来了就遍历map，给map每个成员都发送此消息 func Manager() { for { //mes为string类型的变量，自动推导类型 msg := \u0026lt;-message //遍历map，给map每个成员都发送此消息 for _, cli := range onlineMap { cli.C \u0026lt;- msg } } } func main() { //监听 listener, _ := net.Listen(\u0026#34;tcp\u0026#34;, \u0026#34;127.0.0.1:8888\u0026#34;) defer listener.Close() go Manager() //循环 for { //循环阻塞，形成多个客户端共用一个服务器 conn, _ := listener.Accept() //处理用户连接 go HandleConn(conn) } } 先是主函数启动子协程HandleConn(conn)，它作用于往map中写入在线成员，并将消息发给管道message,再通过Manager()将message管道发给msg字符串，遍历map，往每个map中的管道写入信息,紧接着通过WriteMsgToClient()中的range向客户端写入消息，且for遍历管道时，没有cli.Close()的存在，它会一直堵塞在此处等待新的信息写进cli.C\nTCP黏包 1 2 3 4 5 6 7 8 9 10 11 12 func main() { conn, err := net.Dial(\u0026#34;tcp\u0026#34;, \u0026#34;127.0.0.1:30000\u0026#34;) if err != nil { fmt.Println(\u0026#34;dial failed, err\u0026#34;, err) return } defer conn.Close() for i := 0; i \u0026lt; 20; i++ { msg := `Hello, Hello. How are you?` conn.Write([]byte(msg)) } } 将msg输出20次到服务端，可以看到服务端输出结果如下：\n1 2 3 4 5 收到client发来的数据： Hello, Hello. How are you?Hello, Hello. How are you?Hello, Hello. How are you?Hello, Hello. How are you?Hello, Hello. How are you? 收到client发来的数据： Hello, Hello. How are you?Hello, Hello. How are you?Hello, Hello. How are you?Hello, Hello. How are you?Hello, Hello. How are you?Hello, Hello. How are you?Hello, Hello. How are you?Hello, Hello. How are you? 收到client发来的数据： Hello, Hello. How are you?Hello, Hello. How are you? 收到client发来的数据： Hello, Hello. How are you?Hello, Hello. How are you?Hello, Hello. How are you? 收到client发来的数据： Hello, Hello. How are you?Hello, Hello. How are you? 客户端分20次发送的数据，在服务端并没有成功的输出20次，而是多条数据“粘”到了一起。\n为什么会出现粘包 主要原因就是tcp数据传递模式是流模式，在保持长连接的时候可以进行多次的收和发。\n“粘包”可发生在发送端也可发生在接收端：\n**由Nagle算法造成的发送端的粘包：**Nagle算法是一种改善网络传输效率的算法。简单来说就是当我们提交一段数据给TCP发送时，TCP并不立刻发送此段数据，而是等待一小段时间看看在等待期间是否还有要发送的数据，若有则会一次把这两段数据发送出去。 **接收端接收不及时造成的接收端粘包：**TCP会把接收到的数据存在自己的缓冲区中，然后通知应用层取数据。当应用层由于某些原因不能及时的把TCP的数据取出来，就会造成TCP缓冲区中存放了几段数据。 解决办法 出现”粘包”的关键在于接收方不确定将要传输的数据包的大小，因此我们可以对数据包进行封包和拆包的操作。\n封包：封包就是给一段数据加上包头，这样一来数据包就分为包头和包体两部分内容了(过滤非法包时封包会加入”包尾”内容)。包头部分的长度是固定的，并且它存储了包体的长度，根据包头长度固定以及包头中含有包体长度的变量就能正确的拆分出一个完整的数据包。\n我们可以自己定义一个协议，比如数据包的前4个字节为包头，里面存储的是发送的数据的长度。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 package proto import ( \u0026#34;bufio\u0026#34; \u0026#34;bytes\u0026#34; \u0026#34;encoding/binary\u0026#34; ) // Encode 将消息编码 func Encode(message string) ([]byte, error) { // 读取消息的长度，转换成int32类型（占4个字节） var length = int32(len(message)) var pkg = new(bytes.Buffer) // 写入消息头 //LittleEndian：小端，可以百度搜大端小端进行了解，它只是写入内存的顺序不一样 //只要编码和解码都用小端或大端就没问题 //这段代码的意思就是往pkg字节缓存中写入一个数据，是int32的message长度，int32是32位，8位为一个字节，所以这里就占了4个字节 err := binary.Write(pkg, binary.LittleEndian, length) if err != nil { return nil, err } // 写入消息实体 err = binary.Write(pkg, binary.LittleEndian, []byte(message)) if err != nil { return nil, err } return pkg.Bytes(), nil } // Decode 解码消息 func Decode(reader *bufio.Reader) (string, error) { // 读取消息的长度，前4个字节就是一条消息的长度 lengthByte, _ := reader.Peek(4) // 读取前4个字节的数据 lengthBuff := bytes.NewBuffer(lengthByte) var length int32 //将前4个字节放到length中 err := binary.Read(lengthBuff, binary.LittleEndian, \u0026amp;length) if err != nil { return \u0026#34;\u0026#34;, err } // Buffered返回缓冲中现有的可读取的字节数。现在发送过来的数据长度是消息的长度信息length加上存放长度信息的包头4个字节 if int32(reader.Buffered()) \u0026lt; length+4 { return \u0026#34;\u0026#34;, err } // 读取真正的消息数据 pack := make([]byte, int(4+length)) //这里是测试是否能从pack中读出数据，并将读的数据放入pack，读的出就从第5个字节读到尾（下标是从0开始的，因此下标4代表第五个字节） _, err = reader.Read(pack) if err != nil { return \u0026#34;\u0026#34;, err } return string(pack[4:]), nil } 接下来在服务端和客户端分别使用上面定义的 proto包的 Decode和 Encode函数处理数据。\n服务端代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 // socket_stick/server2/main.go func process(conn net.Conn) { defer conn.Close() reader := bufio.NewReader(conn) for { msg, err := proto.Decode(reader) if err == io.EOF { return } if err != nil { fmt.Println(\u0026#34;decode msg failed, err:\u0026#34;, err) return } fmt.Println(\u0026#34;收到client发来的数据：\u0026#34;, msg) } } func main() { listen, err := net.Listen(\u0026#34;tcp\u0026#34;, \u0026#34;127.0.0.1:30000\u0026#34;) if err != nil { fmt.Println(\u0026#34;listen failed, err:\u0026#34;, err) return } defer listen.Close() for { conn, err := listen.Accept() if err != nil { fmt.Println(\u0026#34;accept failed, err:\u0026#34;, err) continue } go process(conn) } } 客户端代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // socket_stick/client2/main.go func main() { conn, err := net.Dial(\u0026#34;tcp\u0026#34;, \u0026#34;127.0.0.1:30000\u0026#34;) if err != nil { fmt.Println(\u0026#34;dial failed, err\u0026#34;, err) return } defer conn.Close() for i := 0; i \u0026lt; 20; i++ { msg := `Hello, Hello. How are you?` data, err := proto.Encode(msg) if err != nil { fmt.Println(\u0026#34;encode msg failed, err:\u0026#34;, err) return } conn.Write(data) } } b/s架构： 但是如果工作中每次都需要向服务器发送一长串的请求包过于繁琐，所以可以使用net/http包来简化\nhttp服务器： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 //w为给客户端回复的数据 //req，读取客户端的数据 func HandConn(w http.ResponseWriter, req *http.Request) { //给客户端浏览器发送数据 w.Write([]byte(\u0026#34;hello go\u0026#34;)) //获取客户端的请求头部参数等 //在https://studygolang.com/pkgdoc中的net/http中搜type Request可以获取req的所有参数 fmt.Println(req.URL.Path) } func main() { //注册处理函数，用户连接进来自动调用指定的处理函数(即如果域名后面接了/hello则调用后面那个函数) //源代码中第二个参数为handler func(ResponseWriter, *Request) //即在定义函数时已经定义了这是一个func，且默认已经传了两个参数进去，所以不需要加()来调用， //自己写HandConn函数时也不再需要想办法获取ResponseWriter和*http.Request的值了 http.HandleFunc(\u0026#34;/hello\u0026#34;, HandConn) //该方法用于在指定的网络地址进行监听，然后调用服务端处理程序来处理传入的连接请求 //该方法有两个参数：第一个为监听地址；第二个参数表示服务器端处理程序，通常为空 //第二个参数为空意味着服务端调用http.DefaultServerMux进行处理 http.ListenAndServe(\u0026#34;127.0.0.1:8000\u0026#34;, nil) } http客户端： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 //获取从百度回传回来的请求包，http是必须要加的 resp, _ := http.Get(\u0026#34;http://www.baidu.com\u0026#34;) //body是从服务器端读取资源(类似于conn.Read())，最后是需要进行关闭的 defer resp.Body.Close() fmt.Println(\u0026#34;Status =\u0026#34;, resp.Status) fmt.Println(\u0026#34;StatusCode =\u0026#34;, resp.StatusCode) fmt.Println(\u0026#34;Header =\u0026#34;, resp.Header) //获取baidu.com中的数据Read()，然后赋值给buf，最后追加到tmp中 buf := make([]byte, 1024*4) var tmp string for { n, err := resp.Body.Read(buf) if n == 0 { fmt.Println(\u0026#34;read err =\u0026#34;, err) break } tmp += string(buf[:n]) } fmt.Println(tmp) 参考文档 https://www.liwenzhou.com/posts/Go/go_http/\n用go写爬虫爬百度贴吧： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 func SpiderPage(i int, page chan int) { //寻找网址规律，每页pn加50,用for循环获取每个网址 url := \u0026#34;http://tieba.baidu.com/f?kw=%E6%8A%97%E5%8E%8B%E8%83%8C%E9%94%85\u0026amp;ie=utf-8\u0026amp;pn=\u0026#34; + strconv.Itoa((i-1)*50) fmt.Printf(\u0026#34;正在爬取第%d页网页%s\\n\u0026#34;, i, url) result, _ := HttpGet(url) //将内容写到文件中 f, _ := os.Create(strconv.Itoa(i) + \u0026#34;.html\u0026#34;) f.Write([]byte(result)) f.Close() page \u0026lt;- i } func HttpGet(url string) (result string, err error) { rep, _ := http.Get(url) defer rep.Body.Close() //爬取 buf := make([]byte, 1024*4) for { n, err := rep.Body.Read(buf) if n == 0 { fmt.Println(err) break } result += string(buf[:n]) } return } func DoWork(start, end int) { fmt.Printf(\u0026#34;正在爬取 %d到%d的页面。。。\u0026#34;, start, end) //建立一个管道，避免主进程结束导致子进程结束 page := make(chan int) //获取地址 for i := start; i \u0026lt;= end; i++ { //建立子协程，让多个爬虫同时进行 go SpiderPage(i, page) } for i := start; i \u0026lt;= end; i++ { //避免协程结束影响子协程 fmt.Printf(\u0026#34;第%d页已经读取完毕\u0026#34;, \u0026lt;-page) } return } func main() { var start, end int fmt.Println(\u0026#34;请输入起始页\u0026#34;) fmt.Scan(\u0026amp;start) fmt.Println(\u0026#34;请输入结束页\u0026#34;) fmt.Scan(\u0026amp;end) DoWork(start, end) } 此方法可以爬取每页内容（包括html内容）存到新建的.html中，如果要爬取需要的内容，可以查看源代码然后通过正则表达式爬出来再存入文件中\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 如爬取每个帖子的某段内容，先在每个主页查看源代码爬取出每个帖子的url， \u0026lt;a rel=\u0026#34;noreferrer\u0026#34; href=\u0026#34;/p/6978750013\u0026#34; title=\u0026#34;老马是真的叼，剪辑的更吊！！！\u0026#34; target=\u0026#34;_blank\u0026#34; class=\u0026#34;j_th_tit \u0026#34; 用正则表达式 (`\u0026lt;a rel=\u0026#34;noreferrer\u0026#34; href=\u0026#34;(?s:(.*?))\u0026#34; title=`) 来爬取出网页链接， regexp的FindAllStringSubmatch会返回一个二维切片，切片中的里切片第一个值是通过表达式过滤出的内容， 第二个值是正则表达式代表的内容，即/p/6978750013，给它拼接上贴吧网址即可访问 通过range迭代外切片然后在里面调用里切片[1]即可 然后在range中用http.Get爬取内容，依然是查看源代码找到对应的内容进行过滤 过来出来的内容可能会有些\\t \u0026lt;br /\u0026gt;之类的，可以用strings.Replace(text,\u0026#34;\\t\u0026#34;,\u0026#34;\u0026#34;,-1)去掉 最后写入文件，如果多标题多内容，可以分开存入到两个切片中，然后后面一口气写进文件中 如果不用子协程来爬取，它就是单协程的程序，它会爬完一个再爬下一个，开了子协程后它可以同时爬取多个，节省了大量时间，但是要注意子协程开启后主协程不能关闭，这样会导致子协程也同时消失，可以使用切片来阻塞主协程，直到爬取完毕\nGo语言实现UDP通信 UDP协议 UDP协议（User Datagram Protocol）中文名称是用户数据报协议，是OSI（Open System Interconnection，开放式系统互联）参考模型中一种无连接的传输层协议，不需要建立连接就能直接进行数据发送和接收，属于不可靠的、没有时序的通信，但是UDP协议的实时性比较好，通常用于视频直播相关领域。\nUDP服务端 使用Go语言的 net包实现的UDP服务端代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 // UDP/server/main.go // UDP server端 func main() { listen, err := net.ListenUDP(\u0026#34;udp\u0026#34;, \u0026amp;net.UDPAddr{ IP: net.IPv4(0, 0, 0, 0), Port: 30000, }) if err != nil { fmt.Println(\u0026#34;listen failed, err:\u0026#34;, err) return } defer listen.Close() for { var data [1024]byte //addr就是发送端的地址类型 n, addr, err := listen.ReadFromUDP(data[:]) // 接收数据 if err != nil { fmt.Println(\u0026#34;read udp failed, err:\u0026#34;, err) continue } fmt.Printf(\u0026#34;data:%v addr:%v count:%v\\n\u0026#34;, string(data[:n]), addr, n) _, err = listen.WriteToUDP(data[:n], addr) // 发送数据 if err != nil { fmt.Println(\u0026#34;write to udp failed, err:\u0026#34;, err) continue } } } UDP客户端 使用Go语言的 net包实现的UDP客户端代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 // UDP 客户端 func main() { socket, err := net.DialUDP(\u0026#34;udp\u0026#34;, nil, \u0026amp;net.UDPAddr{ IP: net.IPv4(0, 0, 0, 0), Port: 30000, }) if err != nil { fmt.Println(\u0026#34;连接服务端失败，err:\u0026#34;, err) return } //close要写在err判断后面，避免真的出错直接结束，导致直接执行defer造成问题 defer socket.Close() sendData := []byte(\u0026#34;Hello server\u0026#34;) _, err = socket.Write(sendData) // 发送数据 if err != nil { fmt.Println(\u0026#34;发送数据失败，err:\u0026#34;, err) return } data := make([]byte, 4096) n, remoteAddr, err := socket.ReadFromUDP(data) // 接收数据 if err != nil { fmt.Println(\u0026#34;接收数据失败，err:\u0026#34;, err) return } fmt.Printf(\u0026#34;recv:%v addr:%v count:%v\\n\u0026#34;, string(data[:n]), remoteAddr, n) } ","date":"2020-10-21T00:00:00Z","permalink":"https://xiaonuoz.github.io/p/socket%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/","title":"Socket网络编程"},{"content":"NSQ介绍 NSQ是Go语言编写的一个开源的实时分布式内存消息队列，其性能十分优异。 NSQ的优势有以下优势：\nNSQ提倡分布式和分散的拓扑，没有单点故障，支持容错和高可用性，并提供可靠的消息交付保证 NSQ支持横向扩展，没有任何集中式代理。 NSQ易于配置和部署，并且内置了管理界面。 NSQ的应用场景 通常来说，消息队列都适用以下场景。\n异步处理 参照下图利用消息队列把业务流程中的非关键流程异步化，从而显著降低业务请求的响应时间。\n应用解耦 通过使用消息队列将不同的业务逻辑解耦，降低系统间的耦合，提高系统的健壮性。后续有其他业务要使用订单数据可直接订阅消息队列，提高系统的灵活性。\n流量削峰 类似秒杀（大秒）等场景下，某一时间可能会产生大量的请求，使用消息队列能够为后端处理请求提供一定的缓冲区，保证后端服务的稳定性。\n安装 官方下载页面根据自己的平台下载并解压即可。\nNSQ组件 nsqd nsqd是一个守护进程，它接收、排队并向客户端发送消息。\n启动 nsqd，指定 -broadcast-address=127.0.0.1来配置广播地址\n1 ./nsqd -broadcast-address=127.0.0.1 如果是在搭配 nsqlookupd使用的模式下需要还指定 nsqlookupd地址:\n1 ./nsqd -broadcast-address=127.0.0.1 -lookupd-tcp-address=127.0.0.1:4160 如果是部署了多个 nsqlookupd节点的集群，那还可以指定多个 -lookupd-tcp-address。\nnsqdq相关配置项如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 -auth-http-address value \u0026lt;addr\u0026gt;:\u0026lt;port\u0026gt; to query auth server (may be given multiple times) -broadcast-address string address that will be registered with lookupd (defaults to the OS hostname) (default \u0026#34;PROSNAKES.local\u0026#34;) -config string path to config file -data-path string path to store disk-backed messages -deflate enable deflate feature negotiation (client compression) (default true) -e2e-processing-latency-percentile value message processing time percentiles (as float (0, 1.0]) to track (can be specified multiple times or comma separated \u0026#39;1.0,0.99,0.95\u0026#39;, default none) -e2e-processing-latency-window-time duration calculate end to end latency quantiles for this duration of time (ie: 60s would only show quantile calculations from the past 60 seconds) (default 10m0s) -http-address string \u0026lt;addr\u0026gt;:\u0026lt;port\u0026gt; to listen on for HTTP clients (default \u0026#34;0.0.0.0:4151\u0026#34;) -http-client-connect-timeout duration timeout for HTTP connect (default 2s) -http-client-request-timeout duration timeout for HTTP request (default 5s) -https-address string \u0026lt;addr\u0026gt;:\u0026lt;port\u0026gt; to listen on for HTTPS clients (default \u0026#34;0.0.0.0:4152\u0026#34;) -log-prefix string log message prefix (default \u0026#34;[nsqd] \u0026#34;) -lookupd-tcp-address value lookupd TCP address (may be given multiple times) -max-body-size int maximum size of a single command body (default 5242880) -max-bytes-per-file int number of bytes per diskqueue file before rolling (default 104857600) -max-deflate-level int max deflate compression level a client can negotiate (\u0026gt; values == \u0026gt; nsqd CPU usage) (default 6) -max-heartbeat-interval duration maximum client configurable duration of time between client heartbeats (default 1m0s) -max-msg-size int maximum size of a single message in bytes (default 1048576) -max-msg-timeout duration maximum duration before a message will timeout (default 15m0s) -max-output-buffer-size int maximum client configurable size (in bytes) for a client output buffer (default 65536) -max-output-buffer-timeout duration maximum client configurable duration of time between flushing to a client (default 1s) -max-rdy-count int maximum RDY count for a client (default 2500) -max-req-timeout duration maximum requeuing timeout for a message (default 1h0m0s) -mem-queue-size int number of messages to keep in memory (per topic/channel) (default 10000) -msg-timeout string duration to wait before auto-requeing a message (default \u0026#34;1m0s\u0026#34;) -node-id int unique part for message IDs, (int) in range [0,1024) (default is hash of hostname) (default 616) -snappy enable snappy feature negotiation (client compression) (default true) -statsd-address string UDP \u0026lt;addr\u0026gt;:\u0026lt;port\u0026gt; of a statsd daemon for pushing stats -statsd-interval string duration between pushing to statsd (default \u0026#34;1m0s\u0026#34;) -statsd-mem-stats toggle sending memory and GC stats to statsd (default true) -statsd-prefix string prefix used for keys sent to statsd (%s for host replacement) (default \u0026#34;nsq.%s\u0026#34;) -sync-every int number of messages per diskqueue fsync (default 2500) -sync-timeout duration duration of time per diskqueue fsync (default 2s) -tcp-address string \u0026lt;addr\u0026gt;:\u0026lt;port\u0026gt; to listen on for TCP clients (default \u0026#34;0.0.0.0:4150\u0026#34;) -tls-cert string path to certificate file -tls-client-auth-policy string client certificate auth policy (\u0026#39;require\u0026#39; or \u0026#39;require-verify\u0026#39;) -tls-key string path to key file -tls-min-version value minimum SSL/TLS version acceptable (\u0026#39;ssl3.0\u0026#39;, \u0026#39;tls1.0\u0026#39;, \u0026#39;tls1.1\u0026#39;, or \u0026#39;tls1.2\u0026#39;) (default 769) -tls-required require TLS for client connections (true, false, tcp-https) -tls-root-ca-file string path to certificate authority file -verbose enable verbose logging -version print version string -worker-id do NOT use this, use --node-id nsqlookupd nsqlookupd是维护所有nsqd状态、提供服务发现的守护进程。它能为消费者查找特定 topic下的nsqd提供了运行时的自动发现服务。 它不维持持久状态，也不需要与任何其他nsqlookupd实例协调以满足查询。因此根据你系统的冗余要求尽可能多地部署 nsqlookupd节点。它们小豪的资源很少，可以与其他服务共存。我们的建议是为每个数据中心运行至少3个集群。\nnsqlookupd相关配置项如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 -broadcast-address string address of this lookupd node, (default to the OS hostname) (default \u0026#34;PROSNAKES.local\u0026#34;) -config string path to config file -http-address string \u0026lt;addr\u0026gt;:\u0026lt;port\u0026gt; to listen on for HTTP clients (default \u0026#34;0.0.0.0:4161\u0026#34;) -inactive-producer-timeout duration duration of time a producer will remain in the active list since its last ping (default 5m0s) -log-prefix string log message prefix (default \u0026#34;[nsqlookupd] \u0026#34;) -tcp-address string \u0026lt;addr\u0026gt;:\u0026lt;port\u0026gt; to listen on for TCP clients (default \u0026#34;0.0.0.0:4160\u0026#34;) -tombstone-lifetime duration duration of time a producer will remain tombstoned if registration remains (default 45s) -verbose enable verbose logging -version print version string nsqadmin 一个实时监控集群状态、执行各种管理任务的Web管理平台。 启动 nsqadmin，指定 nsqlookupd地址:\n1 ./nsqadmin -lookupd-http-address=127.0.0.1:4161 我们可以使用浏览器打开 http://127.0.0.1:4171/访问如下管理界面。\nnsqadmin相关的配置项如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 -allow-config-from-cidr string A CIDR from which to allow HTTP requests to the /config endpoint (default \u0026#34;127.0.0.1/8\u0026#34;) -config string path to config file -graphite-url string graphite HTTP address -http-address string \u0026lt;addr\u0026gt;:\u0026lt;port\u0026gt; to listen on for HTTP clients (default \u0026#34;0.0.0.0:4171\u0026#34;) -http-client-connect-timeout duration timeout for HTTP connect (default 2s) -http-client-request-timeout duration timeout for HTTP request (default 5s) -http-client-tls-cert string path to certificate file for the HTTP client -http-client-tls-insecure-skip-verify configure the HTTP client to skip verification of TLS certificates -http-client-tls-key string path to key file for the HTTP client -http-client-tls-root-ca-file string path to CA file for the HTTP client -log-prefix string log message prefix (default \u0026#34;[nsqadmin] \u0026#34;) -lookupd-http-address value lookupd HTTP address (may be given multiple times) -notification-http-endpoint string HTTP endpoint (fully qualified) to which POST notifications of admin actions will be sent -nsqd-http-address value nsqd HTTP address (may be given multiple times) -proxy-graphite proxy HTTP requests to graphite -statsd-counter-format string The counter stats key formatting applied by the implementation of statsd. If no formatting is desired, set this to an empty string. (default \u0026#34;stats.counters.%s.count\u0026#34;) -statsd-gauge-format string The gauge stats key formatting applied by the implementation of statsd. If no formatting is desired, set this to an empty string. (default \u0026#34;stats.gauges.%s\u0026#34;) -statsd-interval duration time interval nsqd is configured to push to statsd (must match nsqd) (default 1m0s) -statsd-prefix string prefix used for keys sent to statsd (%s for host replacement, must match nsqd) (default \u0026#34;nsq.%s\u0026#34;) -version print version string NSQ架构 NSQ工作模式 Topic和Channel 每个nsqd实例旨在一次处理多个数据流。这些数据流称为 “topics”，一个 topic具有1个或多个 “channels”。每个 channel都会收到 topic所有消息的副本，实际上下游的服务是通过对应的 channel来消费 topic消息。\ntopic和 channel不是预先配置的。topic在首次使用时创建，方法是将其发布到指定 topic，或者订阅指定 topic上的 channel。channel是通过订阅指定的 channel在第一次使用时创建的。\ntopic和 channel都相互独立地缓冲数据，防止缓慢的消费者导致其他 chennel的积压（同样适用于 topic级别）。\nchannel可以并且通常会连接多个客户端。假设所有连接的客户端都处于准备接收消息的状态，则每条消息将被传递到随机客户端。例如：\n总而言之，消息是从 topic -\u0026gt; channel（每个channel接收该topic的所有消息的副本）多播的，但是从 channel -\u0026gt; consumers均匀分布（每个消费者接收该channel的一部分消息）。\nNSQ接收和发送消息流程 NSQ特性 消息默认不持久化，可以配置成持久化模式。nsq采用的方式时内存+硬盘的模式，当内存到达一定程度时就会将数据持久化到硬盘。 如果将 --mem-queue-size设置为0，所有的消息将会存储到磁盘。 服务器重启时也会将当时在内存中的消息持久化。 每条消息至少传递一次。 消息不保证有序。 Go操作NSQ 官方提供了Go语言版的客户端：go-nsq，更多客户端支持请查看CLIENT LIBRARIES。\n安装 1 go get -u github.com/nsqio/go-nsq 生产者 一个简单的生产者示例代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 // nsq_producer/main.go package main import ( \u0026#34;bufio\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;github.com/nsqio/go-nsq\u0026#34; ) // NSQ Producer Demo var producer *nsq.Producer // 初始化生产者 func initProducer(str string) (err error) { config := nsq.NewConfig() producer, err = nsq.NewProducer(str, config) if err != nil { fmt.Printf(\u0026#34;create producer failed, err:%v\\n\u0026#34;, err) return err } return nil } func main() { nsqAddress := \u0026#34;127.0.0.1:4150\u0026#34; err := initProducer(nsqAddress) if err != nil { fmt.Printf(\u0026#34;init producer failed, err:%v\\n\u0026#34;, err) return } reader := bufio.NewReader(os.Stdin) // 从标准输入读取 for { data, err := reader.ReadString(\u0026#39;\\n\u0026#39;) if err != nil { fmt.Printf(\u0026#34;read string from stdin failed, err:%v\\n\u0026#34;, err) continue } data = strings.TrimSpace(data) if strings.ToUpper(data) == \u0026#34;Q\u0026#34; { // 输入Q退出 break } // 向 \u0026#39;topic_demo\u0026#39; publish 数据 err = producer.Publish(\u0026#34;topic_demo\u0026#34;, []byte(data)) if err != nil { fmt.Printf(\u0026#34;publish msg to nsq failed, err:%v\\n\u0026#34;, err) continue } } } 将上面的代码编译执行，然后在终端输入两条数据 123和 456：\n1 2 3 4 $ ./nsq_producer 123 2018/10/22 18:41:20 INF 1 (127.0.0.1:4150) connecting to nsqd 456 使用浏览器打开 http://127.0.0.1:4171/可以查看到类似下面的页面： 在下面这个页面能看到当前的 topic信息：\n点击页面上的 topic_demo就能进入一个展示更多详细信息的页面，在这个页面上我们可以查看和管理 topic，同时能够看到目前在 LWZMBP:4151 (127.0.01:4151)这个 nsqd上有2条message。又因为没有消费者接入所以暂时没有创建 channel。\n在 /nodes这个页面我们能够很方便的查看当前接入 lookupd的 nsqd节点。\n这个 /counter页面显示了处理的消息数量，因为我们没有接入消费者，所以处理的消息数量为0。\n在 /lookup界面支持创建 topic和 channel。\n消费者 一个简单的消费者示例代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 // nsq_consumer/main.go package main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;os/signal\u0026#34; \u0026#34;syscall\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/nsqio/go-nsq\u0026#34; ) // NSQ Consumer Demo // MyHandler 是一个消费者类型 type MyHandler struct { Title string } // HandleMessage 是需要实现的处理消息的方法 func (m *MyHandler) HandleMessage(msg *nsq.Message) (err error) { fmt.Printf(\u0026#34;%s recv from %v, msg:%v\\n\u0026#34;, m.Title, msg.NSQDAddress, string(msg.Body)) return } // 初始化消费者 func initConsumer(topic string, channel string, address string) (err error) { config := nsq.NewConfig() config.LookupdPollInterval = 15 * time.Second c, err := nsq.NewConsumer(topic, channel, config) if err != nil { fmt.Printf(\u0026#34;create consumer failed, err:%v\\n\u0026#34;, err) return } consumer := \u0026amp;MyHandler{ Title: \u0026#34;沙河1号\u0026#34;, } c.AddHandler(consumer) // if err := c.ConnectToNSQD(address); err != nil { // 直接连NSQD if err := c.ConnectToNSQLookupd(address); err != nil { // 通过lookupd查询 return err } return nil } func main() { err := initConsumer(\u0026#34;topic_demo\u0026#34;, \u0026#34;first\u0026#34;, \u0026#34;127.0.0.1:4161\u0026#34;) if err != nil { fmt.Printf(\u0026#34;init consumer failed, err:%v\\n\u0026#34;, err) return } c := make(chan os.Signal) // 定义一个信号的通道 signal.Notify(c, syscall.SIGINT) // 转发键盘中断信号到c \u0026lt;-c // 阻塞 } 将上面的代码保存之后编译执行，就能够获取之前我们publish的两条消息了：\n1 2 3 4 5 $ ./nsq_consumer 2018/10/22 18:49:06 INF 1 [topic_demo/first] querying nsqlookupd http://127.0.0.1:4161/lookup?topic=topic_demo 2018/10/22 18:49:06 INF 1 [topic_demo/first] (127.0.0.1:4150) connecting to nsqd 沙河1号 recv from 127.0.0.1:4150, msg:123 沙河1号 recv from 127.0.0.1:4150, msg:456 同时在nsqadmin的 /counter页面查看到处理的数据数量为2。\n关于 go-nsq的更多内容请阅读go-nsq的官方文档。\n","date":"2020-10-15T00:00:00Z","permalink":"https://xiaonuoz.github.io/p/gonsq/","title":"GONSQ"},{"content":"在开发过程中，通常我们会将信息输出到终端来查看变量和结果是否和预想的一样，但是如果丢到线上后依然是输出到终端的话，它就会占用部分资源，但是我们并没有使用到，并且后期出错查询也很困难，因此写一个在开发过程中输出到终端，在编译后输出到文件的程序很有必要\n1 2 3 4 5 fileObj,_:=os.OpenFile(\u0026#34;./xx.log\u0026#34;,os.O_APPEND|os.O_CREATE|os.O_WRONLY,0644) //设置日志输出位置为fileObj，默认的值是os.Stdout,即终端 log.SetOutput(fileObj) log.Printf(\u0026#34;这是一条错误输出\u0026#34;) time.Sleep(3*time.Second) 需求分析 支持往不同的地方输出日志（终端和文件中） 日志分级别 Debug Trace Info Warning Error Fatal 日志要支持级别开关控制，比如说开发的时候所有级别都输出，但是上线后只有INFO级别往下的才能输出 完整的日志记录要包含时间、行号、文件名、日志级别、日志信息 日志文件要切割 按文件大小切割 我们初始化时有设置一个maxFileSize文件最大值，可以通过在每次写日志时都判断一下当前文件大小来进行切割 按日志切割（直接将文件名设置为当前时间就可以通过日志切割） runtime.Caller() pc,file,line,ok:=runtime.Caller(skip int) 这个函数是运行时gc的获取的部分信息，它返回四个值\npc返回的是执行函数指针，可以通过这个获取函数名 runtime.FuncForPC(pc).Name()等信息 file是执行函数所在文件名目录，使用 path.Base(file)可以获取到绝对路径中的最后一层目录， line是执行的函数所在行号 ok 是否可以获取到信息,返回false时前面三个值都为零值 执行函数指的是调用Caller的那个函数，通过skip可以向上调用更高级别的执行函数信息\nskip是要提升的堆栈帧数，如就在当前函数调用的Caller，它的值就是0，如果要在另一个函数调用它，并且获取的值要是另一个函数的，那它就得提升一个，即为1，如果main再调用，想获取main的信息，就是2，即a函数调用caller就升一层，b调用a再升一层，想获取b的信息skip就为2\n代码 公用代码：logPublicfunc.go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 package mylogger import ( \u0026#34;fmt\u0026#34; \u0026#34;strings\u0026#34; ) //定义一个接口处理两种不同日志 type Logger interface { Debug(format string,a ...interface{}) Info(format string,a ...interface{}) } //定义日志级别为非0的整数，使用type可以在以后打印类型时知道它的作用，而uint则无法理解用于什么地方 type LogLevel uint16 //设置每个级别对应的uint16值，unknow为0，表示输入的格式不正确 const ( UNKNOW LogLevel =iota DEBUG INFO ) //通过传入的字符串来判断级别，返回的是uint常量 func parseLogLevel(s string) (LogLevel,error){ //将传入的字符串全部转为大写 s=strings.ToUpper(s) switch s { case \u0026#34;DEBUG\u0026#34;: return DEBUG,nil case \u0026#34;INFO\u0026#34;: return INFO,nil default: //创建一个err，如果err不为空则是输入有误 err:=fmt.Errorf(\u0026#34;无效的日志级别，请检查\u0026#34;) return UNKNOW,err } } func getLogString(lv LogLevel) string{ switch lv { case DEBUG: return \u0026#34;DEBUG\u0026#34; case INFO: return \u0026#34;INFO\u0026#34; default: return \u0026#34;UNKNOW\u0026#34; } } 日志写到终端中：consolelogger.go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 package mylogger //往终端中写日志 import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;time\u0026#34; ) //自定义一个日志库结构体 type ConsoleLogger struct { Level LogLevel } //初始化一个Logger结构体 func NewConsoleLogger(s string) *ConsoleLogger{ level,err:=parseLogLevel(s) if err!=nil{ //如果err不为空，则level为零值=0,0对应着UNKNOW fmt.Println(err) os.Exit(0) } return \u0026amp;ConsoleLogger{Level: level} } //实现开关级别,通过对象和参数的比较来执行比设置的级别高的日志消息 func (c *ConsoleLogger)enable(loglevel LogLevel)bool{ return loglevel\u0026gt;=c.Level } //可以通过可变参数（参照fmt.Printf函数）实现格式化输出,实现传变量进字符串 //Sprintf返回一个格式化好的字符串 func (c *ConsoleLogger)logFmt(lv LogLevel,format string,a ...interface{}){ //设置日志时间格式 var timeInit =time.Now().Format(\u0026#34;2006-01-02 15:04:05\u0026#34;) if c.enable(lv){ msg:=fmt.Sprintf(format,a...) fmt.Fprintf(os.Stdout,\u0026#34;[%s] [%s] %s\\n\u0026#34;,timeInit,getLogString(lv),msg) } } func (c *ConsoleLogger)Debug(format string,a ...interface{}){ c.logFmt(DEBUG,format,a...) } func (c *ConsoleLogger)Info(format string,a ...interface{}){ c.logFmt(INFO,format,a...) } 日志写到文件中，且以大小分隔：fileLogger.go 可以使用channel让日志和业务代码分开来，写日志不影响正常业务的运行。写日志报错只会导致日志写入失败，但是程序依然在运行\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 package mylogger import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;path\u0026#34; \u0026#34;time\u0026#34; ) //往文件中写日志 type FileLogger struct { Level LogLevel FileName string\t//日志文件保存的文件名 FilePath string\t//日志文件保存的路径 MaxFileSize int64 //每个日志文件的最大值 FileObj *os.File\t//初始化时打开一个文件 ErrFileObj *os.File\t//记录错误日志的文件 logChan chan *logMsg\t//管道，用于存放日志信息 } type logMsg struct { level LogLevel\t//日志级别 msg string\t//日志信息 timestamp string //时间戳 } //初始化一个Logger结构体 func NewFileLogger(s,fileName,filePath string,maxFileSize int64) *FileLogger{ level,err:=parseLogLevel(s) if err!=nil{ //如果err不为空，则level为零值=0,0对应着UNKNOW fmt.Println(err) os.Exit(0) } f1:= \u0026amp;FileLogger{ Level: level, FileName:fileName, FilePath: filePath, MaxFileSize: maxFileSize, logChan: make(chan *logMsg,50000), } err1:=f1.initFile() if err1!=nil{ //日志文件都无法打开，直接panic中断程序运行 panic(err1) } return f1 } //实现开关级别,通过对象和参数的比较来执行比设置的级别高的日志消息 func (f *FileLogger)enable(loglevel LogLevel)bool{ return loglevel\u0026gt;=f.Level } //初始化，打开一个文件向内写日志 func (f *FileLogger)initFile() error{ //拼接文件路径和文件名，然后打开此文件，没有就进行创建 fullFileName:=path.Join(f.FilePath,f.FileName) fileObj,err:=os.OpenFile(fullFileName,os.O_APPEND|os.O_CREATE|os.O_WRONLY,0644) if err!=nil{ fmt.Printf(\u0026#34;open log file failed,err:%v\u0026#34;,err) return err } //专门记录错误的日志 errfileObj,err:=os.OpenFile(fullFileName+\u0026#34;.err\u0026#34;,os.O_APPEND|os.O_CREATE|os.O_WRONLY,0644) if err!=nil{ fmt.Printf(\u0026#34;open log file failed,err:%v\u0026#34;,err) return err } f.FileObj=fileObj f.ErrFileObj=errfileObj //开启一个后台goroutine往文件内写日志 //不能开启多个，因为同一时间不能有多个程序操作同一个文件 go f.writeLogBackground() //return nil可以实现调用此方法时加if判断来判断是否有错误 return nil } //判断当前文件大小，超过指定值就进行切割，将要写入的文件传进去判断 func (f *FileLogger)checkSize(file *os.File) bool{ fileInfo,err:=file.Stat() if err!=nil{ fmt.Printf(\u0026#34;get file info failed,err:%v\\n\u0026#34;,err) return false } //如果当前文件大小大于等于预设时文件的最大值，就应该返回true，进行切割 return fileInfo.Size()\u0026gt;=f.MaxFileSize } func (f *FileLogger)splitFile(file *os.File) (*os.File,error){ //此时则需要进行文件切割,如果返回为false就会直接跳过该判断，继续往原文件写内容 fileInfo,err:=file.Stat() if err!=nil{ fmt.Printf(\u0026#34;open file faile,err:%v\\n\u0026#34;,err) return nil,err } //然后rename原文件,fileInfo.Name()可以获取文件名，可以同时适配普通文件和错误文件 logName:=path.Join(f.FilePath,fileInfo.Name())//拿到当前日志的路径，join的拼接是用/来拼接的 //在重命名时要先关闭当前的日志文件 nowStr:=time.Now().Format(\u0026#34;20060102150405\u0026#34;)//获取当前时间 file.Close() newName:=logName+\u0026#34;.bak\u0026#34;+nowStr os.Rename(logName,newName)//重命名 //打开新的文件并赋值给FileObj fileObj,err:=os.OpenFile(logName,os.O_APPEND|os.O_CREATE|os.O_WRONLY,0644) if err!=nil{ fmt.Printf(\u0026#34;open file faild,err:%v\\n\u0026#34;,err) return f.FileObj,err } return fileObj,nil } //开携程让日志和业务代码区分开 func (f *FileLogger)writeLogBackground(){ //用for循环让此函数一直运行等待从管道中取数据 for{ //普通日志文件分割 if f.checkSize(f.FileObj){ fileObj,err:=f.splitFile(f.FileObj) if err!=nil{ fmt.Printf(\u0026#34;open file faild,err:%v\\n\u0026#34;,err) return } f.FileObj=fileObj } select { //如果无法从logChan中取出数据，就一直阻塞在此处 case logtmp:=\u0026lt;-f.logChan: logInfo:=fmt.Sprintf(\u0026#34;[%s] [%s] %s\\n\u0026#34;,logtmp.timestamp,getLogString(logtmp.level),logtmp.msg) fmt.Fprintf(f.FileObj,logInfo) //错误日志文件分割 if logtmp.level\u0026gt;=INFO{ if f.checkSize(f.ErrFileObj){ errfileObj,err:=f.splitFile(f.ErrFileObj) if err!=nil{ fmt.Printf(\u0026#34;open file faild,err:%v\u0026#34;,err) return } f.ErrFileObj=errfileObj } //如果要记录的日志等级（输入的参数）大于等于INFO级别，则还要在err日志文件中再记录一遍 fmt.Fprintf(f.ErrFileObj,logInfo) } } } } //可以通过可变参数（参照fmt.Printf函数）实现格式化输出,实现传变量进字符串 //Sprintf返回一个格式化好的字符串 func (f *FileLogger)logFmt(lv LogLevel,format string,a ...interface{}){ if f.enable(lv){ //设置日志时间格式 var timeInit =time.Now().Format(\u0026#34;2006-01-02 15:04:05\u0026#34;) msg:=fmt.Sprintf(format,a...) //先把日志发到通道中 logtmp:=\u0026amp;logMsg{level: lv,msg:msg,timestamp: timeInit} //通过select监听管道，避免极端情况下通道阻塞影响业务代码的运行 //写的进去和写不进去都直接跳过，然后继续执行代码 select { case f.logChan\u0026lt;-logtmp: default: //如果写不进去就进行跳过，保证业务代码正常运行，但是此时日志写不进去（极端情况） } } } func (f *FileLogger)Debug(format string,a ...interface{}){ f.logFmt(DEBUG,format,a...) } func (f *FileLogger)Info(format string,a ...interface{}){ f.logFmt(INFO,format,a...) } //在用完后记得关闭文件 func (f *FileLogger)Close(){ f.FileObj.Close() f.ErrFileObj.Close() } main文件：logermain.go 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 package main import ( \u0026#34;mylogger\u0026#34; \u0026#34;time\u0026#34; ) var logger mylogger.Logger func main() { //这里的参数表示会输出大于等于debug等级的日志 //可以在consolelogger.go中设置大于则输出到终端，小于则输出到文件中 //输出到终端 logger = mylogger.NewConsoleLogger(\u0026#34;debug\u0026#34;) for i:=0;i\u0026lt;2;i++{ logger.Debug(\u0026#34;我是一条debug消息\u0026#34;) id:=10010 name:=\u0026#34;张三\u0026#34; logger.Info(\u0026#34;我是一条info消息，来自%d-%s\u0026#34;,id,name) } //输出到文件中 logger=mylogger.NewFileLogger(\u0026#34;info\u0026#34;,time.Now().Format(\u0026#34;2006-01-02\u0026#34;),\u0026#34;./\u0026#34;,10*1024)//for { //如果消息级别为0表示是错误的等级 for{ logger.Debug(\u0026#34;我是一条debug消息\u0026#34;) id:=10010 name:=\u0026#34;张三\u0026#34; logger.Info(\u0026#34;我是一条info消息，来自%d-%s\u0026#34;,id,name) } } ","date":"2020-10-10T00:00:00Z","permalink":"https://xiaonuoz.github.io/p/golang%E6%97%A5%E5%BF%97%E5%BA%93%E7%9A%84%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0/","title":"golang日志库的简单实现"},{"content":"接口也是一种类型，特殊的类型，它不关心对应方法的变量类型（接口只针对有接收者的函数，即方法）是什么，如fmt包的println函数可以传多种数据类型进行打印，接口常用于对不同变量类型定义一个相同方法名的方法（每个方法中的实现内容和结果都可能不同），这样就可以通过一个调用接口里函数的函数的参数的不同类型来调用不同数据类型的方法（可以少记很多方法名，且统一），可以用于连接各种不同数据库，采用同一函数，用传递进去的参数类型不同来区别连接哪种数据库\n一个变量如果实现了接口中规定的所有方法（必须是实现了所有方法），那么这个变量就实现了这个接口，也可以称为这个接口类型的变量，那么定义一个接口变量就可以接收该变量的值，最后接口变量的类型会由于接收赋值变成该变量的类型\n1 2 3 4 s1 := stu{name: \u0026#34;学生\u0026#34;} var s sayer s = s1 fmt.Printf(\u0026#34;%T\u0026#34;, s) 接口它自己不会存值，它的类型取决于给了它什么值，给什么值就是什么类型，接口类型定义时分配了一个动态类型和一个动态值，类型和值都为nil，然后将其他类型变量赋值给接口类型就是将值和值的类型赋值给接口，这样子就可以实现接口可以存任意类型的值。\n一个类型的方法通常我们会用指针接收者，因为这样可以对原数据进行修改，也可以对值接收者传递指针，但是这样依然不能更改到原数据\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 //定义一个接口类型，里面只有方法声明 type sayer interface { say() } //定义一个学生的结构体和方法 type stu struct { name string } func (s *stu) say() { fmt.Println(s.name, \u0026#34;say hi ttt\u0026#34;) s.name = \u0026#34;王五\u0026#34; } type tea struct { name string } func (t *tea) say() { fmt.Println(t.name, \u0026#34;say hi\u0026#34;) } func toSay(s sayer) { s.say() } func main() { s1 := stu{name: \u0026#34;学生\u0026#34;} toSay(\u0026amp;s1) fmt.Println(s1) } 关于接口需要注意的是：只有当有两个或两个以上的具体类型必须以相同的方式进行处理时才需要定义接口，不要为了写接口而定义接口，那样只会增加不必要的抽象，导致不必要的运行时损耗。\n接口命名习惯以er结尾，只有方法声明，\n没有实现也没有数据字段，但是方法是可以添加参数的\n然后在main函数中声明一个接口类型，var i Humaner,然后让 i 等于其他接收者的自定义类型，当 i 等于某个接收者类型它就会调用那个类型的同名方法，但是i等于其他接收者时，i只能调用属于i接口的方法，专属于接收者的方法以及其变量，接口都不能调用\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 //定义接口类型 type Hunmaner interface { sayhi() } //定义学生结构体类型 type student struct { name string id int } //定义学生方法 func (student) sayhi() { fmt.Println(\u0026#34;student say hi\u0026#34;) } //定义老师结构体类型 type teacher struct { name string addre string } //定义老师方法 func (teacher) sayhi() { fmt.Println(\u0026#34;teacher say hi\u0026#34;) } //实现多态，定义一个接口的方法，方法参数为接口类型，将其他接收者类型的实参传进去就可以实现调用不同方法 func whoSayhi(i Hunmaner){ i.sayhi() } func main() { //i等于学生结构体，i就可以调用学生的方法 var i Hunmaner i = student{name: \u0026#34;mike\u0026#34;} i.sayhi() //输出学生方法中的语句 //通过接口实现不同的方法 whoSayhi(student{name: \u0026#34;mike\u0026#34;}) i = teacher{name: \u0026#34;laoshi\u0026#34;} i.sayhi() //输出老师方法中的语句 } 也可以通过切片同时实现多种接收者的同名的方法\n接口也可以实现继承（使用匿名字段），当定义一个Personer变量时，它可以调用父级的sayhi()方法也可以调用自身的sing()方法\n父级可以等于子级的值，反过来则不可以（由多的向少的转换）,如 定义一个父级i，让i等于子级iPro，i=iPro是可以的，注意语法是 父级=子级，这个作用于设置子级的值，然后将子级的值赋给父级使用，父级只能使用属于父级的方法，子级的方法无法使用\n空接口 如何判断一个空接口变量中值的类型（value.(type)） ,ok模式常用于测试map的对应key是否有值，有值ok为ture，没有则为false，value用于存放为true时将空接口类型中的值强转成对应类型的值，为false时为对应类型的零值\n1 2 i := map[int]string{1: \u0026#34;a\u0026#34;} value, ok := i[1] 还有另外五种用法（https://zhuanlan.zhihu.com/p/129220255）\n有一种用法是判断空接口类型变量的type（切片和数组无法使用这个，因为它们两个在定义之初就已经确定了类型），value的返回依然是ok为true时将空接口类型中的值强转成对应类型的值，为false时存对应类型的零值\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 i := make([]interface{}, 3) i[0] = 1 i[1] = \u0026#34;str\u0026#34; i[2] = stu{\u0026#34;mike\u0026#34;} for _, deta := range i { if value, ok := deta.(int); ok == true { fmt.Println(\u0026#34;这是一个整型\u0026#34;, value) } else if value, ok := deta.(string); ok == true { fmt.Println(\u0026#34;这是一个字符串\u0026#34;, value) } else if value, ok := deta.(stu); ok == true { fmt.Println(\u0026#34;这是一个结构体类型\u0026#34;, value) } } //也可以使用switch方法，用此方法时括号中为type for _, deta := range i { switch value := deta.(type) { case int: fmt.Println(\u0026#34;这是一个整型\u0026#34;, value) case string: fmt.Println(\u0026#34;这是一个字符串\u0026#34;, value) case stu: fmt.Println(\u0026#34;这是一个结构体类型\u0026#34;, value) } } 反射 **反射是指在程序运行期对程序本身进行访问和修改的能力。**程序在编译时，变量被转换为内存地址，变量名不会被编译器写入到可执行部分。在运行程序时，程序无法获取自身的信息。(如函数的参数是一个空接口类型，那么在程序编译时它是不知道这个空接口存入什么类型的，只有在运行到此函数时才能确定)\n支持反射的语言可以在程序编译期将变量的反射信息，如字段名称、类型信息、结构体信息等整合到可执行文件中，并给程序提供接口访问反射信息，这样就可以在程序运行期获取类型的反射信息，并且有能力修改它们。\nGo程序在运行期使用reflect包访问程序的反射信息。\nreflect包 在Go语言的反射机制中，任何接口值都由是 一个具体类型和 具体类型的值两部分组成的(动态类型和动态值)。 在Go语言中反射的相关功能由内置的reflect包提供，任意接口值在反射中都可以理解为由 reflect.Type和 reflect.Value两部分组成，并且reflect包提供了 reflect.TypeOf和 reflect.ValueOf两个函数来获取任意对象的Value和Type。reflect.TypeOf(a)返回值存的是原始值的类型，但此返回值的类型是 *reflect.rtype\n在反射中关于类型还划分为两种：类型（Type）和 种类（Kind）。因为在Go语言中我们可以使用type关键字构造很多自定义类型，而 种类（Kind）就是指底层的类型，但在反射中，当需要区分指针、结构体等大品种的类型时，就会用到 种类（Kind）。通过reflect.TypeOf(a)返回值的name()方法取到的是具体类型，kind()方法取到的是种类（如结构体）\nValueOf reflect.ValueOf()返回的是 reflect.Value类型，其中包含了原始值的值信息。b与原始值之间可以互相转换。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 func reflectValue(x interface{}) { v := reflect.ValueOf(x) k := v.Kind()\t//这个kind是获取值的类型种类 switch k { case reflect.Int64: // v.Int()从反射中获取整型的原始值，然后通过int64()强制类型转换 fmt.Printf(\u0026#34;type is int64, value is %d\\n\u0026#34;, int64(v.Int())) case reflect.Float32: // v.Float()从反射中获取浮点型的原始值，然后通过float32()强制类型转换 fmt.Printf(\u0026#34;type is float32, value is %f\\n\u0026#34;, float32(v.Float())) case reflect.Float64: // v.Float()从反射中获取浮点型的原始值，然后通过float64()强制类型转换 fmt.Printf(\u0026#34;type is float64, value is %f\\n\u0026#34;, float64(v.Float())) } } func main() { var a float32 = 3.14 var b int64 = 100 reflectValue(a) // type is float32, value is 3.140000 reflectValue(b) // type is int64, value is 100 // 将int类型的原始值转换为reflect.Value类型 c := reflect.ValueOf(10) fmt.Printf(\u0026#34;type c :%T\\n\u0026#34;, c) // type c :reflect.Value } ValueOf没有int64、int32之类的，它只有int、float等\n参考文档： https://www.liwenzhou.com/posts/Go/13_reflect/\n实现对ini配置文件的反射（将ini中的内容写入到结构体中） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 package main import ( \u0026#34;errors\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io/ioutil\u0026#34; \u0026#34;reflect\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;strings\u0026#34; ) type MysqlConfig struct { Address string `ini:\u0026#34;address\u0026#34;` Port int `ini:\u0026#34;port\u0026#34;` Username string `ini:\u0026#34;username\u0026#34;` Password string `ini:\u0026#34;password\u0026#34;` } type RedisConfig struct { Host string `ini:\u0026#34;host\u0026#34;` Port int `ini:\u0026#34;port\u0026#34;` Password string `ini:\u0026#34;password\u0026#34;` Database uint8 `ini:\u0026#34;database\u0026#34;` } //嵌套结构体，用于关联配置文件和结构体 type Config struct { MysqlConfig `ini:\u0026#34;mysql\u0026#34;` RedisConfig `ini:\u0026#34;redis\u0026#34;` } func loadIni(b []byte, data interface{}) error { //1.参数的校验 t := reflect.TypeOf(data) //1.1 传进来的data参数必须是指针类型（因为需要在函数中对其赋值，不使用指针就是修改的副本） if t.Kind() != reflect.Ptr { err := errors.New(\u0026#34;传入的参数不是指针\u0026#34;) return err } //1.2 传进来的data参数必须是结构体类型指针（因为配置文件中各种键值对需要赋值给结构体字段） if t.Elem().Kind() != reflect.Struct { //Elem()方法可以由对应的指针函数获取到相应的值 err := errors.New(\u0026#34;传入的参数不是结构体指针\u0026#34;) return err } var structName string //2.一行一行的读数据,先将传进来的字节切片以换行切成字符串切片，windows中换行是\\r\\n lineStr := strings.Split(string(b), \u0026#34;\\r\\n\u0026#34;) for index, line := range lineStr { //先去掉首尾空格 line = strings.TrimSpace(line) //如果是空行就跳过 if len(line)==0{ continue } // 如果是注释就跳过，首字母是#或者; if strings.HasPrefix(line, \u0026#34;#\u0026#34;) || strings.HasPrefix(line, \u0026#34;;\u0026#34;) { continue } //加判断避免以 ] 开头 if strings.HasPrefix(line, \u0026#34;]\u0026#34;) { err := fmt.Errorf(\u0026#34;节点定义错误，line:%d\u0026#34;, index+1) return err } // 如果是[开头就表示是节(section) if strings.HasPrefix(line, \u0026#34;[\u0026#34;) { if line[len(line)-1] != \u0026#39;]\u0026#39; { err := fmt.Errorf(\u0026#34;节点定义错误，line:%d\u0026#34;, index+1) return err } //先切出不包含[]的内容，然后去空格，最好统计长度为0则表示节点中全是空格或者未写 sectionName := strings.TrimSpace(line[1 : len(line)-1]) if len(sectionName) == 0 { err := fmt.Errorf(\u0026#34;节点定义错误，line:%d\u0026#34;, index+1) return err } //根据字符串sectionName去data里面根据反射找到对应的结构体,NumField()返回对应结构体的字段个数 for i := 0; i \u0026lt; t.Elem().NumField(); i++ { filed := t.Elem().Field(i) //找到对应的结构体 if sectionName ==filed.Tag.Get(\u0026#34;ini\u0026#34;) { structName=filed.Name\t//filed.Name指对应结构体名 } } } else { // 不是节点才是需要的键值对 //\t1.1 以等号分隔这一行，等号左边是key，右边是value //如果不包含=，行首或者行位是=就抛出错误 if strings.Index(line,\u0026#34;=\u0026#34;) ==-1 || strings.HasPrefix(line,\u0026#34;=\u0026#34;)||strings.HasSuffix(line,\u0026#34;=\u0026#34;){ err:=fmt.Errorf(\u0026#34;第%d行语法错误\u0026#34;,index+1) return err } idx:=strings.Index(line,\u0026#34;=\u0026#34;) key:=line[:idx]\t//ini中的key value:=line[idx+1:]\t//ini中的value // 1.2 根据structName去data里面把对应的嵌套结构体取出来 v :=reflect.ValueOf(data) sValue:=v.Elem().FieldByName(structName)\t//拿到嵌套结构体中的子结构体值信息 sType:=sValue.Type()\t//拿到嵌套结构体中的子结构体累心信息 if sValue.Kind()!=reflect.Struct{ err:=fmt.Errorf(\u0026#34;%s不是一个结构体\u0026#34;,structName) return err } var filedName string var filedType reflect.StructField // 1.3 遍历嵌套结构体的每一个字段，判断tag是否等于key for i:=0;i\u0026lt;sValue.NumField();i++{ filed:=sType.Field(i)\t//找到对应子结构体的成员 filedType=filed\t//tag是存储在类型信息中的 //将结构体ini的字段和配置文件中的字段进行比较，相等则赋值 if filed.Tag.Get(\u0026#34;ini\u0026#34;)==key{ filedName=filed.Name\t//filed.Name返回结构体的成员名 break } } //for循环过后如果filedName为0则表示在结构体中没找到这个字段，直接跳过 if len(filedName) ==0 { continue } // 1.4 根据filedName取出这个字段并赋值 filedObj:=sValue.FieldByName(filedName) switch filedType.Type.Kind() { case reflect.String: filedObj.SetString(value) case reflect.Int,reflect.Int8,reflect.Int16,reflect.Int32,reflect.Int64: valueInt,err:=strconv.ParseInt(value,10,64) if err!=nil{ err=fmt.Errorf(\u0026#34;int转换失败,line:%d\u0026#34;,index+1) return err } filedObj.SetInt(valueInt) } } } return nil } func main() { var cfg Config //使用ReadFile可以直接将文件内容全部读取出来，然后存进变量。 //用os.open()来一行行的读取会让程序运行时文件一直处于打开状态 buf, err := ioutil.ReadFile(\u0026#34;./config.ini\u0026#34;) if err != nil { fmt.Printf(\u0026#34;read file faild,err:%v\\n\u0026#34;, err) return } err = loadIni(buf, \u0026amp;cfg) if err != nil { fmt.Printf(\u0026#34;loadIni file faild,err:%v\\n\u0026#34;, err) return } fmt.Println(cfg) } ini配置文件：\n1 2 3 4 5 6 7 8 9 10 11 12 13 ; mysql config [mysql] address=10.20.30.40 port=3306 username=root password=123456 # redis config [redis] host=127.0.0.1 port=6379 password=root database=0 ","date":"2020-10-06T00:00:00Z","permalink":"https://xiaonuoz.github.io/p/golang%E6%8E%A5%E5%8F%A3/","title":"golang接口"},{"content":"uint类型不能有负数，int类型可以为正也可以为负\ngoroutinue 1 2 3 4 5 6 7 8 9 10 11 12 13 14 go func() { func() { fmt.Println(\u0026#34;这是子go\u0026#34;) // 退出当前函数（此匿名函数） // return // 退出当前进程（主携程加子携程合在一起是一个进程,似崩溃） // os.Exit(-1) // 退出当前子携程 runtime.Goexit() }() fmt.Println(\u0026#34;子go over\u0026#34;) }() time.Sleep(2 * time.Second) fmt.Println(\u0026#34;主go over\u0026#34;) 对于有缓冲的通道，如果写的次数和读的次数不一样就会造成严重的问题，会无限阻塞，在主程序出现就会崩溃死锁，在子携程中阻塞就会内存泄漏。所以最好使用for range来对管道进行读，写完所有数据后最好进行close来关闭这个管道，这样在读的for range中如果发现读的是一个已经关闭的管道（此时管道关闭，无法继续往里面写数据，会崩溃），那它读完这个管道中的所有数据后就会跳出循环。且close要写在写端，如果在读端，和写端速度不一致会导致可能有数据还在往里面写\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 channels := make(chan int, 10) go func() { for i := 0; i \u0026lt; 2; i++ { time.Sleep(time.Second) channels \u0026lt;- i } close(channels) }() // for range是不知道管道是否已经写完了的，因此当写的携程结束后这边依然在等待新的数据，就会死锁 // 所以要在写完数据之后关闭管道，这样管道就变为了nil，for range遍历关闭的管道时会退出 for v := range channels { fmt.Println(\u0026#34;开始读取\u0026#34;) fmt.Println(v) fmt.Println(\u0026#34;结束读取\u0026#34;) } 可以通过 v,ok:=\u0026lt;-channel 来判断管道是否已经关闭，ok为false则表示没从管道中取出数据，v为对应类型的零值\nbyte和rune类型 组成字符串的元素叫做字符，可以通过遍历（range）或者下标获取单个字符（s[0]）。字符用单引号括起来，字符串用双引号\n当需要处理中文日文等，需要用到rune类型，它实际上是一个int32\n1 2 3 4 s1 := \u0026#34;白萝卜\u0026#34; s2 := []rune(s1) fmt.Println(string(s2[1])) 如果不使用rune直接调用s1，最后出现的会是一个乱码 单词字母可以直接遍历，它属于byte类型\nGO语言中只有显式强制类型转换(var a float32 =float32(n))，没有隐式转换。该语法只能在两个类型相互支持转换的时候使用（int到float，string到切片等）\n1 2 3 //数组可以通过...来自动推断后续数组初始化值的长度 a1 := [...]int{1, 2, 3, 4, 5, 6, 7} fmt.Println(a1) 切片 当使用var定义了切片或map却没有使用make时，它是对应的零值nil，nil时是无法调用成员的，所以需要make，而结构体的零值不是nil，它里面有对应的成员零值\n切片的容量cap()是指从第一个切片所对应的底层数组元素到底层数组最后一个元素的元素总数量\n切片是引用类型，都指向了底层的一个数组，即修改切片会修改到底层数组（切片不保存值，它操作的是底层数组的值），修改数组会影响到切片\n1 2 3 4 5 6 7 8 9 10 a1 := [...]int{1, 3, 5, 7, 8} s1 := a1[:] s1 = append(s1, 555) s1[3] = 777 fmt.Println(s1) fmt.Println(a1) 输出结果： [1 3 5 777 8 555] [1 3 5 7 8] 当切片追加一个元素且超过原数组最大值后，它会重新创建一个属于自己的底层数组，再修改切片的值就不会让原底层数组发送改变\n会报panic错误，因为定义了一个int类型的指针但是没有初始化，默认指针值为nil，对空指针所对应的内容进去赋值是不可能的，所以需要使用new函数来申请一个内存地址\tvar a = new(int)\nmap如果没有进行初始化（make），它就没有在内存中开辟空间，默认值为nil，是无法往map中写入数据的\n切片，map动态扩容会影响到性能，最好是在创建时就直接估算好\nmap可以用value ,ok :=m1[\u0026ldquo;key\u0026rdquo;]来判断key对应的值是否存在，返回的布尔值存在ok中\ncopy复制切片要注意：目标切片如果make出的容量不足以复制源切片的全部元素时，会复制从0开始的部分元素，如果目标切片make给的长度是0，copy是不会自动扩容的，则没有元素复制过去\n1 if unicode.Is(unicode.Han,char) 判断一个字符是不是汉字，unicode.Han 函数类型也可以作为另一个函数的参数和返回值，传参和返回时千万不要加括号，加上括号表示调用此函数方法，不加就是作为参数或者返回值使用\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 func f1(x int){ fmt.Println(\u0026#34;我是f1\u0026#34;) } func f2(x,y int) int{ return x+y } func f3(x func(int)) func(int,int)int{ return f2 } func main(){ f4:=f3(f1) fmt.Println(f4(5,10)) } defer延迟加载在注册时就会将变量的状态代入进去，会先进入代入延迟加载中的变量的值，即a会先代入1这个值，后面a改变也不会影响到defer的值，如果defer函数中嵌套了函数calc(20,add())，它也会先将里面的函数(add)计算出值然后再搁置在最后结束时运行defer,defer只会保存最外层的函数至最后执行\n1 2 3 a:=1 defer fmt.Println(a) a=2 构造函数（结构体） 约定成俗用new开头作为构造函数\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 type person struct { name string age int } func newPerson(name string,age int) *person{ return \u0026amp;person{ name:name, age:age, } } func main() { p1:=newPerson(\u0026#34;张三\u0026#34;,12) fmt.Println(p1) } 结构体可以使用声明的方式产生新的变量，也可以通过函数来产生，通过函数产生时最好返回值为结构体的指针，减少程序的内存开销，因为结构体属于值传递，如果不使用指针就是将函数返回值拷贝一份给变量，当结构体的元素更多或者使用函数的次数更多，会给内存造成压力，而且复制指针比复制结构体内容要更快，内存压力更小\n方法 只能给自己定义的类型添加方法，是无法给go标准库中的(int,string等)类型和别的包里面的类型添加方法的，所以才有type myInt int这种情况，给其他包添加方法也可以通过type来实现\n方法的接受者形参约定成俗用类型的首字母小写来定义，接受者分为指针传递和值传递，如果是值传递，那么方法内部对变量的改变不会影响到外部，且建议当同一类型的某个方法使用了指针接受者，其他方法也应该用指针接受者\n一般都使用指针接受者，因为如果结构体成员过多，值传递拷贝会影响内存的占用，而指针永远都是一个uint64类型的值\ntime包 time.Now()记录了当前时间，time.Now().Year()可以获取当前年，还有Month，Day等\ntime.Now().Unix()表示从1970年1月1日8点整到现在的时间戳，UnixNano()精确到纳秒\ntime.Unix()可以将时间戳转换为时间格式\ntime有个add方法可以进行时间增加，如当前时间的5小时后为 time.Now().Add(5*time.Hour)\n格式化时间将时间对象转换成字符串类型的时间，使用 time.Now().Format(2006-01-02 15:04:05),它使用的是GO的诞生时间2006年1月2日15点4分5秒（可以记作2006 1 2 3 4 5），如果想用12小时制的在后面加个PM即可\n按照对应字符串格式将字符串时间解析成时间对象类型 使用Parse()方法来进行解析\n1 2 3 4 5 6 t,err:=time.Parse(\u0026#34;2006-01-02\u0026#34;,\u0026#34;1998-01-19\u0026#34;) if err!=nil{ fmt.Println(\u0026#34;输入的时间格式不对\u0026#34;) return } fmt.Println(t) 对于求两个时间间隔的结果不准确的解决方案 1 2 3 4 5 6 7 8 9 10 11 //Now()会按照本地时间获取，而Parse则会按照UTC时间获取，因此两者相减的时间(如果本地不是UTC时间)会比正常情况多几个小时 now := time.Now() tim, _ := time.Parse(\u0026#34;2006-01-02 15:04:05\u0026#34;, \u0026#34;2020-10-29 22:00:00\u0026#34;) d := tim.Sub(now) fmt.Println(d) //按照指定时区解析时间 loc, _ := time.LoadLocation(\u0026#34;Asia/Shanghai\u0026#34;) tim1, _ := time.ParseInLocation(\u0026#34;2006-01-02 15:04:05\u0026#34;, \u0026#34;2020-10-29 22:00:00\u0026#34;, loc) d1 := tim1.Sub(now) fmt.Println(d1) 对于可变参数的理解 1 2 3 4 5 6 7 8 9 10 func f1(a ...interface{}){ fmt.Println(a) fmt.Println(a...) } func main(){ f1(1,2,3) } 输出结果： [1 2 3]\t//这个为一个变量的输出 1 2 3\t//这是三个变量 在使用函数的可变参数时（只有空接口类型才能使用三个\u0026hellip;进行拆解），在参数后面加三个点表示拆开可变参数生成的切片，分成多个变量，如果不加三个点就表示将a作为一个空接口切片类型传入\n在golang中，栈（stack）和堆（heap）存的东西不一样，栈存的是对应类型的名字，堆存的是对应类型的具体数据，由栈指向堆\n使用map时，value如果是一个结构体，必须使用指针类型，map里结构体无法直接寻址，必须取址 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 var i map[string]*Age=make(map[string]*Age) i[\u0026#34;张文\u0026#34;]=\u0026amp;Age{Id: 23} i[\u0026#34;利尔\u0026#34;]=\u0026amp;Age{Id: 38} //此处map的value如果不是地址而是结构体本身的话，v的地址一直都是一个地址，遍历只是将新的值覆盖到旧地址上。并且它是无法寻址的，即无法找到对应结构体的成员进行改变 for _,v:=range i{ v.Id=v.Id+3 } //将结构体赋值给map则需要这样 for i:=0;i\u0026lt;len(ages);i++{ x[ages[i].Name]=\u0026amp;ages[i] } for _,v:=range i{ fmt.Println(v.Id) } 重写sort方法实现结构体的排序 1 2 3 4 5 6 7 8 9 10 11 // SortByReadBook 按照阅读量排序，定义一个结构体是原结构体的切片 type SortByReadBook []ReadLeaderboardResp func (s SortByReadBook) Len() int { return len(s) } //重写len方法 func (s SortByReadBook) Swap(i, j int) { s[i], s[j] = s[j], s[i] } //重写swap方法 func (s SortByReadBook) Less(i, j int) bool {\t//重写less方法 //uint用大于小于，Decimal使用自身的lessThan方法 return s[i].ReadBookCount \u0026gt; (s[j].ReadBookCount) } 调用时则使用sort方法，强转原结构体 sort.Sort(SortByReadBook(readLeader)) vscode 调试配置 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { // 使用 IntelliSense 了解相关属性。 // 悬停以查看现有属性的描述。 // 欲了解更多信息，请访问: https://go.microsoft.com/fwlink/?linkid=830387 \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Launch Package\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;go\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;auto\u0026#34;, \u0026#34;program\u0026#34;: \u0026#34;${workspaceFolder}//cmd//operation//main.go\u0026#34; } ] } 结构体的String方法 1 2 3 4 5 6 7 8 9 10 11 12 type Test struct { name string } func (t Test) String() string { return t.name + \u0026#34;======111\u0026#34; } func main() { t := Test{\u0026#34;张三\u0026#34;} fmt.Println(t) } 如果给结构体绑定一个String的方法，那么在打印时只打印此结构体会隐式的调用此结构体的String方法\n通过reflect.Value修改值 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 type test struct { Name string Age int } func main() { t := reflect.ValueOf(\u0026amp;test{\u0026#34;张三\u0026#34;, 18}) // a reflect.Type switch t.Kind() { case reflect.Ptr: if t.IsNil() { log.Println(\u0026#34;this is nil\u0026#34;) } else { va := t.Elem() for i := 0; i \u0026lt; va.NumField(); i++ { // 1. 针对结构体成员是小写不公开的情况需要这种 // temp := va.Addr().Interface().(*test) // temp.name = \u0026#34;李四\u0026#34; // 2. 公开的可以直接通过set方法来设置 if va.Type().Field(i).Name == \u0026#34;Name\u0026#34; { fmt.Println(\u0026#34;true\u0026#34;) va.Field(i).SetString(\u0026#34;历史\u0026#34;) } } } } fmt.Println(t) } golang是没有野指针的，因为只要有一个指针指向函数返回的变量，它就不会被回收。Go语言的内存回收机制规定，只要有一个指针指向一个变量，那么这个变量就不会被释放（内存逃逸），因此在 Go 语言中返回函数参数或临时变量是安全的。\n","date":"2020-10-05T00:00:00Z","permalink":"https://xiaonuoz.github.io/p/golang%E5%B0%8F%E7%9F%A5%E8%AF%86%E7%82%B9/","title":"golang小知识点"},{"content":"函数类型是可以作为参数和返回值使用的\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 func main() { ret := f2(f1, 10, 20) f3(ret) //或者f3(f2(f1, 10, 20)) } func f1(x, y int) int { fmt.Println(\u0026#34;this is f2\u0026#34;) return x + y } func f2(x func(int, int) int, m, n int) func() { tmp := func() { a1 := x(m, n) fmt.Println(a1) } return tmp } func f3(f func()) { fmt.Println(\u0026#34;this is f3\u0026#34;) f() } 将f2作为两个函数的中转站，f3只能接收无参无返回值，但是我们只有一个f1，是有参的，所以定义一个f2让它返回一个无参函数类型的返回值，将f1和f1的参数传进去，这样通过调用f2就可以同时调用f3，并且在f2中使用匿名函数调用f1，这样就可以通过f3(f2(f1,10,20))来实现f3调用f1\n闭包的概念就是一个函数除了调用本身内部的变量还可以调用函数外部作用域的变量（其他函数中的局部变量）\n1 2 3 4 5 6 7 8 9 10 11 12 func adder(x int) func (int) int{ return func(y int) int{ x +=y return x } } func main(){ a1:=adder(100) y:=a1(100) fmt.Println(y) } 平时的函数，你调用它就只需要看它内部有哪些变量，但是闭包通过匿名函数可以实现我在函数中使用x并不需要定义，它直接在匿名函数上一级函数进行定义了，匿名变量作为返回值就可以对上一级的函数变量进行引用\n这样示例一就实现了函数一的参数是一个无参无返回值的函数类型，但是只有一个有参有返回值的函数二，通过定义一个函数三，将有参有返回值的函数二和它需要的变量作为参数传进去，然后直接返回一个无参无返回值的匿名函数，在匿名函数中调用有参函数二，这样在需要的地方只需要调用中转函数三并将有参函数二传进去，就可以获得一个无参无返回值但是调用了有参函数二的函数类型，可以作为函数一的参数使用了\n闭包=函数 + 函数上一级作用域的变量的引用\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 func calc(base int) (func(int)int,func(int)int){ add:=func(i int)int{ base+=i return base } sub:=func (i int)int{ base-=i return base } return add,sub } func main() { f1,f2:=calc(10) fmt.Println(f1(1),f2(2)) //11 9 fmt.Println(f1(3),f2(4)) //12 8 } add和sub函数调用了外部的base变量，calc函数只调用了一次，即base只初始化了一次（10），第一次先调用add就是寻找外部的变量，增加后返回，但是此时base值已经改变（变为11），然后进行sub函数,重新寻找base,此时为11（11-2），无论进行多少次都是在上次的基础上进行计算，不会重新初始化为10了\n","date":"2020-10-04T00:00:00Z","permalink":"https://xiaonuoz.github.io/p/golang%E9%97%AD%E5%8C%85/","title":"golang闭包"},{"content":"设备文件有屏幕键盘等，标准输出就是屏幕，可以通过os.Stdout.Close()来阻止后面的程序输出内容，可以通过os.Stdint.Close()来阻止后面的程序通过键盘获取输入内容\n文件的创建和写入 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 path := \u0026#34;./test.txt\u0026#34; //创建文件，如果文件存在则打开文件且清空文件内容,它返回一个file的指针和一个error f, err := os.Create(path) if err != nil { //如果有错误就输出错误信息并且中止函数 fmt.Println(err) return } //使用完毕记得要关闭文件，使用defer就可以在函数结束的前一刹那进行关闭 defer f.Close() //Sprintln()可以将一行信息赋值给一个变量 buf := fmt.Sprintln(\u0026#34;我往文件内写入东西\u0026#34;) //往文件内写入字符串，f代表文件的指针，WriteString就可以将内容追加到文件，注意是追加 //它返回两个值，一个是写入的字节数，另一个是error n, err := f.WriteString(buf) if err != nil { fmt.Println(err) return } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 fileObj,err:= os.OpenFile(\u0026#34;./xxx.txt\u0026#34;,os.O_CREATE|os.O_APPEND,0644) if err!=nil{ fmt.Println(err) return } fileObj.Write([]byte(\u0026#34;强转string为字节切片\u0026#34;)) //也可以用bufio进行写操作 wr := bufio.NewWriter(fileObj) //写到缓存中 wr.WriteString(\u0026#34;hello\\n\u0026#34;) //这一步是将缓存中的内容写入文件，如果没有这一步，内容是在缓存中的，等程序执行完毕就会消失 writer.Flush() //也可以使用ioutil.WriteFile将内容写入文件 ioutil.WriteFile(\u0026#34;./xx.txt\u0026#34;,[]byte(\u0026#34;写入的内容\u0026#34;),0666) 注意：在windows中，第三个参数文件权限可以随便写，因为这只针对于linux用户，os.O_CREATE|os.O_APPEND表示没有文件就创建，有文件就打开，内容是追加写入(它的实现方式是这两个参数分别对应一个十六进制的数，转成2进制后根据数的值进行操作)，如CREATE是0x00040，二进制是0100 0000，APPEND是0x00400，二进制是0100 0000 0000，使用| 位运算符表示两位有一个1就为1，即最后的值是0100 0100 0000，这样就实现了传一个int参数有多个功能，可以进行多个位运算符。\n像只读和只写就是在同一位上一个为0一个为1，这样最后这一位就为1，只执行值为1的那个操作，就像linux的权限一样，每个位置的1代表不同的意义\nos.O_CREATE|os.O_APPEND 和 os.O_CREATE|os.O_TRUNC APPEND表示追加写入，TRUNC表示清空后写入\n文件的读取 下面的代码可以实现指定读取多少内容，如果想直接读完整个文件，只需要在读取那里加一个for循环，它就会一直往下读取直到结尾出现error抛出EOF错误（EOF表示结尾），注意：抛出EOF的if语句要写到err !=nil中，因为EOF错误是它的子集，在err不为空中加return，而EOF中只需要加break，因为break触发后直接跳出循环了，就不会触发return了\n尽量将关闭文件的defer写到err判定后面，因为如果先写defer，如果出现了err错误程序返回的*File是对应的零值nil，当程序结束时，defer中对于nil进行Close()函数就会造成panic错误（使用文件操作这里不会报错，因为close会返回一个error，但是如果调用的是第三方函数库，可能就会因此产生panic），而在err中写入return语句后，如果出现了err错误的话，直接返回，并不会执行到下面defer中的Close()函数\n1 2 3 4 5 6 7 if err1 != nil { if err1 == io.EOF{ break } fmt.Println(err1) return } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 path := \u0026#34;./test.txt\u0026#34; //打开文件,返回一个文件指针和一个error f, err := os.Open(path) if err != nil { fmt.Println(err) } //函数运行结束时关闭文件 defer f.Close() //创建一个2k长度的字节切片，表示读取的总量 buf := make([]byte, 1024*10) //Read返回两个值，一个是字节数，一个是error //n是读取了多少文件字节数，如果buf大于文件总字节数，n返回总字节数。 //如果总字节数大于buf，则n等于buf，即最多只能读取buf字节的内容，如果想多读取点就只能增大buf的长度值 //Read()方法会将读取的内容放入buf中，后面查看buf内容即是查看文件内容 n, err1 := f.Read(buf) if err1 != nil \u0026amp;\u0026amp; err1 != io.EOF {\t//文件出错并且没有读到结尾 fmt.Println(err1) return } //可以用m:n来读取buf中间内容，n不能大于buf前面设定的值（即不能大于总长度），否则会报错 //n可以设置常量，即表示查看buf容量中从m到n的内容 //设置n则可以实现当文件总字节数小于buf量全部读取，大于则读取设置的最大值 fmt.Print(\u0026#34;buf:\u0026#34;, string(buf[:n])) //buf中的值都是字节型的，可以通过强转变成string //不用println是原因是ln会进行换行，如果第一次读取并没有读取到行末尾， //ln还是会让它换行，这会导致读取内容有问题 如何解决fmt.Scanln()无法读取输入空格后的内容 Scanln()是以空白符分隔的，空白符就包括空格 回车等，如果输入的内容中含有空格，就只能读取到空格以前的内容，因此可以使用bufio来获取标准输入\n1 2 3 4 5 fmt.Println(\u0026#34;请输入内容：\u0026#34;) reader := bufio.NewReader(os.Stdin) //一直读，读到\\n结束,返回读取到的内容 str, _ := reader.ReadString(\u0026#39;\\n\u0026#39;) fmt.Println(\u0026#34;你输入的内容是：\u0026#34;, str) 如何一行一行的进行读取且务必读完文件的全部内容（不论大小） bufio包有一个NewReader函数可以为文件的读取创建一个缓冲区并返回一个缓冲区的指针\nReadBytes是缓冲区 *Reader的方法，读取缓冲区直到第一次遇到delim字节（即指定的关键值\u0026rsquo;\\n\u0026rsquo;等），读取出一次后缓冲区中对应的内容就会消失，因此可以用for死循环来读取多行缓冲区内容\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 path := \u0026#34;./test.txt\u0026#34; //打开文件,返回一个文件指针和一个error f, err := os.Open(path) if err != nil { fmt.Println(err) } //函数运行结束时关闭文件 defer f.Close() //新建一个缓冲区，先把内容放进缓冲区里 r := bufio.NewReader(f) for { //遇到\\n就结束读取,buf为读取到的数据,这种会将\\n也读取进去，所以使用Printf() buf, err1 := r.ReadBytes(\u0026#39;\\n\u0026#39;) //当读到结尾即退出循环 fmt.Printf(\u0026#34;%v\u0026#34;, string(buf)) /*把err判断放到打印后面的原因是如果将它放在前面，它会先判断是否是最后一行，当处于最后 一行时它直接跳出循环，最后一行的输出语句就不会进行，那么就会少输出一行 */ if err1 != nil { if err1 == io.EOF { break } fmt.Println(err1) } } 使用ioutil直接读取整个文件 1 2 3 4 5 6 7 8 9 10 11 12 13 //它的底层用的依然是os.Open()的原理,文件的打开和defer关闭在底层已经设置好了，直接使用就行 func fileByioutil(){ b,err:=ioutil.ReadFile(\u0026#34;C:\\\\Users\\\\admin\\\\Desktop\\\\新建文件夹\\\\gold_control_config.xml\u0026#34;) if err ==io.EOF{ fmt.Println(\u0026#34;读取完了\u0026#34;) return } if err!=nil{ fmt.Println(\u0026#34;file err\u0026#34;,err) return } fmt.Println(string(b)) } 实例：拷贝文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 //获取命令行参数，判断是否为3个 list := os.Args if len(list) != 3 { fmt.Println(\u0026#34;拷贝格式为xx.exe src dst\u0026#34;) } //比较目标文件和源文件是否同名 srcName := list[1] dstName := list[2] if srcName == dstName { fmt.Println(\u0026#34;源文件和目标文件不能相等\u0026#34;) } //只读方式打开源文件 sf, err1 := os.Open(srcName) if err1 != nil { fmt.Println(err1) return } //新建目标文件 df, err2 := os.Create(dstName) if err2 != nil { fmt.Println(err2) return } //操作完毕要关闭文件 defer sf.Close() defer df.Close() for { //读源文件 buf := make([]byte, 1024*4) n, err3 := sf.Read(buf) //从源文件读取内容 if err3 != nil { if err3 == io.EOF { //源文件读取完毕 break } fmt.Println(err3) return } //写入目标文件，读多少写多少 _, err4 := df.Write(buf[:n]) if err4 != nil { fmt.Println(err3) return } } 如何在一个文件中间部分插入内容 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 //打开源文件，第一次读取到要插入的点 //读取完后下次读取时光标会在此处 f1,_:=os.Open(\u0026#34;./a.txt\u0026#34;) buf:=make([]byte,1) n,_:=f1.Read(buf) //创建临时文件，并将读取的内容写入进去 f2,_:=os.Create(\u0026#34;./b.txt\u0026#34;) f2.Write(buf[:n]) //写入自己需要插入的内容 f2.Write([]byte(\u0026#34;哈哈哈哈哈\u0026#34;)) //读取源文件后面的部分，如果多就需要使用for循环 buf1:=make([]byte,1024) n1,err:=f1.Read(buf1) //将源文件后半部分写入临时文件 f2.Write(buf1[:n1]) if err!=nil{ return } f2.Close() f1.Close() //现在f2临时文件就有插入想要的内容，通过rename重命名成源文件实现覆盖 os.Rename(\u0026#34;./b.txt\u0026#34;,\u0026#34;./a.txt\u0026#34;) ","date":"2020-10-02T00:00:00Z","permalink":"https://xiaonuoz.github.io/p/golang%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C/","title":"golang文件操作"},{"content":"通过结构体生成json文件 默认情况下从结构体生成json格式，结构体的成员首字母必须大写，结构体名可以小写，因为在本包使用 Marshal方法是从本包传结构体到json的包让它编码，它肯定能访问到结构体，但是成员如果小写，json的包就无法调用到\n1 2 3 4 5 6 7 8 9 10 11 12 13 type person struct { Name string Age int } func main() { p1 := person{\u0026#34;张三\u0026#34;,19} b, _ := json.Marshal(p1) fmt.Println(string(b)) var p2 person json.Unmarshal(b, \u0026amp;p2)\t//结构体是值传递，如果不使用指针就只会修改副本，不会修改p2原有的内容 fmt.Println(p2) } Marshal()会返回两个值，一个json内容一个错误值，可以用If语句接受错误，然后用return让main函数中断，return在函数中的意义：如果函数返回值有定义，则return表示返回数据，如果没有定义返回值，return就表示退出此函数，在main函数中有return就表示中断程序但是不会报错\nbuf获得的值是一个字节切片类型，可以直接string(buf)强转成string类型\n转换成string后输出的是一个整体值，无法再用切片[0]来调用任意一个成员\n通过map生成json文件 MarshalIndent()的第三个参数为一个TAB键字符串\njson解析到结构体(json解码) json的变量名是小写，而结构体的首字母是大写，所以可以通过对结构体二次编码来关联上，不建议将结构体的首字母变成小写，因为如果变成小写的话，以后又需要生成json就会失败，并且如果程序其他地方有使用到这个结构体，成员名赋值也需要修改\nUnmarshal()有两个参数，一个是byte切片类型（通过强转jsonBuf获得），一个是结构体的指针，放指针是因为给函数传结构体变量属于值传递，并不会修改本身结构体的内容，是拷贝了一份新的结构体给函数，只有通过指针实现引用传递才可以修改本身结构体内容，这个函数的返回值只有error\n如果只需要部分解析，可以重新定义一个只含有部分成员的结构体来接受解析内容\njson解析到map map也需要传递指针过去，这里原理不太清楚，但是测试过发现不加指针\u0026amp;就无法对原值进行修改，尽管map属于引用传递\n如果通过map接受解析的值，它接受到的值类型都属于空接口类型，对其强转string(m[\u0026ldquo;test\u0026rdquo;])是无法成功的，需要使用switch断言来回推类型进行赋值\nswitch可以在赋值的同时进行判断，即 switch a := 3; 判断的是st的值，空接口类型断言隐性的将data的类型放在了判断上面，且将data空接口类型自动转换成适合它的类型\n推荐使用结构体来解析，因为map如果要确定值的类型就需要断言，过于麻烦\n","date":"2020-10-01T00:00:00Z","permalink":"https://xiaonuoz.github.io/p/%E9%80%9A%E8%BF%87golang%E5%A4%84%E7%90%86json%E6%96%87%E4%BB%B6/","title":"通过golang处理json文件"},{"content":"包的导入 go语言中导入包是从 $GOPATH/src/后开始计算的，使用 /进行路径分隔。Go语言中禁止循环导入包（A导B，B导C，C导A），原因看此连接，https://www.jianshu.com/p/ea76c0d1b609，禁用虽然会导致写代码的时候麻烦点，但是可以让依赖整洁，开发高效。\n包名可以自定义，ca \u0026quot;github.com/calc\u0026quot;\n在Go语言程序执行时导入包语句会自动触发包内部 init()函数的调用。需要注意的是： init()函数没有参数也没有返回值。 init()函数在程序运行时自动被调用执行，不能在代码中主动调用它。运行自己的包也会执行自己的init函数，可以借此使用匿名包来只调用init函数，多用于自动加载配置文件等情况\niota常量自动生成 const() 括号中的为一个常量组，后一个不赋值默认与上一行值相同，因此可以只写一个iota\niota是常量的自动生成器，他每新增一行常量声明（即使新增的不是Iota）就自动累加1，只适用于常量，const出现的时候iota被置为0，如果iota第一次使用不是在const的第一行，如它在第五行，那iota第一次的值就为4\n如果c3不赋值为iota，那它的值为100（和上一行值相同）\n如果下面重新定义const内的iota，它会重置为0，也可以只写一个iota\n如果是同一行，值就都一样\n\u0026laquo;表示把1左移十位，后面接0，即kb等于1然后十个0，但这是二进制，转换为十进制就是1024，mb就是移20位，就是1024*1024，即为1M\nfor、函数别名、匿名函数、指针 go语言在switch语句中默认保留了break，但是默认是不用写的\ngolang中函数名首字母大写表示公有的，小写表示私有的\n可以由此实现多态\n如果有多个defer，遵循先进后出原则，即先定义的defer在最后输出，defer由最下面的defer语句开始向上执行直到第一个defer语句，中间如果有defer执行失败，后面的defer语句依然会继续执行，但是如果main函数中有非defer语句执行失败导致脚本退出，那就无法继续执行后面的defer语句和普通语句\ngo有自动的垃圾回收机制，我们只需要创建内存即可。\n数组 数组比较只支持等于和不等于，比较的是两个数组的每一个元素是否都一样，且比较的两个数组类型要一样\n二维数组定义则为\n当数组通过函数传递过去后，如果在主函数中没有值可以接收return的话，它就会释放，即传过去的值无论怎么修改都和主函数中的数组没关系\n如何将数组通过地址传递呢？\n首先方法的参数为(a *[5]int),这样就可以将数组的指针实参传递给方法，然后操作原数组的某一个下标值就可以使用 ( *a)[3]=22来改变原数组某个下标值，*a必须要用圆括号合成一个整体\n随机数产生 切片 array := [5]int{1,2,3,4,5} slice := array[1:3:5] 其中第一个数表示从数组的下标几开始切（下标的起点）；第二个数表示为从何处结束（下标的终点，不包括此下标），切边的长度为第二个数减去第一个数，遵循左闭右开原则；第三个数可以自己定义，最小等于第二个数（即容量等于长度），不写默认为父级（被切的那个数组或切片）的len(），容量为第三个数减去第一个数\n切片和数组的区别：\n数组[]里面的长度是一个固定的常量，设定后便不能进行修改；\na := [5]int{} (现在数组元素为5个0)\n切片,[]里面为空或者为\u0026hellip;，切片的长度可以不固定，可以通过append方法给切片末尾追加一个成员\ns := []int{} （{}中没有值，0元素，也可以像数组一样进行添加{1,2,3}）\ns=append(s,11)\n所以可以根据[]的值来判断是切片还是数组\n切片与底层数组的关系（重要）： 对一个数组或切片进行切片操作后，它并不是和数组传参一样拷贝一个新数组使用，如果在新切片中对某个值重新定义，它会反映到最初那个数组或切片中\n切片copy的作用为将一个切片复制到另一个切片，对应下标，不会改变容量，即如果参数交换，最终也是{6,6},使用copy是因为如果直接使用等号最后操作的依然是初始切片\nmap map是无序的键值对，可以通过range进行遍历，range还可以用于数组切片等\n1 2 3 4 5 6 7 8 9 10 11 map定义: m :=map[int]string{1:\u0026#34;value\u0026#34;,2:\u0026#34;value1\u0026#34;} m :=make(map[int]string,10) #10为长度 map赋值： m[\u0026#34;key\u0026#34;]=\u0026#34;value\u0026#34; map删除： delete(m,1) #删除key为1的内容 map作为函数参数传过去时属于引用传参，即在函数中修改了map的值，原值也会修改（切片也是） 结构体类型（多个变量合成一个变量） 也可以用s1.name=\u0026ldquo;mike\u0026quot;来进行部分成员初始化或者获取成员的值\n这样就可以和Java操作对象一样使用，虽然new返回的是一个内存指针，但是在结构体类型中*p2.id和p2.id指向的都是同一块内存，所以可以直接p2.id来对新申请的结构体成员赋值\n同一个结构体的不同变量可以进行赋值以及 等于和不等于的比较（它会比较两者每一个成员是否相等）\n结构体的函数传参属于值传递，它在另一个函数中值改变并不会影响到本函数的值\n引用传参才会对同一个值进行修改，如果要改为引用传参，则需要使用指针*和\u0026amp;进行指针传递\ngo语言只分为可见和不可见，当函数名、结构体、结构体成员名首字母为大写时，它为可见的，其他包可以进行调用，调用方法为包名.函数名、包名.结构体名，如果首字母为小写则只有自己包内的其他文件可以调用\n匿名字段（类似java中的继承） 对于父级的变量部分初始化首先需要子集中的名字Person，然后是修改父级中的某一个成员，需要再加一个Person\n我比较喜欢用st :=st{}先进行初始化（或者用var st student先声明），然后再用st.name（可以直接调用父级的元素）来进行赋值\n也可以用st.Person=Person{\u0026ldquo;go\u0026rdquo;,18}或者st.Person.name这种来对父级元素进行赋值\n如果父级和子级有同名成员(如name)，那么就根据就近原则，默认调用的为子级的name成员，想调用父级的name就需要用显式调用 st.Person.name来赋值\n当使用type定义了一个普通类型的别名后，用别名定义的变量和普通类型定义的变量是不同的类型，go将他们认作两个不同的类型\n方法（类似java封装） 这样就可以通过定义的结构体元素（甚至是普通类型起别名后）调用属于它的方法，只要定义了一个对应类型的变量，就可以调用这个类型的方法（就和java中new一个新对象一样），但是方法实际上依然是函数，所以如果对象是结构体或数组，它传参依然是值传参，可以通过对方法中绑定的实例设置为实例的指针来进行引用传参\n接收者类型本身不能是指针，*int之类的可以使用，但是如果在其他地方定义别名 type long *int，再将此long作为接收者的话就不能编译通过\n只要接收者类型不一样，就算方法同名也属于不同方法\n方法集 即一个变量或变量指针调用方法不受接收者类型的约束，只要接受者类型是同种自定义类型的值或者指针都可以进行调用（它在执行方法时，内部会先自动将指针转换成变量或者反之）\n方法的继承和重写 子级不仅可以继承父级的结构体成员，还会继承父级的方法（直接调用就行）\n重写就是将接收者改为子级的自定义函数，其他参数和变量名相同，go语言会采用就近原则，先调用同作用域的方法，如果同作用域没有该方法就会调用父级的，如果需要调用父级的方法，可以采用显式调用，st.Person.PrintInfo()\n方法值：保存方法的入口地址。调用方法时无需再传递接收者，因为它已经隐藏了接受者 pFunc := p.SetInfo()，下次直接pFunc()就可以直接调用\n方法表达式：通过自定义类型来显式的把接收者传递过去，pFunc := (Person).SetInfo，调用的时候就可以使用pFunc(p)来进行调用，这里p变量可以指针和变量通用，但是Person必须和方法对应，方法是指针就需要输入 *Person\n接口 接口命名习惯以er结尾，只有方法声明，没有实现也没有数据字段，但是方法是可以添加参数的\n然后在main函数中声明一个接口类型，var i Humaner,然后让 i 等于其他接收者的自定义类型，当 i 等于某个接收者类型它就会调用那个类型的同名方法，但是i等于其他接收者时，i只能调用属于i接口的方法，专属于接收者的方法以及其变量，接口都不能调用\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 //定义接口类型 type Hunmaner interface { sayhi() } //定义学生结构体类型 type student struct { name string id int } //定义学生方法 func (student) sayhi() { fmt.Println(\u0026#34;student say hi\u0026#34;) } //定义老师结构体类型 type teacher struct { name string addre string } //定义老师方法 func (teacher) sayhi() { fmt.Println(\u0026#34;teacher say hi\u0026#34;) } //实现多态，定义一个接口的方法，方法参数为接口类型，将其他接收者类型的实参传进去就可以实现调用不同方法 func whoSayhi(i Hunmaner){ i.sayhi() } func main() { //i等于学生结构体，i就可以调用学生的方法 var i Hunmaner i = student{name: \u0026#34;mike\u0026#34;} i.sayhi() //输出学生方法中的语句 //通过接口实现不同的方法 whoSayhi(student{name: \u0026#34;mike\u0026#34;}) i = teacher{name: \u0026#34;laoshi\u0026#34;} i.sayhi() //输出老师方法中的语句 } 也可以通过切片同时实现多种接收者的同名的方法\n接口也可以实现继承（使用匿名字段），当定义一个Personer变量时，它可以调用父级的sayhi()方法也可以调用自身的sing()方法\n父级可以等于子级的值，反过来则不可以（由多的向少的转换）,如 定义一个父级i，让i等于子级iPro，i=iPro是可以的，注意语法是 父级=子级，这个作用于设置子级的值，然后将子级的值赋给父级使用，父级只能使用属于父级的方法，子级的方法无法使用\n如何判断一个空接口变量中值的类型（value.(int)） ,ok模式常用于测试map的对应key是否有值，有值ok为ture，没有则为false，value用于存放为true时的值，为false时为空\n1 2 i := map[int]string{1: \u0026#34;a\u0026#34;} value, ok := i[1] 还有另外五种用法（https://zhuanlan.zhihu.com/p/129220255）\n有一种用法是判断空接口类型变量的type（切片和数组无法使用这个，因为它们两个在定义之初就已经确定了类型）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 i := make([]interface{}, 3) i[0] = 1 i[1] = \u0026#34;str\u0026#34; i[2] = stu{\u0026#34;mike\u0026#34;} for _, deta := range i { if value, ok := deta.(int); ok == true { fmt.Println(\u0026#34;这是一个整型\u0026#34;, value) } else if value, ok := deta.(string); ok == true { fmt.Println(\u0026#34;这是一个字符串\u0026#34;, value) } else if value, ok := deta.(stu); ok == true { fmt.Println(\u0026#34;这是一个结构体类型\u0026#34;, value) } } //也可以使用switch方法，用此方法时括号中为type for _, deta := range i { switch value := deta.(type) { case int: fmt.Println(\u0026#34;这是一个整型\u0026#34;, value) case string: fmt.Println(\u0026#34;这是一个字符串\u0026#34;, value) case stu: fmt.Println(\u0026#34;这是一个结构体类型\u0026#34;, value) } } error错误接口的应用（错误抛出，不致命错误使用这种） 在工作中error常用于普通错误，这样可以自建一个错误信息抛出，error是默认值为nil （空值）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 //要先导入包errors，import \u0026#34;errors\u0026#34; func Mydiv(a, b int) (result int, err error) { if b == 0 { err = errors.New(\u0026#34;分母不能为0\u0026#34;) } else { result = a / b } return } func main() { result, err := Mydiv(10, 0) fmt.Println(result) //值为默认值0 fmt.Println(err) //值为自己定义的错误值 } panic错误接口的调用（致命错误） 自己调用panic属于显式调用，但在数组越界，空指针引用等情况发生时，go也会抛出panic异常，因为它有自己隐式调用了默认定义的panic异常\nrecover错误接口的调用（拦截panic异常，恢复程序运行流程） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 func test01(x int) { //使用数组下标越界生成panic异常 //recover必须和defer一起存在，recover()会拦截panic异常并返回panic的异常信息 //如果未发生panic异常，recover()会返回一个nil //程序会继续运行,但是出错的函数会跳过 defer func() { if err := recover(); err != nil { fmt.Println(err) } }() var a [10]int a[x] = 111 } func test02() { fmt.Println(\u0026#34;正常输出的一个方法\u0026#34;) } func main() { test01(11) test02() } 运行结果： runtime error: index out of range [11] with length 10 正常输出的一个方法 文本和文件处理 字符串处理 字符串操作：(需要导入包strings)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 Contains(查看是否包含某一字串，包含返回true) strings.Contains(\u0026#34;hello\u0026#34;, \u0026#34;he\u0026#34;) Join(字符串拼接，参数为一个切片和一个string，返回结果为he@ta@ssss) s := []string{\u0026#34;he\u0026#34;, \u0026#34;ta\u0026#34;, \u0026#34;ssss\u0026#34;} fmt.Println(strings.Join(s, \u0026#34;@\u0026#34;)) Index(在字符串s中查找sep所在的位置，返回位置值，找不到返回-1，如果ken第二个值为t，返回的就是-1) strings.Index(\u0026#34;chicken\u0026#34;,\u0026#34;ken\u0026#34;) 返回4 Repeat(重复字符串n次，最后返回重复的字符串) strings.Repeat(\u0026#34;na\u0026#34;,2) Replace(替换,2表示替换几个，-1表示全部替换) strings.Replace(\u0026#34;ok ok ok\u0026#34;,\u0026#34;k\u0026#34;,\u0026#34;t\u0026#34;,2) Split(分离，把字符串按某个字符进行分割，返回分割后的值，返回值是一个切片) strings.Split(\u0026#34;a,b,c\u0026#34;,\u0026#34;,\u0026#34;) Trim(在s字符串的头部和尾部去除指定的字符串,最后三个感叹号都会去除，如果结尾没有感叹号就只去除开头的) strings.Trim(\u0026#34;!!!Anan!!!\u0026#34;,\u0026#34;!\u0026#34;) Fields(去除s字符串的空格符，并且按照空格分割返回一个切片类型,任意个空格都会去除) strings.Fields(\u0026#34; a b c \u0026#34;) 字符串转换：(需要导入包strconv)\nAppend系列函数将整数等转换为字符串后，添加到现有的字节数组中\nParse系列函数把字符串转换为其他类型\nParse系列函数的返回值有两个，第一个是转换的值，第二个是error值，当转换失败时error会抛出错误，如转布尔值输入的是tr11ue\n正则表达式的使用（导入包regexp） 1 2 3 4 5 6 7 buf := \u0026#34;abc adc aac a88 k8s\u0026#34; //1.解释规则，它会解析正则表达式，如果成功返回解释器，失败返回nil reg1 := regexp.MustCompile(`a.c`) //括号中可以用双引号和反引号，推荐反引号 //2.根据规则提取关键信息，-1表示匹配所有，1表示只匹配一个，返回的值是一个切片类型 result := reg1.FindAllStringSubmatch(buf, -1) fmt.Println(result, result[2]) JSON处理(需要encoding/json包) 通过结构体生成json文件 默认情况下从结构体生成json格式，结构体的成员首字母必须大写，结构体名可以小写，因为在本包使用 Marshal方法是从本包传结构体到json的包让它编码，它肯定能访问到结构体，但是成员如果小写，json的包就无法调用到\n1 2 3 4 5 6 7 8 9 10 11 12 13 type person struct { Name string Age int } func main() { p1 := person{\u0026#34;张三\u0026#34;,19} b, _ := json.Marshal(p1) fmt.Println(string(b)) var p2 person json.Unmarshal(b, \u0026amp;p2)\t//结构体是值传递，如果不使用指针就只会修改副本，不会修改p2原有的内容 fmt.Println(p2) } Marshal()会返回两个值，一个json内容一个错误值，可以用If语句接受错误，然后用return让main函数中断，return在函数中的意义：如果函数返回值有定义，则return表示返回数据，如果没有定义返回值，return就表示退出此函数，在main函数中有return就表示中断程序但是不会报错\nbuf获得的值是一个字节切片类型，可以直接string(buf)强转成string类型\n转换成string后输出的是一个整体值，无法再用切片[0]来调用任意一个成员\n通过map生成json文件 MarshalIndent()的第三个参数为一个TAB键字符串\njson解析到结构体(json解码) json的变量名是小写，而结构体的首字母是大写，所以可以通过对结构体二次编码来关联上，不建议将结构体的首字母变成小写，因为如果变成小写的话，以后又需要生成json就会失败，并且如果程序其他地方有使用到这个结构体，成员名赋值也需要修改\nUnmarshal()有两个参数，一个是byte切片类型（通过强转jsonBuf获得），一个是结构体的指针，放指针是因为给函数传结构体变量属于值传递，并不会修改本身结构体的内容，是拷贝了一份新的结构体给函数，只有通过指针实现引用传递才可以修改本身结构体内容，这个函数的返回值只有error\n如果只需要部分解析，可以重新定义一个只含有部分成员的结构体来接受解析内容\njson解析到map map也需要传递指针过去，这里原理不太清楚，但是测试过发现不加指针\u0026amp;就无法对原值进行修改，尽管map属于引用传递\n如果通过map接受解析的值，它接受到的值类型都属于空接口类型，对其强转string(m[\u0026ldquo;test\u0026rdquo;])是无法成功的，需要使用switch断言来回推类型进行赋值\nswitch可以在赋值的同时进行判断，即 switch a := 3; 判断的是st的值，空接口类型断言隐性的将data的类型放在了判断上面，且将data空接口类型自动转换成适合它的类型\n推荐使用结构体来解析，因为map如果要确定值的类型就需要断言，过于麻烦\n文件操作（需要os包） 设备文件有屏幕键盘等，标准输出就是屏幕，可以通过os.Stdout.Close()来阻止后面的程序输出内容，可以通过os.Stdint.Close()来阻止后面的程序通过键盘获取输入内容\n文件的创建和写入 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 path := \u0026#34;./test.txt\u0026#34; //创建文件，如果文件存在则打开文件且清空文件内容,它返回一个file的指针和一个error f, err := os.Create(path) if err != nil { //如果有错误就输出错误信息并且中止函数 fmt.Println(err) return } //使用完毕记得要关闭文件，使用defer就可以在函数结束的前一刹那进行关闭 defer f.Close() //Sprintln()可以将一行信息赋值给一个变量 buf := fmt.Sprintln(\u0026#34;我往文件内写入东西\u0026#34;) //往文件内写入字符串，f代表文件的指针，WriteString就可以将内容追加到文件，注意是追加 //它返回两个值，一个是写入的字节数，另一个是error n, err := f.WriteString(buf) if err != nil { fmt.Println(err) return } 文件的读取 下面的代码可以实现指定读取多少内容，如果想直接读完整个文件，只需要在读取那里加一个for循环，它就会一直往下读取直到结尾出现error抛出EOF错误（EOF表示结尾），注意：抛出EOF的if语句要写到err !=nil中，因为EOF错误是它的子集，在err不为空中加return，而EOF中只需要加break，因为break触发后直接跳出循环了，就不会触发return了\n1 2 3 4 5 6 7 if err1 != nil { if err1 == io.EOF{ break } fmt.Println(err1) return } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 path := \u0026#34;./test.txt\u0026#34; //打开文件,返回一个文件指针和一个error f, err := os.Open(path) if err != nil { fmt.Println(err) } //函数运行结束时关闭文件 defer f.Close() //创建一个2k长度的字节切片，表示读取的总量 buf := make([]byte, 1024*10) //Read返回两个值，一个是字节数，一个是error //n是读取了多少文件字节数，如果buf大于文件总字节数，n返回总字节数。 //如果总字节数大于buf，则n等于buf，即最多只能读取buf字节的内容，如果想多读取点就只能增大buf的长度值 //Read()方法会将读取的内容放入buf中，后面查看buf内容即是查看文件内容 n, err1 := f.Read(buf) if err1 != nil \u0026amp;\u0026amp; err1 != io.EOF {\t//文件出错并且没有读到结尾 fmt.Println(err1) return } //可以用m:n来读取buf中间内容，n不能大于buf前面设定的值（即不能大于总长度），否则会报错 //n可以设置常量，即表示查看buf容量中从m到n的内容 //设置n则可以实现当文件总字节数小于buf量全部读取，大于则读取设置的最大值 fmt.Println(\u0026#34;buf:\u0026#34;, string(buf[:n])) //buf中的值都是字节型的，可以通过强转变成string 如何一行一行的进行读取且务必读完文件的全部内容（不论大小） bufio包有一个NewReader函数可以为文件的读取创建一个缓冲区并返回一个缓冲区的指针\nReadBytes是缓冲区 *Reader的方法，读取缓冲区直到第一次遇到delim字节（即指定的关键值\u0026rsquo;\\n\u0026rsquo;等），读取出一次后缓冲区中对应的内容就会消失，因此可以用for死循环来读取多行缓冲区内容\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 path := \u0026#34;./test.txt\u0026#34; //打开文件,返回一个文件指针和一个error f, err := os.Open(path) if err != nil { fmt.Println(err) } //函数运行结束时关闭文件 defer f.Close() //新建一个缓冲区，先把内容放进缓冲区里 r := bufio.NewReader(f) for { //遇到\\n就结束读取,buf为读取到的数据,这种会将\\n也读取进去，所以使用Printf() buf, err1 := r.ReadBytes(\u0026#39;\\n\u0026#39;) //当读到结尾即退出循环 fmt.Printf(\u0026#34;%v\u0026#34;, string(buf)) /*把err判断放到打印后面的原因是如果将它放在前面，它会先判断是否是最后一行，当处于最后 一行时它直接跳出循环，最后一行的输出语句就不会进行，那么就会少输出一行 */ if err1 != nil { if err1 == io.EOF { break } fmt.Println(err1) } } 实例：拷贝文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 //获取命令行参数，判断是否为3个 list := os.Args if len(list) != 3 { fmt.Println(\u0026#34;拷贝格式为xx.exe src dst\u0026#34;) } //比较目标文件和源文件是否同名 srcName := list[1] dstName := list[2] if srcName == dstName { fmt.Println(\u0026#34;源文件和目标文件不能相等\u0026#34;) } //只读方式打开源文件 sf, err1 := os.Open(srcName) if err1 != nil { fmt.Println(err1) return } //新建目标文件 df, err2 := os.Create(dstName) if err2 != nil { fmt.Println(err2) return } //操作完毕要关闭文件 defer sf.Close() defer df.Close() for { //读源文件 buf := make([]byte, 1024*4) n, err3 := sf.Read(buf) //从源文件读取内容 if err3 != nil { if err3 == io.EOF { //源文件读取完毕 break } fmt.Println(err3) return } //写入目标文件，读多少写多少 _, err4 := df.Write(buf[:n]) if err4 != nil { fmt.Println(err3) return } } go语言多线程 Go从语言层面就支持了并发，虽然并发程序的内存管理是非常复杂的，但是GO提供了自动垃圾回收机制\n并行和并发的区别：并行指在同一时刻有多条指令在多个处理器上同时进行，并发指虽然在单个处理器上同一时刻只能有一条指令执行，但多个进程指令被快速的轮换执行，使得在宏观上具有多个进程同时执行的效果\ngoroutine(协程) 通常main函数在一个单独的协程中运行，成为主协程，新的goroutine用go语句来创建，称为子协程，如果主协程中有for死循环，子协程要在死循环前建立，否则一直死循环就无法运行到创建子协程的语句\n注意：主协程退出后，子协程会同时跟着退出\nruntime包 Gosched() Gosched()的作用就像linux中的进程优先级一样，但是如果在A协程中添加了runtime.Gosched()后，A协程会搁置到其他协程完成任务退出协程后再继续进行A协程\nGoexit() 调用runtime.Goexit()将立即终止当前协程的进行，即使写在协程中的调用函数里也会中止当前协程\nGOMAXPROCS() 调用runtime.GOMAXPROCS()用来设置 可以并发计算 的CPU核数的最大值，并返回之前的值\nn := runtime.GOMAXPROCS(4) //以四核并发计算，核数可以大于当前系统的最大核数\nchannel类型 定义了两个函数person1和person2，虽然协程是同时进行的，但是两个公用了一个公共资源，最后打印就会出现这边打印一个字母那边打印一个，就造成了资源竞争问题。channel属于引用传递，即调用的都是同一个channel。\n如果在person2中设置channel堵塞，它就会让此进程一直堵在channel步骤，而person1中先调用公共资源，person2暂停，当person1资源调用完毕后将int=666传入ch,子进程1结束，ch管道中有内容了不再堵塞，此时person2中的同一个ch管道将int传入函数并丢弃，然后继续执行后面的代码来调用公共资源，这样就可以避免资源竞争问题。\n如果希望在子协程工作完成后再关闭主协程的话（主协程关闭会导致子协程同时关闭），可以在子协程中设置管道 ch\u0026lt;- \u0026ldquo;子协程完毕\u0026rdquo;，然后主协程接受这段内容并丢弃( \u0026lt;-ch )，这样就可以实现子协程没有进行到发送信息到管道那一步时，主协程ch永远堵塞，只有完成子协程任务并关闭后，主协程channel才有信息不堵塞，然后才可以正常完成主协程（channel也可以用于发送接受数据，类似linux的竖线管道）\n无缓存通道和有缓存通道 channer分为无缓存通道和有缓存通道，无缓存channel没有接收或者没有发送都会造成堵塞，有缓存值的在写满缓存时就会造成堵塞，通道中没值时也无法取数据\n有缓存cannel属于异步处理，每当接收者从cannel取出一条数据时，cannel中就会丢弃这条数据，将空间闲置出来给新的数据使用，当数据取完或写满时就会造成阻塞\nclose(ch)可以关闭通道，接收者可以通过 value,ok := \u0026lt;-ch来获取值，value为管道中的数据，ok在当管道没有关闭时为true，管道关闭了则为false\n单项channel管道 双向channel可以隐式的转换为单向channel ( var writeCan chan\u0026lt;- int = ch ),单向无法转换为双向\n案例：\nchannel可以通过range来依次读取通道内的数据，它的参数只有num，并非两个值。且必须搭配close(ch)使用，不然继续迭代下去，没有值但是还在进行\u0026lt;-ch，这会造成通道阻塞，出现死锁问题。在写入channel的函数中最后加上 close(ch) 就可以给它发送一个信号，它会在读取完全部值后退出for循环\n由于channel属于引用传递，所以虽然函数的参数是单项通道，但是最终修改的依然是本来的双向通道ch，这可以避免在函数中又读又写造成逻辑混乱\n定时器 Timer(类似一次性闹钟) NewTimer只是返回了一个2s后的Timer指针变量，如果需要真的延时2s，必须使用 \u0026lt;-timer.C来将通道数据丢弃或赋值，因为它是在2s后接受数据并传入变量，在2s之前这个通道都是堵塞的（通道内没有数据），程序不会向下运行\nNewTimer函数返回一个Timer的指针，Timer只会响应一次，过后不会再响应（一次性闹钟），即只会向cannel写一次2s后的时间\n1 2 3 4 5 \u0026lt;-time.After(2 * time.Second) 定时程序2s,2s后产生一个事件，往channerl中写入时间（2s后那个时间点）数据 timer.Stop() 停止定时器 timer.Reset(1 * time.Second) 重新设置为1s Ticker（类似循环闹钟） Select (可以监听channel通道的数据流动) 注意：如果写了default，即每次都能判定成功，会导致select语句完成判定然后结束，不写就会（一直）阻塞直到case判定成功执行某一个case语句然后结束\n如果select语句不加for循环，那么它只会判定一次并只将数据写入一次管道，监听一次就结束显然不符合监听的目的，所以需要往select外套一个for死循环来实现监听操作\n第二个case语句，写入通道的操作必须要有一个读的操作（\u0026lt;-chan2）可以接收它的数据，只有写没有读是不能写成功的，有读没写也是不能读成功，都会造成管道死锁问题，这样就可以通过select实现在外部写入，select中的case读操作就判断成功。\n注意：case不止是判断，它判断后面的语句能否读写成功，那么在判断成功的同时它也会往通道中读写数据\n斐波那契数列 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 func fibonacci(ch chan\u0026lt;- int, quit \u0026lt;-chan bool) { x, y := 1, 1 for { select { case ch \u0026lt;- x: x, y = y, x+y case flag := \u0026lt;-quit: return } } } func main() { ch := make(chan int) quit := make(chan bool) //输出数字 go func() { for i := 0; i \u0026lt; 8; i++ { num := \u0026lt;-ch fmt.Println(num) } quit \u0026lt;- true }() //产生数字，写入管道 fibonacci(ch, quit) } 输出结果为：1\t1\t2\t3\t5\t8\t13\t21 除去第二个数，其他数为前两个数相加 如果select语句不加for循环，那么它只会判定一次并只将数据写入一次管道，而fibonacci函数处于主协程，当判定成功后就会直接完成主协程，那么子协程也会退出，后面的数据都无法继续输出\n用select实现超时退出 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 ch := make(chan int) quit := make(chan bool) //监听管道数据 go func() { for { select { case num := \u0026lt;-ch: fmt.Println(num) case \u0026lt;-time.NewTimer(3 * time.Second).C: fmt.Println(\u0026#34;3s没有输出数据\u0026#34;) quit \u0026lt;- true return } } }() go func() { for i := 0; i \u0026lt; 4; i++ { ch \u0026lt;- i time.Sleep(time.Second) } \u0026lt;-quit fmt.Println(\u0026#34;程序结束\u0026#34;) }() for {} 当ch中没有数据时，case ch会堵塞，然后三秒后case time会有数据，执行case2，往quit管道中写入数据，最下面的读取quit就不会堵塞，程序就会继续执行，如果不希望主程序结束，可以将quit管道放到一个子协程中（且主程序有for循环之类的不会结束），那么三秒后子协程运行完自动退出，不会波及主协程\n注意：case语句是会执行之后的语句的，所以time.NewTimer()会在3s后继续有值，且会再输出fmt，然后此时quit管道没接收者，会一直堵塞在这里，子协程会一直存在直到主协程关闭，所以加上return语句让它在第一次就关闭此函数，或者break跳出for循环\nSocket网络编程（需要net包） c/s架构 服务器部分： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 //设置监听，此处及下面都会返回err，为了缩短代码量丢弃了err，工作中不要丢弃 listener, _ := net.Listen(\u0026#34;tcp\u0026#34;, \u0026#34;127.0.0.1:8888\u0026#34;) //阻塞等待用户数据 conn, _ := listener.Accept() //接收用户请求 buf := make([]byte, 1024) //用户数据最后返回到了buf切片中，n表示从用户数据那读取的字节数，最大值为切片的长度1024 n, _ := conn.Read(buf) fmt.Println(string(buf[:n])) //最后处理完数据记得关闭连接 defer func() { listener.Close() conn.Close() }() 服务器端先定义一个监听，表示将这个服务器以什么协议放置于什么位置，然后listener.Accept()让服务器阻塞等待用户向服务器端发送数据，用户发送数据后会存入conn中，通过conn.Read()来获取用户输入的数据并放到buf切片中，通过string(buf[:n])强转用户的字节数据为字符串，n表示数据量大于切片则返回切片最大值数据量，小于切片则返回全部数据，n返回的是Read所读取的总字节量，其值不会超过buf定义的1024字节\n客户端部分： 1 2 3 4 5 //主动连接服务器 conn, _ := net.Dial(\u0026#34;tcp\u0026#34;, \u0026#34;127.0.0.1:8888\u0026#34;) //发送数据 conn.Write([]byte(\u0026#34;are you ok?\u0026#34;)) defer conn.Close() 客户端部分只需要连接服务器并且发送数据，连接服务器需要指定服务器的ip端口和协议，发送的数据是字节切片类型\n如何多个客户端同时连接同一个服务器（重要） 服务器端：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 //conn的类型为net包里的Conn接口 func HandleConn(conn net.Conn) { //每个用户使用完毕后关闭协程 defer conn.Close() //获取客户端的网络信息,并以ip 端口的形式输出 addr := conn.RemoteAddr().String() fmt.Println(addr, \u0026#34;---连接成功\u0026#34;) buf := make([]byte, 2048) //用for循环套住Read()，用户发送一次数据，接收一次并赋值给buf， //运行下面的打印和回传，再次循环并阻塞在Read()处等待用户再次发送请求 //但是这样会让子进程一直存在，除非设置了err不为空时退出实现强制退出 //因此加一个用户输入exit退出的逻辑 for{ n, _ := conn.Read(buf) //打印用户发送过来的内容 fmt.Printf(\u0026#34;%s输入了: %s\\n\u0026#34;, addr, string(buf[:n])) //把用户信息转为大写发回给用户(先将小写的字节切片转为string，然后变成大写再强转为字节切片) //n-是因为在windows中输入的语句有一个\\r\\n换行符，需要-2去除它 //各个平台都不一样，因此可以在前面通过len(string(buf[:n]))来判断到底多了几个字符 if string(buf[:n-2]) == \u0026#34;exit\u0026#34; { fmt.Println(addr, \u0026#34; exit\u0026#34;) return } conn.Write([]byte(strings.ToUpper(string(buf[:n])))) } } func main() { //设置监听，此处及下面都会返回err，为了缩短代码量丢弃了err，工作中不要丢弃 listener, _ := net.Listen(\u0026#34;tcp\u0026#34;, \u0026#34;127.0.0.1:8888\u0026#34;) //利用for循环实现多个客户端连接同一个服务器 for { //循环阻塞等待用户请求，一个用户请求然后往下走，然后循环继续等下一个用户 conn, _ := listener.Accept() //开子协程处理多个用户请求，如果没有用户进入就会阻塞到第一步直到第一个用户请求， //然后往下开一个新协程给此用户，继续for循环等待下一个用户请求 go HandleConn(conn) } defer listener.Close() } 客户端：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 func main() { //主动连接服务端 conn, _ := net.Dial(\u0026#34;tcp\u0026#34;, \u0026#34;127.0.0.1:8888\u0026#34;) defer conn.Close() //从键盘获取输入并发往服务器端 go func() { str := make([]byte, 2048) //用for循环可以实现当os.Stdin.Read()中没有数据时（即没有进行输入）， //阻塞在这一步，直到用户输入内容才继续向下进行 for { //os.Stdin.Read可以提示键盘输入并且将输入的内容转换为字节切片，并赋值到str中，返回切片的长度 n, _ := os.Stdin.Read(str) //发送给服务器端 conn.Write(str[:n]) } }() //从服务器端获取数据 buf := make([]byte, 2048) for { //for循环实现当服务器未往客户端发送数据时，conn.Read(buf)为空，阻塞在这一步， //有数据循环一遍，然后等待下次服务器的数据 //当服务器端输入exit后，服务器端所对应的子协程结束，conn.Read()返回err， //通过return结束主协程，子协程同时结束，退出程序 n, err := conn.Read(buf) if err != nil { fmt.Println(err) return } fmt.Println(string(buf[:n])) } } 远程发送文件： 服务器端：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 func RecvFile(path string, conn net.Conn) { //创建文件 f, _ := os.Create(path) //循环接收文件的内容 buf := make([]byte, 1024*4) for { n, err := conn.Read(buf) if err != nil { if err == io.EOF { fmt.Println(\u0026#34;文件接收完毕\u0026#34;) } else { fmt.Println(err) } return } //往文件写入内容 f.Write(buf[:n]) } } func main() { //建立监听 listener, _ := net.Listen(\u0026#34;tcp\u0026#34;, \u0026#34;127.0.0.1:8888\u0026#34;) //阻塞等待用户请求 conn, _ := listener.Accept() //接收用户文件名 buf := make([]byte, 1024) n, _ := conn.Read(buf) path := string(buf[:n]) //返回消息 conn.Write([]byte(\u0026#34;开始发送\u0026#34;)) //接收文件内容 RecvFile(path, conn) defer listener.Close() defer conn.Close() } 客户端：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 func SendFile(path string, conn net.Conn) { //打开文件，读取文件 f, _ := os.Open(path) buf := make([]byte, 1024*4) for { n, err := f.Read(buf) if err != nil { if err == io.EOF { fmt.Println(\u0026#34;文件传输完成\u0026#34;) } else { fmt.Println(err) } return } //发送到服务器 conn.Write(buf[:n]) } defer f.Close() defer conn.Close() } func main() { fmt.Println(\u0026#34;请输入文件名：\u0026#34;) var path string fmt.Scan(\u0026amp;path) //os.Stat()返回FileInfo类型变量，可以获取文件信息,info.Name()获取文件名,没有此文件则会报错 info, err := os.Stat(path) if err != nil { fmt.Println(\u0026#34;没有这个文件\u0026#34;, err) return } //连接服务器,工作中err别丢空 conn, _ := net.Dial(\u0026#34;tcp\u0026#34;, \u0026#34;127.0.0.1:8888\u0026#34;) //给服务器先发送文件名,err和n都可以丢空 conn.Write([]byte(info.Name())) //服务器接收到文件名，向客户端发送消息,客户端进行判断开始进行发送 buf := make([]byte, 1024) n, _ := conn.Read(buf) if string(buf[:n]) == \u0026#34;开始发送\u0026#34; { SendFile(path, conn) } } 并发聊天服务器 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 type Clinet struct { C chan string //管道string类型，暂存用户发送的数据 Name string //用户名 Addr string //网络地址 } var onlineMap = make(map[string]Clinet) var message = make(chan string) func WriteMsgToClient(cli Clinet, conn net.Conn) { //这个是为了实现cli.C中有数据时向各自客户端发送数据，没有数据时阻塞在这一步 //且任意用户都会在登录时都会通过Manager()方法向每个用户的cli.C发送登录信息， //只要cli.C一有信息，这个子协程就会检测到cli.C不再阻塞，就能向客户端写入新的他人登录信息 //只要发送处的管道没有关闭，cli.Close()，这个for就不会检测到false，会一直堵塞在这里 for msg := range cli.C { conn.Write([]byte(msg + \u0026#34;\\n\u0026#34;)) } } //将用户存进在线用户变量onlineMap中 func HandleConn(conn net.Conn) { defer conn.Close() //获取网络地址 cliAddr := conn.RemoteAddr().String() //创建一个结构体,添加到map中 cli := Clinet{make(chan string), cliAddr, cliAddr} onlineMap[cliAddr] = cli //新开一个协程，专门给当前用户发送消息 go WriteMsgToClient(cli, conn) //广播某个人在线 message \u0026lt;- \u0026#34;[\u0026#34; + cli.Name + \u0026#34;]---login\u0026#34; //退出进行广播并且关闭子协程 isQuit := make(chan bool) hasData := make(chan bool) //新开一个协程，接收用户发送过来的请求 go func() { buf := make([]byte, 2048) //for循环可以避免输入一次就不再进行接收信息的问题 for { n, _ := conn.Read(buf) if n == 0 { //对方断开或者出问题 isQuit \u0026lt;- true return } msg := string(buf[:n-2]) //过滤window末尾的/r/n符号 //查询所有用户 if len(msg) == 3 \u0026amp;\u0026amp; msg == \u0026#34;who\u0026#34; { //避免whoami和who匹配 //遍历map，给当前用户发送所有成员 conn.Write([]byte(\u0026#34;user list:\\n\u0026#34;)) for _, tmp := range onlineMap { msg = tmp.Name + \u0026#34;-----is online\\n\u0026#34; conn.Write([]byte(msg)) } //给用户重命名，输入rename|mike } else if len(msg) \u0026gt;= 8 \u0026amp;\u0026amp; msg[:6] == \u0026#34;rename\u0026#34; { name := strings.Split(msg, \u0026#34;|\u0026#34;)[1] //将msg以|分割 cli.Name = name conn.Write([]byte(\u0026#34;u name is rename\u0026#34;)) } else { //message复用来给所有用户广播它发送的消息，包括自己也看见 message \u0026lt;- cli.Name + \u0026#34;:\u0026#34; + msg } hasData \u0026lt;- true } }() //用for循环让此子协程不会结束，避免发送消息后子协程结束，这个用户就通信结束了 //目的是让用户可以接收到后面登录和发送的信息，所以这个子协程就必须一直存在，除非用户退出 for { //通过select检测管道isQuit的流动 select { case \u0026lt;-isQuit: //删除用户并且广播谁下线了 delete(onlineMap, cliAddr) message \u0026lt;- cli.Name + \u0026#34;--is login out\u0026#34; return case \u0026lt;-hasData: //有数据不作处理 case \u0026lt;-time.After(60 * time.Second): //60s后超时执行此case,超时强制退出 delete(onlineMap, cliAddr) message \u0026lt;- cli.Name + \u0026#34;--is time out leave out\u0026#34; return } } } //新开一个协程，转发消息，只要消息来了就遍历map，给map每个成员都发送此消息 func Manager() { for { //mes为string类型的变量，自动推导类型 msg := \u0026lt;-message //遍历map，给map每个成员都发送此消息 for _, cli := range onlineMap { cli.C \u0026lt;- msg } } } func main() { //监听 listener, _ := net.Listen(\u0026#34;tcp\u0026#34;, \u0026#34;127.0.0.1:8888\u0026#34;) defer listener.Close() go Manager() //循环 for { //循环阻塞，形成多个客户端共用一个服务器 conn, _ := listener.Accept() //处理用户连接 go HandleConn(conn) } } 先是主函数启动子协程HandleConn(conn)，它作用于往map中写入在线成员，并将消息发给管道message,再通过Manager()将message管道发给msg字符串，遍历map，往每个map中的管道写入信息,紧接着通过WriteMsgToClient()中的range向客户端写入消息，且for遍历管道时，没有cli.Close()的存在，它会一直堵塞在此处等待新的信息写进cli.C\nb/s架构： 但是如果工作中每次都需要向服务器发送一长串的请求包过于繁琐，所以可以使用net/http包来简化\nhttp服务器： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 //w为给客户端回复的数据 //req，读取客户端的数据 func HandConn(w http.ResponseWriter, req *http.Request) { //给客户端浏览器发送数据 w.Write([]byte(\u0026#34;hello go\u0026#34;)) //获取客户端的请求头部参数等 //在https://studygolang.com/pkgdoc中的net/http中搜type Request可以获取req的所有参数 fmt.Println(req.URL.Path) } func main() { //注册处理函数，用户连接进来自动调用指定的处理函数(即如果域名后面接了/hello则调用后面那个函数) //源代码中第二个参数为handler func(ResponseWriter, *Request) //即在定义函数时已经定义了这是一个func，且默认已经传了两个参数进去，所以不需要加()来调用， //自己写HandConn函数时也不再需要想办法获取ResponseWriter和*http.Request的值了 http.HandleFunc(\u0026#34;/hello\u0026#34;, HandConn) //该方法用于在指定的网络地址进行监听，然后调用服务端处理程序来处理传入的连接请求 //该方法有两个参数：第一个为监听地址；第二个参数表示服务器端处理程序，通常为空 //第二个参数为空意味着服务端调用http.DefaultServerMux进行处理 http.ListenAndServe(\u0026#34;127.0.0.1:8000\u0026#34;, nil) } http客户端： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 //获取从百度回传回来的请求包，http是必须要加的 resp, _ := http.Get(\u0026#34;http://www.baidu.com\u0026#34;) //body是从服务器端读取资源(类似于conn.Read())，最后是需要进行关闭的 defer resp.Body.Close() fmt.Println(\u0026#34;Status =\u0026#34;, resp.Status) fmt.Println(\u0026#34;StatusCode =\u0026#34;, resp.StatusCode) fmt.Println(\u0026#34;Header =\u0026#34;, resp.Header) //获取baidu.com中的数据Read()，然后赋值给buf，最后追加到tmp中 buf := make([]byte, 1024*4) var tmp string for { n, err := resp.Body.Read(buf) if n == 0 { fmt.Println(\u0026#34;read err =\u0026#34;, err) break } tmp += string(buf[:n]) } fmt.Println(tmp) 用go写爬虫爬百度贴吧： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 func SpiderPage(i int, page chan int) { //寻找网址规律，每页pn加50,用for循环获取每个网址 url := \u0026#34;http://tieba.baidu.com/f?kw=%E6%8A%97%E5%8E%8B%E8%83%8C%E9%94%85\u0026amp;ie=utf-8\u0026amp;pn=\u0026#34; + strconv.Itoa((i-1)*50) fmt.Printf(\u0026#34;正在爬取第%d页网页%s\\n\u0026#34;, i, url) result, _ := HttpGet(url) //将内容写到文件中 f, _ := os.Create(strconv.Itoa(i) + \u0026#34;.html\u0026#34;) f.Write([]byte(result)) f.Close() page \u0026lt;- i } func HttpGet(url string) (result string, err error) { rep, _ := http.Get(url) defer rep.Body.Close() //爬取 buf := make([]byte, 1024*4) for { n, err := rep.Body.Read(buf) if n == 0 { fmt.Println(err) break } result += string(buf[:n]) } return } func DoWork(start, end int) { fmt.Printf(\u0026#34;正在爬取 %d到%d的页面。。。\u0026#34;, start, end) //建立一个管道，避免主进程结束导致子进程结束 page := make(chan int) //获取地址 for i := start; i \u0026lt;= end; i++ { //建立子协程，让多个爬虫同时进行 go SpiderPage(i, page) } for i := start; i \u0026lt;= end; i++ { //避免协程结束影响子协程 fmt.Printf(\u0026#34;第%d页已经读取完毕\u0026#34;, \u0026lt;-page) } return } func main() { var start, end int fmt.Println(\u0026#34;请输入起始页\u0026#34;) fmt.Scan(\u0026amp;start) fmt.Println(\u0026#34;请输入结束页\u0026#34;) fmt.Scan(\u0026amp;end) DoWork(start, end) } 此方法可以爬取每页内容（包括html内容）存到新建的.html中，如果要爬取需要的内容，可以查看源代码然后通过正则表达式爬出来再存入文件中\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 如爬取每个帖子的某段内容，先在每个主页查看源代码爬取出每个帖子的url， \u0026lt;a rel=\u0026#34;noreferrer\u0026#34; href=\u0026#34;/p/6978750013\u0026#34; title=\u0026#34;老马是真的叼，剪辑的更吊！！！\u0026#34; target=\u0026#34;_blank\u0026#34; class=\u0026#34;j_th_tit \u0026#34; 用正则表达式 (`\u0026lt;a rel=\u0026#34;noreferrer\u0026#34; href=\u0026#34;(?s:(.*?))\u0026#34; title=`) 来爬取出网页链接， regexp的FindAllStringSubmatch会返回一个二维切片，切片中的里切片第一个值是通过表达式过滤出的内容， 第二个值是正则表达式代表的内容，即/p/6978750013，给它拼接上贴吧网址即可访问 通过range迭代外切片然后在里面调用里切片[1]即可 然后在range中用http.Get爬取内容，依然是查看源代码找到对应的内容进行过滤 过来出来的内容可能会有些\\t \u0026lt;br /\u0026gt;之类的，可以用strings.Replace(text,\u0026#34;\\t\u0026#34;,\u0026#34;\u0026#34;,-1)去掉 最后写入文件，如果多标题多内容，可以分开存入到两个切片中，然后后面一口气写进文件中 如果不用子协程来爬取，它就是单协程的程序，它会爬完一个再爬下一个，开了子协程后它可以同时爬取多个，节省了大量时间，但是要注意子协程开启后主协程不能关闭，这样会导致子协程也同时消失，可以使用切片来阻塞主协程，直到爬取完毕\n常见知识点 switch中隐式含有break，但是select中必须要用break或continue跳出循环 结构体实现接口时不需要导入接口所在的包 一个go文件中可以包含多个init同名函数，他属于官方在内存中定义的函数，自上而下运行，自己定义的函数不能同名 变量定义可以用中文，会转换成ASCII码，变量名中不能有$ string不能用nil，它的空值为“”空字符串 make([]int,0)初始化切片的时候，必须要指定容量，即使是0，不指定会报错 golang的自增自减++ \u0026ndash; 只有后置，没有前置 只要两个接口拥有相同的方法列表，那么它们就是等价的，可以相互赋值。 cap函数（容量大小）只能传数组，切片，管道 recover()在func()外运行是无法阻止异常的，必须要写func(){recover()}才能处理异常 指针、数组、切片、map字典、结构体都属于复合类型 new()返回*T，只分配内存，不初始化。make返回T，分配并初始化，只适用于map,slice,chan ","date":"2020-09-09T00:00:00Z","permalink":"https://xiaonuoz.github.io/p/golang%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","title":"golang学习笔记"}]